{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем forward и backward pass для функции $$f(x, w) = \\frac{1}{1 + \\exp(-(w_0 x + w_1))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass\n",
    "\n",
    "Пусть $x = 2, w_0 = 3, w_1 = -4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2\n",
    "w = [3, -4]\n",
    "\n",
    "f = # YOUR CODE HERE\n",
    "\n",
    "print('f = {}'.format(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь разобьем вычисление на части, чтобы было удобнее считать производные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = # YOUR CODE HERE\n",
    "b = # YOUR CODE HERE\n",
    "c = # YOUR CODE HERE\n",
    "d = # YOUR CODE HERE\n",
    "e = # YOUR CODE HERE\n",
    "f = # YOUR CODE HERE\n",
    "\n",
    "print('f = {}'.format(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass\n",
    "\n",
    "Для упрощения имен переменных здесь и далее в коде будем придерживаться следующего алиаса: $$\\frac{df}{dx} = dx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = # YOUR CODE HERE\n",
    "de = # YOUR CODE HERE\n",
    "dd = # YOUR CODE HERE\n",
    "dc = # YOUR CODE HERE\n",
    "db = # YOUR CODE HERE\n",
    "da = # YOUR CODE HERE\n",
    "\n",
    "dx = # YOUR CODE HERE\n",
    "dw0 = # YOUR CODE HERE\n",
    "dw1 = # YOUR CODE HERE\n",
    "\n",
    "print('dx = {}\\n'\n",
    "      'dw0 = {}\\n'\n",
    "      'dw1 = {}'.\n",
    "      format(dx, dw0, dw1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим производную $\\frac{df}{db}$ воспользовавшись знанием, что $$f = \\sigma(b)$$ $$\\frac{d\\sigma(x)}{dx} = \\sigma(x)(1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "dsigmoid = sigmoid(b) * (1 - sigmoid(b))\n",
    "\n",
    "print('dsigmoid = \\t{}\\n'\n",
    "      'db = \\t\\t{}\\n'.\n",
    "      format(dsigmoid, db))\n",
    "assert np.isclose(dsigmoid, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient checking\n",
    "\n",
    "А теперь проверим, что мы правильно посчитали все производные\n",
    "\n",
    "Для этого воспользуемся [gradient checking](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/).\n",
    "\n",
    "Суть этого метода заключается в численном вычислении аппроксимации производной по формуле $$\\frac{df}{dx} = \\frac{f(x + \\epsilon) - f(x - \\epsilon)}{2\\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала проверим, что мы верно нашли производную $\\sigma(b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sigmoid(b) = {}'.format(sigmoid(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-5\n",
    "\n",
    "dsigmoid_check = # YOUR CODE HERE\n",
    "\n",
    "print('dsigmoid = \\t\\t{}\\n'\n",
    "      'dsigmoid_check = \\t{}\\n'.\n",
    "      format(dsigmoid, dsigmoid_check))\n",
    "assert np.isclose(dsigmoid, dsigmoid_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь проверим производные нашей функции $f$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f(x, w):\n",
    "    return sigmoid(x * w[0] + w[1])\n",
    "\n",
    "f = calculate_f(x, w)\n",
    "\n",
    "print('f = {}'.format(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_check = # YOUR CODE HERE\n",
    "\n",
    "print('dx = \\t\\t{}\\n'\n",
    "      'dx_check = \\t{}\\n'.\n",
    "      format(dx, dx_check))\n",
    "assert np.isclose(dx, dx_check)\n",
    "\n",
    "dw0_check = # YOUR CODE HERE\n",
    "\n",
    "print('dw0 = \\t\\t{}\\n'\n",
    "      'dw0_check = \\t{}\\n'.\n",
    "      format(dw0, dw0_check))\n",
    "assert np.isclose(dw0, dw0_check)\n",
    "\n",
    "dw1_check = # YOUR CODE HERE\n",
    "\n",
    "print('dw1 = \\t\\t{}\\n'\n",
    "      'dw1_check = \\t{}\\n'.\n",
    "      format(dw1, dw1_check))\n",
    "assert np.isclose(dw1, dw1_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Матрицы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем forward и backward pass для функции $$L = \\lVert XW - Y \\rVert_2^2 + \\lambda \\lVert W \\rVert_1 $$\n",
    "\n",
    "<img src=\"img/example2.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42 * 2)\n",
    "\n",
    "X = np.random.randint(-4, 4, size=(3, 2))\n",
    "W = np.random.randint(-4, 4, size=(2, 1))\n",
    "Y = np.random.randint(-4, 4, size=(3, 1))\n",
    "lam = 2 * (-1 if np.random.randint(2) == 0 else 1)\n",
    "\n",
    "print('X = {}\\n'\n",
    "      'W = {}\\n'\n",
    "      'Y = {}\\n'\n",
    "      'lambda = {}\\n'.\n",
    "      format(X, W, Y, lam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = # YOUR CODE HERE\n",
    "pred_loss = # YOUR CODE HERE\n",
    "reg_loss = # YOUR CODE HERE\n",
    "loss = # YOUR CODE HERE\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW = # YOUR CODE HERE\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(X, W, Y, lam):\n",
    "    # YOUR CODE HERE (just copy from forward pass)\n",
    "    return loss\n",
    "\n",
    "calculate_loss(X, W, Y, lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-5\n",
    "dW_check = # YOUR CODE HERE\n",
    "\n",
    "        \n",
    "print('dW = \\t\\t{}\\n'\n",
    "      'dW_check = \\t{}\\n'.\n",
    "      format(dW, dW_check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Simple NN\n",
    "\n",
    "<img src=\"img/example3.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "def sigmoid(x):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "def fprop(x, y, params):\n",
    "    W1, b1, W2, b2 = [params[key] for key in ('W1', 'b1', 'W2', 'b2')]\n",
    "    z1 = # YOUR CODE HERE\n",
    "    a1 = # YOUR CODE HERE\n",
    "    z2 = # YOUR CODE HERE\n",
    "    a2 = # YOUR CODE HERE\n",
    "    loss = # YOUR CODE HERE\n",
    "    \n",
    "    ret = OrderedDict({'x': x, 'y': y, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'loss': loss})\n",
    "    for key in params:\n",
    "        ret[key] = params[key]\n",
    "    return ret\n",
    "\n",
    "def bprop(fprop_cache):\n",
    "    x, y, z1, a1, z2, a2, loss = [fprop_cache[key] for key in ('x', 'y', 'z1', 'a1', 'z2', 'a2', 'loss')]\n",
    "    \n",
    "    dW2 = # YOUR CODE HERE\n",
    "    db2 = # YOUR CODE HERE\n",
    "        \n",
    "    dW1 = # YOUR CODE HERE\n",
    "    db1 = # YOUR CODE HERE\n",
    "    return OrderedDict({'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "W1 = np.random.rand(2, 2)\n",
    "b1 = np.random.rand(2, 1)\n",
    "W2 = np.random.rand(1, 2)\n",
    "b2 = np.random.rand(1, 1)\n",
    "params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "x = np.random.rand(2, 1)\n",
    "y = np.random.randint(0, 2)\n",
    "\n",
    "print('x = ', x)\n",
    "print('y = ', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fprop_cache = fprop(x, y, params)\n",
    "print('loss')\n",
    "print(fprop_cache['loss'])\n",
    "\n",
    "bprop_cache = bprop(fprop_cache)\n",
    "\n",
    "for key in bprop_cache:\n",
    "    print('d' + key)\n",
    "    print(bprop_cache[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-6\n",
    "check_cache = OrderedDict()\n",
    "\n",
    "# For every single parameter (W, b)\n",
    "for key in params:\n",
    "    param = params[key]\n",
    "    param_check_cache = np.zeros(param.shape)\n",
    "    for j in range(param_check_cache.shape[0]):\n",
    "        for k in range(param_check_cache.shape[1]):\n",
    "            # For every element of parameter matrix, compute gradient of loss wrt\n",
    "            # that element numerically using finite differences\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            \n",
    "    check_cache[key] = param_check_cache\n",
    "\n",
    "for key in params:\n",
    "    print('d{} = \\n{}'.format(key, bprop_cache[key]))\n",
    "    print('d{}_check = \\n{}'.format(key, check_cache[key]))\n",
    "    print('===================')\n",
    "\n",
    "    assert np.isclose(bprop_cache[key], check_cache[key]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "def softmax_grad(x):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2])\n",
    "s = softmax(x)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = softmax_grad(softmax(x))\n",
    "dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-4\n",
    "dx_check = # YOUR CODE HERE\n",
    "\n",
    "        \n",
    "print('dx = \\n{}'.format(dx))\n",
    "print('dx_check = \\n{}'.format(dx_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
