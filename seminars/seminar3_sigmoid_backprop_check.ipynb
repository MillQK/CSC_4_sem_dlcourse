{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем forward и backward pass для функции $$f(x, w) = \\frac{1}{1 + \\exp(-(w_0 x + w_1))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass\n",
    "\n",
    "Пусть $x = 2, w_0 = 3, w_1 = -4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f = 0.8807970779778823\n"
     ]
    }
   ],
   "source": [
    "x = 2\n",
    "w = [3, -4]\n",
    "\n",
    "f = 1 / (1 + np.exp(-(x * w[0] + w[1])))\n",
    "\n",
    "print('f = {}'.format(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь разобьем вычисление на части, чтобы было удобнее считать производные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f = 0.8807970779778823\n"
     ]
    }
   ],
   "source": [
    "a = x * w[0]\n",
    "b = a + w[1]\n",
    "c = -1 * b\n",
    "d = np.exp(c)\n",
    "e = 1 + d\n",
    "f = 1 / e\n",
    "\n",
    "print('f = {}'.format(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass\n",
    "\n",
    "Для упрощения имен переменных здесь и далее в коде будем придерживаться следующего алиаса: $$\\frac{df}{dx} = dx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx = 0.3149807562105195\n",
      "dw0 = 0.209987170807013\n",
      "dw1 = 0.1049935854035065\n"
     ]
    }
   ],
   "source": [
    "df = 1\n",
    "de = -1 / e**2\n",
    "dd = de * 1\n",
    "dc = dd * np.exp(c)\n",
    "db = dc * -1\n",
    "da = db * 1\n",
    "\n",
    "dx = da * w[0]\n",
    "dw0 = da * x\n",
    "dw1 = db * 1\n",
    "\n",
    "print('dx = {}\\n'\n",
    "      'dw0 = {}\\n'\n",
    "      'dw1 = {}'.\n",
    "      format(dx, dw0, dw1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим производную $\\frac{df}{db}$ воспользовавшись знанием, что $$f = \\sigma(b)$$ $$\\frac{d\\sigma(x)}{dx} = \\sigma(x)(1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsigmoid = \t0.10499358540350662\n",
      "db = \t\t0.1049935854035065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "dsigmoid = sigmoid(b) * (1 - sigmoid(b))\n",
    "\n",
    "print('dsigmoid = \\t{}\\n'\n",
    "      'db = \\t\\t{}\\n'.\n",
    "      format(dsigmoid, db))\n",
    "assert np.isclose(dsigmoid, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient checking\n",
    "\n",
    "А теперь проверим, что мы правильно посчитали все производные\n",
    "\n",
    "Для этого воспользуемся [gradient checking](http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/).\n",
    "\n",
    "Суть этого метода заключается в численном вычислении аппроксимации производной по формуле $$\\frac{df}{dx} = \\frac{f(x + \\epsilon) - f(x - \\epsilon)}{2\\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала проверим, что мы верно нашли производную $\\sigma(b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(b) = 0.8807970779778823\n"
     ]
    }
   ],
   "source": [
    "print('sigmoid(b) = {}'.format(sigmoid(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dsigmoid = \t\t0.10499358540350662\n",
      "dsigmoid_check = \t0.10499358540916325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPS = 1e-5\n",
    "\n",
    "dsigmoid_check = (sigmoid(b + EPS) - sigmoid(b - EPS)) / (2 * EPS)\n",
    "\n",
    "print('dsigmoid = \\t\\t{}\\n'\n",
    "      'dsigmoid_check = \\t{}\\n'.\n",
    "      format(dsigmoid, dsigmoid_check))\n",
    "assert np.isclose(dsigmoid, dsigmoid_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь проверим производные нашей функции $f$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f = 0.8807970779778823\n"
     ]
    }
   ],
   "source": [
    "def calculate_f(x, w):\n",
    "    return sigmoid(x * w[0] + w[1])\n",
    "\n",
    "f = calculate_f(x, w)\n",
    "\n",
    "print('f = {}'.format(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx = \t\t0.3149807562105195\n",
      "dx_check = \t0.3149807562330409\n",
      "\n",
      "dw0 = \t\t0.209987170807013\n",
      "dw0_check = \t0.2099871708183265\n",
      "\n",
      "dw1 = \t\t0.1049935854035065\n",
      "dw1_check = \t0.10499358540916325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dx_check = (calculate_f(x + EPS, w) - calculate_f(x - EPS, w)) / (2 * EPS)\n",
    "\n",
    "print('dx = \\t\\t{}\\n'\n",
    "      'dx_check = \\t{}\\n'.\n",
    "      format(dx, dx_check))\n",
    "assert np.isclose(dx, dx_check)\n",
    "\n",
    "dw0_check = (calculate_f(x, [w[0] + EPS, w[1]]) - calculate_f(x, [w[0] - EPS, w[1]])) / (2 * EPS)\n",
    "\n",
    "print('dw0 = \\t\\t{}\\n'\n",
    "      'dw0_check = \\t{}\\n'.\n",
    "      format(dw0, dw0_check))\n",
    "assert np.isclose(dw0, dw0_check)\n",
    "\n",
    "dw1_check = (calculate_f(x, [w[0], w[1] + EPS]) - calculate_f(x, [w[0], w[1] - EPS])) / (2 * EPS)\n",
    "\n",
    "print('dw1 = \\t\\t{}\\n'\n",
    "      'dw1_check = \\t{}\\n'.\n",
    "      format(dw1, dw1_check))\n",
    "assert np.isclose(dw1, dw1_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Матрицы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем forward и backward pass для функции $$L = \\lVert XW - Y \\rVert_2^2 + \\lambda \\lVert W \\rVert_1 $$\n",
    "\n",
    "<img src=\"img/example2.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [[-2 -3]\n",
      " [-3 -4]\n",
      " [ 1  0]]\n",
      "W = [[2]\n",
      " [1]]\n",
      "Y = [[-4]\n",
      " [ 2]\n",
      " [-3]]\n",
      "lambda = -2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42 * 2)\n",
    "\n",
    "X = np.random.randint(-4, 4, size=(3, 2))\n",
    "W = np.random.randint(-4, 4, size=(2, 1))\n",
    "Y = np.random.randint(-4, 4, size=(3, 1))\n",
    "lam = 2 * (-1 if np.random.randint(2) == 0 else 1)\n",
    "\n",
    "print('X = {}\\n'\n",
    "      'W = {}\\n'\n",
    "      'Y = {}\\n'\n",
    "      'lambda = {}\\n'.\n",
    "      format(X, W, Y, lam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0], [-2], [1]], dtype=float)\n",
    "Y = np.array([[0, 1], [1, -1], [0, -2]], dtype=float)\n",
    "W = np.array([[1, 1]], dtype=float)\n",
    "lam = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = X.dot(W)\n",
    "pred_loss = np.sum((pred - Y) ** 2)\n",
    "reg_loss = np.sum(np.abs(W))\n",
    "loss = pred_loss + lam * reg_loss\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sign(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15. 11.]]\n"
     ]
    }
   ],
   "source": [
    "dloss = 1\n",
    "dreg_loss = lam * dloss\n",
    "dpred_loss = dloss\n",
    "dW1 = np.sign(W) * dreg_loss\n",
    "\n",
    "dpred = dpred_loss * 2 * (pred - Y)\n",
    "dW2 = X.T.dot(dpred)\n",
    "\n",
    "dW = dW1 + dW2\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_loss(X, W, Y, lam):\n",
    "    pred = X.dot(W)\n",
    "    pred_loss = np.sum((pred - Y)**2)\n",
    "    reg_loss = np.sum(np.abs(W))\n",
    "    loss = pred_loss + lam * reg_loss\n",
    "    return loss\n",
    "\n",
    "calculate_loss(X, W, Y, lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW = \t\t[[ 92]\n",
      " [112]]\n",
      "dW_check = \t[[ 92.]\n",
      " [112.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPS = 1e-5\n",
    "dW_check = np.zeros_like(dW, dtype=np.float32)\n",
    "\n",
    "for i in range(dW.shape[0]):\n",
    "    for j in range(dW.shape[1]):\n",
    "        eps = np.zeros_like(W, dtype=np.float32)\n",
    "        eps[i, j] = EPS\n",
    "        W_add_eps = W + eps       \n",
    "        W_sub_eps = W - eps\n",
    "        \n",
    "        dW_check[i, j] = (calculate_loss(X, W_add_eps, Y, lam) - calculate_loss(X, W_sub_eps, Y, lam)) / (2 * float(EPS))\n",
    "\n",
    "        \n",
    "print('dW = \\t\\t{}\\n'\n",
    "      'dW_check = \\t{}\\n'.\n",
    "      format(dW, dW_check))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Simple NN\n",
    "\n",
    "<img src=\"img/example3.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "def fprop(x, y, params):\n",
    "    W1, b1, W2, b2 = [params[key] for key in ('W1', 'b1', 'W2', 'b2')]\n",
    "    z1 = np.dot(W1, x) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    loss = -(y * np.log(a2) + (1 - y) * np.log(1 - a2))\n",
    "    \n",
    "    ret = OrderedDict({'x': x, 'y': y, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2, 'loss': loss})\n",
    "    for key in params:\n",
    "        ret[key] = params[key]\n",
    "    return ret\n",
    "\n",
    "def bprop(fprop_cache):\n",
    "    x, y, z1, a1, z2, a2, loss = [fprop_cache[key] for key in ('x', 'y', 'z1', 'a1', 'z2', 'a2', 'loss')]\n",
    "    \n",
    "    dz2 = (a2 - y)\n",
    "    dW2 = np.dot(dz2, a1.T)\n",
    "    db2 = dz2\n",
    "        \n",
    "    da1 = np.dot(fprop_cache['W2'].T, dz2)\n",
    "    dz1 = da1 * (z1 > 0)\n",
    "    dW1 = np.dot(dz1, x.T)\n",
    "    db1 = dz1\n",
    "    return OrderedDict({'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[0.70807258]\n",
      " [0.02058449]]\n",
      "y =  1\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "W1 = np.random.rand(2, 2)\n",
    "b1 = np.random.rand(2, 1)\n",
    "W2 = np.random.rand(1, 2)\n",
    "b2 = np.random.rand(1, 1)\n",
    "params = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "x = np.random.rand(2, 1)\n",
    "y = np.random.randint(0, 2)\n",
    "\n",
    "print('x = ', x)\n",
    "print('y = ', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "[[0.25835725]]\n",
      "dW1\n",
      "[[-0.00936392 -0.00027222]\n",
      " [-0.13964014 -0.0040595 ]]\n",
      "db1\n",
      "[[-0.01322452]\n",
      " [-0.19721162]]\n",
      "dW2\n",
      "[[-0.10035944 -0.1563307 ]]\n",
      "db2\n",
      "[[-0.22768073]]\n"
     ]
    }
   ],
   "source": [
    "fprop_cache = fprop(x, y, params)\n",
    "print('loss')\n",
    "print(fprop_cache['loss'])\n",
    "\n",
    "bprop_cache = bprop(fprop_cache)\n",
    "\n",
    "for key in bprop_cache:\n",
    "    print('d' + key)\n",
    "    print(bprop_cache[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = \n",
      "[[-0.00936392 -0.00027222]\n",
      " [-0.13964014 -0.0040595 ]]\n",
      "dW1_check = \n",
      "[[-0.00936392 -0.00027222]\n",
      " [-0.13964014 -0.0040595 ]]\n",
      "===================\n",
      "db1 = \n",
      "[[-0.01322452]\n",
      " [-0.19721162]]\n",
      "db1_check = \n",
      "[[-0.01322452]\n",
      " [-0.19721162]]\n",
      "===================\n",
      "dW2 = \n",
      "[[-0.10035944 -0.1563307 ]]\n",
      "dW2_check = \n",
      "[[-0.10035944 -0.1563307 ]]\n",
      "===================\n",
      "db2 = \n",
      "[[-0.22768073]]\n",
      "db2_check = \n",
      "[[-0.22768073]]\n",
      "===================\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6\n",
    "check_cache = OrderedDict()\n",
    "\n",
    "# For every single parameter (W, b)\n",
    "for key in params:\n",
    "    param = params[key]\n",
    "    param_check_cache = np.zeros(param.shape)\n",
    "    for j in range(param_check_cache.shape[0]):\n",
    "        for k in range(param_check_cache.shape[1]):\n",
    "            # For every element of parameter matrix, compute gradient of loss wrt\n",
    "            # that element numerically using finite differences\n",
    "            param_add_eps = np.copy(param)\n",
    "            param_sub_eps = np.copy(param)\n",
    "            param_add_eps[j, k] += eps\n",
    "            param_sub_eps[j, k] -= eps\n",
    "            \n",
    "            add_params = copy(params)\n",
    "            sub_params = copy(params)\n",
    "            add_params[key] = param_add_eps\n",
    "            sub_params[key] = param_sub_eps\n",
    "\n",
    "            param_check_cache[j, k] = (fprop(x, y, add_params)['loss'] - fprop(x, y, sub_params)['loss']) / (2 * eps)\n",
    "    check_cache[key] = param_check_cache\n",
    "\n",
    "for key in params:\n",
    "    print('d{} = \\n{}'.format(key, bprop_cache[key]))\n",
    "    print('d{}_check = \\n{}'.format(key, check_cache[key]))\n",
    "    print('===================')\n",
    "\n",
    "    assert np.isclose(bprop_cache[key], check_cache[key]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / (np.sum(np.exp(x), axis=0, keepdims=True) + EPS)\n",
    "\n",
    "def softmax_grad(x):\n",
    "    # Reshape the 1-d softmax to 2-d so that np.dot will do the matrix multiplication\n",
    "    x = x.reshape(-1, 1)\n",
    "    return np.diagflat(x) - np.dot(x, x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26894116, 0.73105786])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2])\n",
    "s = softmax(x)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19661181, -0.19661154],\n",
       "       [-0.19661154,  0.19661227]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx = softmax_grad(softmax(x))\n",
    "dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx = \n",
      "[[ 0.19661181 -0.19661154]\n",
      " [-0.19661154  0.19661227]]\n",
      "dx_check = \n",
      "[[ 0.1966107  -0.19660804]\n",
      " [-0.19660804  0.19661526]]\n"
     ]
    }
   ],
   "source": [
    "EPS = 1e-4\n",
    "dx_check = np.zeros_like(dx, dtype=np.float32)\n",
    "\n",
    "for i in range(dx.shape[0]):\n",
    "    eps = np.zeros_like(x, dtype=np.float32)\n",
    "    eps[i] = EPS\n",
    "    x_add = x + eps\n",
    "    x_sub = x - eps\n",
    "\n",
    "    dx_check[i] = (softmax(x_add) - softmax(x_sub)) / (2 * float(EPS))\n",
    "\n",
    "        \n",
    "print('dx = \\n{}'.format(dx))\n",
    "print('dx_check = \\n{}'.format(dx_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
