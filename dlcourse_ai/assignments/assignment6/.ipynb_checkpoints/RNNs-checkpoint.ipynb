{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9"
   },
   "outputs": [],
   "source": [
    "!pip3 -qq install torch==0.4.1\n",
    "!pip3 -qq install bokeh==0.13.0\n",
    "!pip3 -qq install gensim==3.6.0\n",
    "!pip3 -qq install nltk\n",
    "!pip3 -qq install scikit-learn==0.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiA2dGmgF1rW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/jetbrains/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/jetbrains/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QstS4NO0L97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTai8Ta0lgwL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'ADJ', 'X', 'NOUN', 'PRT', 'VERB', 'DET', 'ADV', 'NUM', 'ADP', 'PRON', 'CONJ', '.'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URC1B2nvPGFt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rWmSToIaeAo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XCuxEBVbOY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4XsRII5kW5x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35, 4), (35, 4))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "#         <create layers>\n",
    "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count)\n",
    "        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "#         <apply them>\n",
    "        emb = self._emb(inputs)\n",
    "        output, _ = self._lstm(emb)\n",
    "        out = self._out_layer(output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12162162162162163"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "# <calc accuracy>\n",
    "def calc_correct(logits, y_batch):\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    mask = (y_batch != 0).float()\n",
    "    correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
    "    total_count = mask.sum().item()\n",
    "\n",
    "    return correct_count, total_count\n",
    "    \n",
    "correct, total = calc_correct(logits, y_batch)\n",
    "correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMUyUm1hgpe3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4505, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# <calc loss>\n",
    "def calc_loss(criterion, logits, y_batch):\n",
    "    loss = 0.0\n",
    "    batch_size = y_batch.shape[-1]\n",
    "    for num_batch in range(batch_size):\n",
    "        loss += criterion(logits[:,num_batch].type(torch.FloatTensor), y_batch[:,num_batch])\n",
    "\n",
    "    return loss / batch_size\n",
    "\n",
    "calc_loss(criterion, logits, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "\n",
    "#                 <calc loss>\n",
    "                loss = calc_loss(criterion, logits, y_batch)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "#                   <calc accuracy>\n",
    "                cur_correct_count, cur_sum_count = calc_correct(logits, y_batch)\n",
    "\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.71610, Accuracy = 77.80%: 100%|██████████| 572/572 [02:02<00:00,  4.67it/s]\n",
      "[1 / 50]   Val: Loss = 0.38125, Accuracy = 87.42%: 100%|██████████| 13/13 [00:04<00:00,  3.84it/s]\n",
      "[2 / 50] Train: Loss = 0.29283, Accuracy = 90.36%: 100%|██████████| 572/572 [02:11<00:00,  4.36it/s]\n",
      "[2 / 50]   Val: Loss = 0.24971, Accuracy = 91.71%: 100%|██████████| 13/13 [00:04<00:00,  3.21it/s]\n",
      "[3 / 50] Train: Loss = 0.19553, Accuracy = 93.44%: 100%|██████████| 572/572 [02:10<00:00,  5.57it/s]\n",
      "[3 / 50]   Val: Loss = 0.20053, Accuracy = 93.13%: 100%|██████████| 13/13 [00:04<00:00,  3.18it/s]\n",
      "[4 / 50] Train: Loss = 0.14257, Accuracy = 95.07%: 100%|██████████| 572/572 [02:10<00:00,  4.38it/s]\n",
      "[4 / 50]   Val: Loss = 0.17019, Accuracy = 94.18%: 100%|██████████| 13/13 [00:04<00:00,  3.31it/s]\n",
      "[5 / 50] Train: Loss = 0.10812, Accuracy = 96.14%: 100%|██████████| 572/572 [02:10<00:00,  4.38it/s]\n",
      "[5 / 50]   Val: Loss = 0.15688, Accuracy = 94.48%: 100%|██████████| 13/13 [00:04<00:00,  2.94it/s]\n",
      "[6 / 50] Train: Loss = 0.08409, Accuracy = 96.88%: 100%|██████████| 572/572 [02:10<00:00,  4.37it/s]\n",
      "[6 / 50]   Val: Loss = 0.15016, Accuracy = 94.75%: 100%|██████████| 13/13 [00:04<00:00,  3.57it/s]\n",
      "[7 / 50] Train: Loss = 0.06677, Accuracy = 97.45%: 100%|██████████| 572/572 [02:11<00:00,  4.35it/s]\n",
      "[7 / 50]   Val: Loss = 0.14608, Accuracy = 95.05%: 100%|██████████| 13/13 [00:04<00:00,  2.61it/s]\n",
      "[8 / 50] Train: Loss = 0.05385, Accuracy = 97.89%: 100%|██████████| 572/572 [02:11<00:00,  4.35it/s]\n",
      "[8 / 50]   Val: Loss = 0.14702, Accuracy = 95.02%: 100%|██████████| 13/13 [00:04<00:00,  3.12it/s]\n",
      "[9 / 50] Train: Loss = 0.04403, Accuracy = 98.24%: 100%|██████████| 572/572 [02:11<00:00,  5.28it/s]\n",
      "[9 / 50]   Val: Loss = 0.15090, Accuracy = 95.17%: 100%|██████████| 13/13 [00:04<00:00,  3.24it/s]\n",
      "[10 / 50] Train: Loss = 0.03630, Accuracy = 98.54%: 100%|██████████| 572/572 [02:12<00:00,  4.33it/s]\n",
      "[10 / 50]   Val: Loss = 0.15352, Accuracy = 95.18%: 100%|██████████| 13/13 [00:04<00:00,  3.41it/s]\n",
      "[11 / 50] Train: Loss = 0.03008, Accuracy = 98.80%: 100%|██████████| 572/572 [02:10<00:00,  4.38it/s]\n",
      "[11 / 50]   Val: Loss = 0.16305, Accuracy = 95.20%: 100%|██████████| 13/13 [00:04<00:00,  3.22it/s]\n",
      "[12 / 50] Train: Loss = 0.02512, Accuracy = 99.01%: 100%|██████████| 572/572 [02:10<00:00,  4.38it/s]\n",
      "[12 / 50]   Val: Loss = 0.17036, Accuracy = 95.07%: 100%|██████████| 13/13 [00:04<00:00,  3.18it/s]\n",
      "[13 / 50] Train: Loss = 0.02099, Accuracy = 99.18%: 100%|██████████| 572/572 [02:10<00:00,  4.39it/s]\n",
      "[13 / 50]   Val: Loss = 0.17656, Accuracy = 95.06%: 100%|██████████| 13/13 [00:04<00:00,  3.71it/s]\n",
      "[14 / 50] Train: Loss = 0.01764, Accuracy = 99.35%: 100%|██████████| 572/572 [02:15<00:00,  4.23it/s]\n",
      "[14 / 50]   Val: Loss = 0.18531, Accuracy = 95.17%: 100%|██████████| 13/13 [00:04<00:00,  3.45it/s]\n",
      "[15 / 50] Train: Loss = 0.01510, Accuracy = 99.46%: 100%|██████████| 572/572 [02:13<00:00,  4.28it/s] \n",
      "[15 / 50]   Val: Loss = 0.19112, Accuracy = 95.03%: 100%|██████████| 13/13 [00:04<00:00,  3.18it/s]\n",
      "[16 / 50] Train: Loss = 0.01295, Accuracy = 99.56%: 100%|██████████| 572/572 [02:14<00:00,  6.13it/s] \n",
      "[16 / 50]   Val: Loss = 0.20332, Accuracy = 95.01%: 100%|██████████| 13/13 [00:04<00:00,  3.77it/s]\n",
      "[17 / 50] Train: Loss = 0.01129, Accuracy = 99.64%: 100%|██████████| 572/572 [02:15<00:00,  4.24it/s] \n",
      "[17 / 50]   Val: Loss = 0.20726, Accuracy = 94.97%: 100%|██████████| 13/13 [00:04<00:00,  3.19it/s]\n",
      "[18 / 50] Train: Loss = 0.01017, Accuracy = 99.69%: 100%|██████████| 572/572 [02:15<00:00,  4.22it/s] \n",
      "[18 / 50]   Val: Loss = 0.21497, Accuracy = 94.97%: 100%|██████████| 13/13 [00:04<00:00,  2.86it/s]\n",
      "[19 / 50] Train: Loss = 0.00936, Accuracy = 99.73%: 100%|██████████| 572/572 [02:16<00:00,  4.20it/s] \n",
      "[19 / 50]   Val: Loss = 0.22252, Accuracy = 94.96%: 100%|██████████| 13/13 [00:04<00:00,  2.94it/s]\n",
      "[20 / 50] Train: Loss = 0.00856, Accuracy = 99.76%: 100%|██████████| 572/572 [02:15<00:00,  4.22it/s] \n",
      "[20 / 50]   Val: Loss = 0.22780, Accuracy = 95.01%: 100%|██████████| 13/13 [00:04<00:00,  2.98it/s]\n",
      "[21 / 50] Train: Loss = 0.00825, Accuracy = 99.77%: 100%|██████████| 572/572 [02:16<00:00,  5.73it/s] \n",
      "[21 / 50]   Val: Loss = 0.22666, Accuracy = 94.92%: 100%|██████████| 13/13 [00:04<00:00,  3.42it/s]\n",
      "[22 / 50] Train: Loss = 0.00794, Accuracy = 99.78%: 100%|██████████| 572/572 [02:17<00:00,  4.17it/s] \n",
      "[22 / 50]   Val: Loss = 0.23149, Accuracy = 94.97%: 100%|██████████| 13/13 [00:04<00:00,  3.14it/s]\n",
      "[23 / 50] Train: Loss = 0.00732, Accuracy = 99.81%: 100%|██████████| 572/572 [02:13<00:00,  4.28it/s] \n",
      "[23 / 50]   Val: Loss = 0.24668, Accuracy = 94.78%: 100%|██████████| 13/13 [00:04<00:00,  2.82it/s]\n",
      "[24 / 50] Train: Loss = 0.00731, Accuracy = 99.81%: 100%|██████████| 572/572 [02:12<00:00,  4.30it/s] \n",
      "[24 / 50]   Val: Loss = 0.23959, Accuracy = 94.92%: 100%|██████████| 13/13 [00:04<00:00,  2.72it/s]\n",
      "[25 / 50] Train: Loss = 0.00696, Accuracy = 99.82%: 100%|██████████| 572/572 [02:12<00:00,  4.31it/s] \n",
      "[25 / 50]   Val: Loss = 0.24713, Accuracy = 94.89%: 100%|██████████| 13/13 [00:04<00:00,  3.14it/s]\n",
      "[26 / 50] Train: Loss = 0.00683, Accuracy = 99.82%: 100%|██████████| 572/572 [02:11<00:00,  4.34it/s] \n",
      "[26 / 50]   Val: Loss = 0.24802, Accuracy = 94.99%: 100%|██████████| 13/13 [00:04<00:00,  2.87it/s]\n",
      "[27 / 50] Train: Loss = 0.00658, Accuracy = 99.83%: 100%|██████████| 572/572 [02:11<00:00,  4.34it/s] \n",
      "[27 / 50]   Val: Loss = 0.25485, Accuracy = 94.87%: 100%|██████████| 13/13 [00:04<00:00,  3.30it/s]\n",
      "[28 / 50] Train: Loss = 0.00670, Accuracy = 99.82%: 100%|██████████| 572/572 [02:12<00:00,  4.32it/s] \n",
      "[28 / 50]   Val: Loss = 0.25976, Accuracy = 94.92%: 100%|██████████| 13/13 [00:04<00:00,  3.15it/s]\n",
      "[29 / 50] Train: Loss = 0.00645, Accuracy = 99.83%: 100%|██████████| 572/572 [02:12<00:00,  5.55it/s] \n",
      "[29 / 50]   Val: Loss = 0.25662, Accuracy = 94.92%: 100%|██████████| 13/13 [00:04<00:00,  3.53it/s]\n",
      "[30 / 50] Train: Loss = 0.00643, Accuracy = 99.83%: 100%|██████████| 572/572 [02:12<00:00,  4.33it/s] \n",
      "[30 / 50]   Val: Loss = 0.25928, Accuracy = 94.93%: 100%|██████████| 13/13 [00:04<00:00,  3.16it/s]\n",
      "[31 / 50] Train: Loss = 0.00611, Accuracy = 99.84%: 100%|██████████| 572/572 [02:12<00:00,  4.32it/s] \n",
      "[31 / 50]   Val: Loss = 0.26598, Accuracy = 94.77%: 100%|██████████| 13/13 [00:04<00:00,  3.60it/s]\n",
      "[32 / 50] Train: Loss = 0.00601, Accuracy = 99.84%: 100%|██████████| 572/572 [02:16<00:00,  4.20it/s] \n",
      "[32 / 50]   Val: Loss = 0.26199, Accuracy = 94.96%: 100%|██████████| 13/13 [00:04<00:00,  3.12it/s]\n",
      "[33 / 50] Train: Loss = 0.00596, Accuracy = 99.84%: 100%|██████████| 572/572 [02:17<00:00,  4.17it/s] \n",
      "[33 / 50]   Val: Loss = 0.26245, Accuracy = 94.84%: 100%|██████████| 13/13 [00:04<00:00,  2.82it/s]\n",
      "[34 / 50] Train: Loss = 0.00631, Accuracy = 99.82%: 100%|██████████| 572/572 [02:18<00:00,  4.13it/s] \n",
      "[34 / 50]   Val: Loss = 0.26401, Accuracy = 94.87%: 100%|██████████| 13/13 [00:04<00:00,  2.72it/s]\n",
      "[35 / 50] Train: Loss = 0.00645, Accuracy = 99.82%: 100%|██████████| 572/572 [02:18<00:00,  4.14it/s] \n",
      "[35 / 50]   Val: Loss = 0.26032, Accuracy = 94.95%: 100%|██████████| 13/13 [00:04<00:00,  3.24it/s]\n",
      "[36 / 50] Train: Loss = 0.00598, Accuracy = 99.83%: 100%|██████████| 572/572 [02:18<00:00,  4.14it/s] \n",
      "[36 / 50]   Val: Loss = 0.26409, Accuracy = 94.88%: 100%|██████████| 13/13 [00:04<00:00,  2.68it/s]\n",
      "[37 / 50] Train: Loss = 0.00573, Accuracy = 99.84%: 100%|██████████| 572/572 [02:17<00:00,  4.15it/s] \n",
      "[37 / 50]   Val: Loss = 0.26367, Accuracy = 94.99%: 100%|██████████| 13/13 [00:04<00:00,  2.98it/s]\n",
      "[38 / 50] Train: Loss = 0.00557, Accuracy = 99.85%: 100%|██████████| 572/572 [02:18<00:00,  5.50it/s] \n",
      "[38 / 50]   Val: Loss = 0.26788, Accuracy = 95.00%: 100%|██████████| 13/13 [00:04<00:00,  2.93it/s]\n",
      "[39 / 50] Train: Loss = 0.00557, Accuracy = 99.85%: 100%|██████████| 572/572 [02:19<00:00,  4.11it/s] \n",
      "[39 / 50]   Val: Loss = 0.26666, Accuracy = 95.00%: 100%|██████████| 13/13 [00:04<00:00,  3.72it/s]\n",
      "[40 / 50] Train: Loss = 0.00552, Accuracy = 99.85%: 100%|██████████| 572/572 [02:17<00:00,  4.17it/s] \n",
      "[40 / 50]   Val: Loss = 0.27185, Accuracy = 94.91%: 100%|██████████| 13/13 [00:04<00:00,  3.36it/s]\n",
      "[41 / 50] Train: Loss = 0.00556, Accuracy = 99.84%: 100%|██████████| 572/572 [02:14<00:00,  4.24it/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[41 / 50]   Val: Loss = 0.27230, Accuracy = 94.91%: 100%|██████████| 13/13 [00:04<00:00,  3.14it/s]\n",
      "[42 / 50] Train: Loss = 0.00623, Accuracy = 99.81%: 100%|██████████| 572/572 [02:14<00:00,  4.25it/s] \n",
      "[42 / 50]   Val: Loss = 0.28400, Accuracy = 94.66%: 100%|██████████| 13/13 [00:04<00:00,  3.04it/s]\n",
      "[43 / 50] Train: Loss = 0.00672, Accuracy = 99.79%: 100%|██████████| 572/572 [02:14<00:00,  4.26it/s] \n",
      "[43 / 50]   Val: Loss = 0.27384, Accuracy = 94.87%: 100%|██████████| 13/13 [00:04<00:00,  3.09it/s]\n",
      "[44 / 50] Train: Loss = 0.00559, Accuracy = 99.84%: 100%|██████████| 572/572 [02:13<00:00,  4.29it/s] \n",
      "[44 / 50]   Val: Loss = 0.27688, Accuracy = 94.91%: 100%|██████████| 13/13 [00:04<00:00,  3.35it/s]\n",
      "[45 / 50] Train: Loss = 0.00532, Accuracy = 99.85%: 100%|██████████| 572/572 [02:14<00:00,  4.25it/s] \n",
      "[45 / 50]   Val: Loss = 0.27404, Accuracy = 95.00%: 100%|██████████| 13/13 [00:04<00:00,  3.16it/s]\n",
      "[46 / 50] Train: Loss = 0.00532, Accuracy = 99.85%: 100%|██████████| 572/572 [02:15<00:00,  4.23it/s] \n",
      "[46 / 50]   Val: Loss = 0.27606, Accuracy = 94.92%: 100%|██████████| 13/13 [00:04<00:00,  3.17it/s]\n",
      "[47 / 50] Train: Loss = 0.00524, Accuracy = 99.85%: 100%|██████████| 572/572 [02:14<00:00,  4.24it/s] \n",
      "[47 / 50]   Val: Loss = 0.27788, Accuracy = 94.97%: 100%|██████████| 13/13 [00:04<00:00,  3.14it/s]\n",
      "[48 / 50] Train: Loss = 0.00527, Accuracy = 99.85%: 100%|██████████| 572/572 [02:14<00:00,  4.25it/s] \n",
      "[48 / 50]   Val: Loss = 0.28235, Accuracy = 94.86%: 100%|██████████| 13/13 [00:04<00:00,  3.33it/s]\n",
      "[49 / 50] Train: Loss = 0.00530, Accuracy = 99.85%: 100%|██████████| 572/572 [02:15<00:00,  4.21it/s] \n",
      "[49 / 50]   Val: Loss = 0.28124, Accuracy = 94.90%: 100%|██████████| 13/13 [00:04<00:00,  3.29it/s]\n",
      "[50 / 50] Train: Loss = 0.00540, Accuracy = 99.85%: 100%|██████████| 572/572 [02:14<00:00,  4.26it/s] \n",
      "[50 / 50]   Val: Loss = 0.28926, Accuracy = 94.80%: 100%|██████████| 13/13 [00:04<00:00,  3.24it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98wr38_rw55D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: Loss = 0.29037, Accuracy = 94.79%: 100%|██████████| 28/28 [00:10<00:00,  2.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2903704898697989, 0.9479118253582999)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc = do_epoch(model, criterion, (X_test, y_test), 512, None, 'Test:')\n",
    "test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional=True)\n",
    "        self._out_layer = nn.Linear(2 * lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        emb = self._emb(inputs)\n",
    "        output, _ = self._lstm(emb)\n",
    "        out = self._out_layer(output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.58426, Accuracy = 81.58%: 100%|██████████| 572/572 [03:26<00:00,  3.35it/s]\n",
      "[1 / 50]   Val: Loss = 0.31297, Accuracy = 89.58%: 100%|██████████| 13/13 [00:08<00:00,  1.57it/s]\n",
      "[2 / 50] Train: Loss = 0.22314, Accuracy = 92.87%: 100%|██████████| 572/572 [03:36<00:00,  3.45it/s]\n",
      "[2 / 50]   Val: Loss = 0.21825, Accuracy = 92.81%: 100%|██████████| 13/13 [00:08<00:00,  1.78it/s]\n",
      "[3 / 50] Train: Loss = 0.13984, Accuracy = 95.50%: 100%|██████████| 572/572 [03:33<00:00,  3.41it/s]\n",
      "[3 / 50]   Val: Loss = 0.18930, Accuracy = 93.91%: 100%|██████████| 13/13 [00:08<00:00,  1.49it/s]\n",
      "[4 / 50] Train: Loss = 0.09314, Accuracy = 96.91%: 100%|██████████| 572/572 [03:33<00:00,  3.65it/s]\n",
      "[4 / 50]   Val: Loss = 0.17003, Accuracy = 94.56%: 100%|██████████| 13/13 [00:08<00:00,  1.73it/s]\n",
      "[5 / 50] Train: Loss = 0.06348, Accuracy = 97.82%: 100%|██████████| 572/572 [03:34<00:00,  2.67it/s]\n",
      "[5 / 50]   Val: Loss = 0.14889, Accuracy = 95.25%: 100%|██████████| 13/13 [00:08<00:00,  1.63it/s]\n",
      "[6 / 50] Train: Loss = 0.04288, Accuracy = 98.50%: 100%|██████████| 572/572 [03:31<00:00,  3.36it/s]\n",
      "[6 / 50]   Val: Loss = 0.16199, Accuracy = 95.15%: 100%|██████████| 13/13 [00:08<00:00,  1.82it/s]\n",
      "[7 / 50] Train: Loss = 0.02918, Accuracy = 98.95%: 100%|██████████| 572/572 [04:46<00:00,  3.12it/s]  \n",
      "[7 / 50]   Val: Loss = 0.16414, Accuracy = 95.22%: 100%|██████████| 13/13 [00:08<00:00,  1.57it/s]\n",
      "[8 / 50] Train: Loss = 0.01937, Accuracy = 99.32%: 100%|██████████| 572/572 [03:48<00:00,  3.86it/s]\n",
      "[8 / 50]   Val: Loss = 0.16717, Accuracy = 95.41%: 100%|██████████| 13/13 [00:09<00:00,  1.37it/s]\n",
      "[9 / 50] Train: Loss = 0.01313, Accuracy = 99.54%: 100%|██████████| 572/572 [03:47<00:00,  4.02it/s] \n",
      "[9 / 50]   Val: Loss = 0.19179, Accuracy = 95.20%: 100%|██████████| 13/13 [00:08<00:00,  1.81it/s]\n",
      "[10 / 50] Train: Loss = 0.00869, Accuracy = 99.72%: 100%|██████████| 572/572 [03:44<00:00,  3.64it/s] \n",
      "[10 / 50]   Val: Loss = 0.18897, Accuracy = 95.32%: 100%|██████████| 13/13 [00:08<00:00,  1.91it/s]\n",
      "[11 / 50] Train: Loss = 0.00582, Accuracy = 99.83%: 100%|██████████| 572/572 [03:44<00:00,  2.82it/s] \n",
      "[11 / 50]   Val: Loss = 0.20090, Accuracy = 95.33%: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n",
      "[12 / 50] Train: Loss = 0.00360, Accuracy = 99.91%: 100%|██████████| 572/572 [03:41<00:00,  3.35it/s] \n",
      "[12 / 50]   Val: Loss = 0.20737, Accuracy = 95.40%: 100%|██████████| 13/13 [00:08<00:00,  1.62it/s]\n",
      "[13 / 50] Train: Loss = 0.00263, Accuracy = 99.94%: 100%|██████████| 572/572 [03:36<00:00,  2.64it/s] \n",
      "[13 / 50]   Val: Loss = 0.21844, Accuracy = 95.41%: 100%|██████████| 13/13 [00:08<00:00,  1.82it/s]\n",
      "[14 / 50] Train: Loss = 0.00185, Accuracy = 99.97%: 100%|██████████| 572/572 [03:36<00:00,  2.97it/s] \n",
      "[14 / 50]   Val: Loss = 0.22476, Accuracy = 95.47%: 100%|██████████| 13/13 [00:08<00:00,  1.61it/s]\n",
      "[15 / 50] Train: Loss = 0.00113, Accuracy = 99.99%: 100%|██████████| 572/572 [03:35<00:00,  3.41it/s] \n",
      "[15 / 50]   Val: Loss = 0.23165, Accuracy = 95.44%: 100%|██████████| 13/13 [00:08<00:00,  1.78it/s]\n",
      "[16 / 50] Train: Loss = 0.00101, Accuracy = 99.99%: 100%|██████████| 572/572 [03:37<00:00,  3.08it/s] \n",
      "[16 / 50]   Val: Loss = 0.24451, Accuracy = 95.33%: 100%|██████████| 13/13 [00:08<00:00,  1.41it/s]\n",
      "[17 / 50] Train: Loss = 0.00417, Accuracy = 99.84%: 100%|██████████| 572/572 [03:37<00:00,  2.63it/s] \n",
      "[17 / 50]   Val: Loss = 0.26251, Accuracy = 95.18%: 100%|██████████| 13/13 [00:08<00:00,  1.79it/s]\n",
      "[18 / 50] Train: Loss = 0.00173, Accuracy = 99.95%: 100%|██████████| 572/572 [03:34<00:00,  3.89it/s] \n",
      "[18 / 50]   Val: Loss = 0.27023, Accuracy = 95.30%: 100%|██████████| 13/13 [00:08<00:00,  1.96it/s]\n",
      "[19 / 50] Train: Loss = 0.00082, Accuracy = 99.99%: 100%|██████████| 572/572 [03:34<00:00,  3.99it/s] \n",
      "[19 / 50]   Val: Loss = 0.27640, Accuracy = 95.38%: 100%|██████████| 13/13 [00:08<00:00,  1.93it/s]\n",
      "[20 / 50] Train: Loss = 0.00045, Accuracy = 100.00%: 100%|██████████| 572/572 [03:37<00:00,  3.21it/s]\n",
      "[20 / 50]   Val: Loss = 0.26438, Accuracy = 95.49%: 100%|██████████| 13/13 [00:09<00:00,  1.53it/s]\n",
      "[21 / 50] Train: Loss = 0.00033, Accuracy = 100.00%: 100%|██████████| 572/572 [1:19:24<00:00,  3.85it/s]      \n",
      "[21 / 50]   Val: Loss = 0.27551, Accuracy = 95.48%: 100%|██████████| 13/13 [00:08<00:00,  1.67it/s]\n",
      "[22 / 50] Train: Loss = 0.00038, Accuracy = 100.00%: 100%|██████████| 572/572 [03:40<00:00,  3.39it/s]\n",
      "[22 / 50]   Val: Loss = 0.28362, Accuracy = 95.46%: 100%|██████████| 13/13 [00:08<00:00,  2.03it/s]\n",
      "[23 / 50] Train: Loss = 0.00375, Accuracy = 99.86%: 100%|██████████| 572/572 [03:38<00:00,  3.73it/s] \n",
      "[23 / 50]   Val: Loss = 0.28828, Accuracy = 95.27%: 100%|██████████| 13/13 [00:08<00:00,  1.99it/s]\n",
      "[24 / 50] Train: Loss = 0.00155, Accuracy = 99.95%: 100%|██████████| 572/572 [03:37<00:00,  3.99it/s] \n",
      "[24 / 50]   Val: Loss = 0.28394, Accuracy = 95.49%: 100%|██████████| 13/13 [00:08<00:00,  1.85it/s]\n",
      "[25 / 50] Train: Loss = 0.00043, Accuracy = 99.99%: 100%|██████████| 572/572 [03:40<00:00,  3.46it/s] \n",
      "[25 / 50]   Val: Loss = 0.28238, Accuracy = 95.57%: 100%|██████████| 13/13 [00:08<00:00,  1.69it/s]\n",
      "[26 / 50] Train: Loss = 0.00025, Accuracy = 100.00%: 100%|██████████| 572/572 [03:39<00:00,  3.34it/s]\n",
      "[26 / 50]   Val: Loss = 0.28981, Accuracy = 95.51%: 100%|██████████| 13/13 [00:08<00:00,  1.74it/s]\n",
      "[27 / 50] Train: Loss = 0.00017, Accuracy = 100.00%: 100%|██████████| 572/572 [03:36<00:00,  3.81it/s]\n",
      "[27 / 50]   Val: Loss = 0.28556, Accuracy = 95.60%: 100%|██████████| 13/13 [00:08<00:00,  1.68it/s]\n",
      "[28 / 50] Train: Loss = 0.00022, Accuracy = 100.00%: 100%|██████████| 572/572 [03:36<00:00,  3.52it/s]\n",
      "[28 / 50]   Val: Loss = 0.29259, Accuracy = 95.57%: 100%|██████████| 13/13 [00:08<00:00,  1.61it/s]\n",
      "[29 / 50] Train: Loss = 0.00016, Accuracy = 100.00%: 100%|██████████| 572/572 [04:08<00:00,  3.91it/s]\n",
      "[29 / 50]   Val: Loss = 0.28549, Accuracy = 95.65%: 100%|██████████| 13/13 [00:08<00:00,  1.53it/s]\n",
      "[30 / 50] Train: Loss = 0.00017, Accuracy = 100.00%: 100%|██████████| 572/572 [03:36<00:00,  3.62it/s]\n",
      "[30 / 50]   Val: Loss = 0.29866, Accuracy = 95.59%: 100%|██████████| 13/13 [00:08<00:00,  1.56it/s]\n",
      "[31 / 50] Train: Loss = 0.00018, Accuracy = 100.00%: 100%|██████████| 572/572 [03:36<00:00,  3.45it/s]\n",
      "[31 / 50]   Val: Loss = 0.29232, Accuracy = 95.60%: 100%|██████████| 13/13 [00:08<00:00,  1.65it/s]\n",
      "[32 / 50] Train: Loss = 0.00546, Accuracy = 99.78%: 100%|██████████| 572/572 [03:38<00:00,  4.13it/s] \n",
      "[32 / 50]   Val: Loss = 0.32119, Accuracy = 95.33%: 100%|██████████| 13/13 [00:08<00:00,  1.66it/s]\n",
      "[33 / 50] Train: Loss = 0.00121, Accuracy = 99.96%: 100%|██████████| 572/572 [03:32<00:00,  3.95it/s] \n",
      "[33 / 50]   Val: Loss = 0.28533, Accuracy = 95.64%: 100%|██████████| 13/13 [00:08<00:00,  1.76it/s]\n",
      "[34 / 50] Train: Loss = 0.00035, Accuracy = 99.99%: 100%|██████████| 572/572 [03:35<00:00,  3.72it/s] \n",
      "[34 / 50]   Val: Loss = 0.29915, Accuracy = 95.57%: 100%|██████████| 13/13 [00:08<00:00,  1.64it/s]\n",
      "[35 / 50] Train: Loss = 0.00018, Accuracy = 100.00%: 100%|██████████| 572/572 [03:36<00:00,  2.64it/s]\n",
      "[35 / 50]   Val: Loss = 0.29815, Accuracy = 95.63%: 100%|██████████| 13/13 [00:08<00:00,  1.52it/s]\n",
      "[36 / 50] Train: Loss = 0.00015, Accuracy = 100.00%: 100%|██████████| 572/572 [03:34<00:00,  3.63it/s]\n",
      "[36 / 50]   Val: Loss = 0.30678, Accuracy = 95.60%: 100%|██████████| 13/13 [00:09<00:00,  1.66it/s]\n",
      "[37 / 50] Train: Loss = 0.00015, Accuracy = 100.00%: 100%|██████████| 572/572 [03:37<00:00,  3.72it/s]\n",
      "[37 / 50]   Val: Loss = 0.31180, Accuracy = 95.61%: 100%|██████████| 13/13 [00:08<00:00,  1.89it/s]\n",
      "[38 / 50] Train: Loss = 0.00016, Accuracy = 100.00%: 100%|██████████| 572/572 [03:36<00:00,  2.65it/s]\n",
      "[38 / 50]   Val: Loss = 0.31222, Accuracy = 95.59%: 100%|██████████| 13/13 [00:08<00:00,  1.56it/s]\n",
      "[39 / 50] Train: Loss = 0.00014, Accuracy = 100.00%: 100%|██████████| 572/572 [03:35<00:00,  2.38it/s]\n",
      "[39 / 50]   Val: Loss = 0.30760, Accuracy = 95.64%: 100%|██████████| 13/13 [00:09<00:00,  1.50it/s]\n",
      "[40 / 50] Train: Loss = 0.00015, Accuracy = 100.00%: 100%|██████████| 572/572 [03:37<00:00,  3.54it/s]\n",
      "[40 / 50]   Val: Loss = 0.31508, Accuracy = 95.62%: 100%|██████████| 13/13 [00:07<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[41 / 50] Train: Loss = 0.00112, Accuracy = 99.96%: 100%|██████████| 572/572 [03:37<00:00,  3.25it/s] \n",
      "[41 / 50]   Val: Loss = 0.34854, Accuracy = 95.03%: 100%|██████████| 13/13 [00:08<00:00,  1.85it/s]\n",
      "[42 / 50] Train: Loss = 0.00432, Accuracy = 99.83%: 100%|██████████| 572/572 [03:37<00:00,  3.36it/s] \n",
      "[42 / 50]   Val: Loss = 0.33752, Accuracy = 95.44%: 100%|██████████| 13/13 [00:08<00:00,  1.70it/s]\n",
      "[43 / 50] Train: Loss = 0.00072, Accuracy = 99.98%: 100%|██████████| 572/572 [03:35<00:00,  3.65it/s] \n",
      "[43 / 50]   Val: Loss = 0.30913, Accuracy = 95.67%: 100%|██████████| 13/13 [00:08<00:00,  1.47it/s]\n",
      "[44 / 50] Train: Loss = 0.00023, Accuracy = 100.00%: 100%|██████████| 572/572 [03:34<00:00,  3.49it/s]\n",
      "[44 / 50]   Val: Loss = 0.32033, Accuracy = 95.65%: 100%|██████████| 13/13 [00:08<00:00,  1.62it/s]\n",
      "[45 / 50] Train: Loss = 0.00016, Accuracy = 100.00%: 100%|██████████| 572/572 [03:35<00:00,  3.90it/s]\n",
      "[45 / 50]   Val: Loss = 0.32320, Accuracy = 95.66%: 100%|██████████| 13/13 [00:08<00:00,  1.66it/s]\n",
      "[46 / 50] Train: Loss = 0.00015, Accuracy = 100.00%: 100%|██████████| 572/572 [03:37<00:00,  3.81it/s]\n",
      "[46 / 50]   Val: Loss = 0.32475, Accuracy = 95.66%: 100%|██████████| 13/13 [00:08<00:00,  1.78it/s]\n",
      "[47 / 50] Train: Loss = 0.00015, Accuracy = 100.00%: 100%|██████████| 572/572 [03:36<00:00,  2.65it/s]\n",
      "[47 / 50]   Val: Loss = 0.33214, Accuracy = 95.65%: 100%|██████████| 13/13 [00:08<00:00,  1.59it/s]\n",
      "[48 / 50] Train: Loss = 0.00015, Accuracy = 100.00%: 100%|██████████| 572/572 [03:38<00:00,  4.01it/s]\n",
      "[48 / 50]   Val: Loss = 0.33465, Accuracy = 95.63%: 100%|██████████| 13/13 [00:08<00:00,  1.63it/s]\n",
      "[49 / 50] Train: Loss = 0.00015, Accuracy = 100.00%: 100%|██████████| 572/572 [03:37<00:00,  3.45it/s]\n",
      "[49 / 50]   Val: Loss = 0.33933, Accuracy = 95.62%: 100%|██████████| 13/13 [00:08<00:00,  1.67it/s]\n",
      "[50 / 50] Train: Loss = 0.00013, Accuracy = 100.00%: 100%|██████████| 572/572 [03:37<00:00,  3.83it/s]\n",
      "[50 / 50]   Val: Loss = 0.34047, Accuracy = 95.66%: 100%|██████████| 13/13 [00:08<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: Loss = 0.34383, Accuracy = 95.71%: 100%|██████████| 28/28 [00:18<00:00,  1.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.34383030235767365, 0.9570797572202198)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc = do_epoch(model, criterion, (X_test, y_test), 512, None, 'Test:')\n",
    "test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsCstxiO03oT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat"
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "#         <create me>\n",
    "        self._emb = nn.Embedding.from_pretrained(torch.from_numpy(embeddings).float())\n",
    "        self._lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional=True)\n",
    "        self._out_layer = nn.Linear(2 * lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "#         <use me>\n",
    "        emb = self._emb(inputs)\n",
    "        output, _ = self._lstm(emb)\n",
    "        out = self._out_layer(output)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.58334, Accuracy = 83.17%: 100%|██████████| 572/572 [01:12<00:00,  7.87it/s]\n",
      "[1 / 50]   Val: Loss = 0.27692, Accuracy = 92.00%: 100%|██████████| 13/13 [00:04<00:00,  3.13it/s]\n",
      "[2 / 50] Train: Loss = 0.20283, Accuracy = 93.99%: 100%|██████████| 572/572 [01:14<00:00,  7.71it/s]\n",
      "[2 / 50]   Val: Loss = 0.19139, Accuracy = 94.33%: 100%|██████████| 13/13 [00:05<00:00,  2.72it/s]\n",
      "[3 / 50] Train: Loss = 0.14431, Accuracy = 95.66%: 100%|██████████| 572/572 [01:13<00:00,  9.55it/s]\n",
      "[3 / 50]   Val: Loss = 0.15683, Accuracy = 95.36%: 100%|██████████| 13/13 [00:04<00:00,  3.01it/s]\n",
      "[4 / 50] Train: Loss = 0.11544, Accuracy = 96.42%: 100%|██████████| 572/572 [01:12<00:00,  7.88it/s]\n",
      "[4 / 50]   Val: Loss = 0.13674, Accuracy = 95.88%: 100%|██████████| 13/13 [00:04<00:00,  2.98it/s]\n",
      "[5 / 50] Train: Loss = 0.09840, Accuracy = 96.90%: 100%|██████████| 572/572 [01:12<00:00,  7.85it/s]\n",
      "[5 / 50]   Val: Loss = 0.12615, Accuracy = 96.18%: 100%|██████████| 13/13 [00:04<00:00,  3.14it/s]\n",
      "[6 / 50] Train: Loss = 0.08681, Accuracy = 97.22%: 100%|██████████| 572/572 [01:13<00:00,  7.82it/s]\n",
      "[6 / 50]   Val: Loss = 0.11851, Accuracy = 96.43%: 100%|██████████| 13/13 [00:04<00:00,  2.58it/s]\n",
      "[7 / 50] Train: Loss = 0.07846, Accuracy = 97.45%: 100%|██████████| 572/572 [01:13<00:00,  7.81it/s]\n",
      "[7 / 50]   Val: Loss = 0.11263, Accuracy = 96.61%: 100%|██████████| 13/13 [00:04<00:00,  2.87it/s]\n",
      "[8 / 50] Train: Loss = 0.07199, Accuracy = 97.62%: 100%|██████████| 572/572 [01:12<00:00,  7.84it/s]\n",
      "[8 / 50]   Val: Loss = 0.11117, Accuracy = 96.60%: 100%|██████████| 13/13 [00:04<00:00,  3.47it/s]\n",
      "[9 / 50] Train: Loss = 0.06655, Accuracy = 97.77%: 100%|██████████| 572/572 [02:57<00:00,  3.21it/s]  \n",
      "[9 / 50]   Val: Loss = 0.10759, Accuracy = 96.72%: 100%|██████████| 13/13 [00:04<00:00,  3.14it/s]\n",
      "[10 / 50] Train: Loss = 0.06217, Accuracy = 97.89%: 100%|██████████| 572/572 [01:12<00:00,  7.84it/s]\n",
      "[10 / 50]   Val: Loss = 0.10590, Accuracy = 96.78%: 100%|██████████| 13/13 [00:04<00:00,  3.33it/s]\n",
      "[11 / 50] Train: Loss = 0.05832, Accuracy = 98.01%: 100%|██████████| 572/572 [01:12<00:00,  7.84it/s]\n",
      "[11 / 50]   Val: Loss = 0.10566, Accuracy = 96.81%: 100%|██████████| 13/13 [00:05<00:00,  2.82it/s]\n",
      "[12 / 50] Train: Loss = 0.05490, Accuracy = 98.11%: 100%|██████████| 572/572 [01:12<00:00,  7.85it/s]\n",
      "[12 / 50]   Val: Loss = 0.10282, Accuracy = 96.87%: 100%|██████████| 13/13 [00:05<00:00,  3.12it/s]\n",
      "[13 / 50] Train: Loss = 0.05197, Accuracy = 98.17%: 100%|██████████| 572/572 [01:13<00:00,  7.80it/s]\n",
      "[13 / 50]   Val: Loss = 0.10231, Accuracy = 96.89%: 100%|██████████| 13/13 [00:04<00:00,  2.90it/s]\n",
      "[14 / 50] Train: Loss = 0.04905, Accuracy = 98.26%: 100%|██████████| 572/572 [01:12<00:00,  7.84it/s]\n",
      "[14 / 50]   Val: Loss = 0.10204, Accuracy = 96.91%: 100%|██████████| 13/13 [00:04<00:00,  3.06it/s]\n",
      "[15 / 50] Train: Loss = 0.04672, Accuracy = 98.33%: 100%|██████████| 572/572 [01:13<00:00,  7.73it/s]\n",
      "[15 / 50]   Val: Loss = 0.10445, Accuracy = 96.91%: 100%|██████████| 13/13 [00:04<00:00,  3.21it/s]\n",
      "[16 / 50] Train: Loss = 0.04424, Accuracy = 98.39%: 100%|██████████| 572/572 [01:13<00:00,  7.82it/s]\n",
      "[16 / 50]   Val: Loss = 0.10277, Accuracy = 96.90%: 100%|██████████| 13/13 [00:04<00:00,  3.44it/s]\n",
      "[17 / 50] Train: Loss = 0.04217, Accuracy = 98.46%: 100%|██████████| 572/572 [01:13<00:00,  7.77it/s]\n",
      "[17 / 50]   Val: Loss = 0.10407, Accuracy = 96.91%: 100%|██████████| 13/13 [00:04<00:00,  3.02it/s]\n",
      "[18 / 50] Train: Loss = 0.04014, Accuracy = 98.52%: 100%|██████████| 572/572 [01:10<00:00,  8.15it/s]\n",
      "[18 / 50]   Val: Loss = 0.10411, Accuracy = 96.91%: 100%|██████████| 13/13 [00:05<00:00,  2.85it/s]\n",
      "[19 / 50] Train: Loss = 0.03846, Accuracy = 98.56%: 100%|██████████| 572/572 [01:10<00:00,  8.16it/s]\n",
      "[19 / 50]   Val: Loss = 0.10580, Accuracy = 96.89%: 100%|██████████| 13/13 [00:04<00:00,  3.10it/s]\n",
      "[20 / 50] Train: Loss = 0.03666, Accuracy = 98.63%: 100%|██████████| 572/572 [01:10<00:00,  8.13it/s]\n",
      "[20 / 50]   Val: Loss = 0.10671, Accuracy = 96.91%: 100%|██████████| 13/13 [00:04<00:00,  2.95it/s]\n",
      "[21 / 50] Train: Loss = 0.03492, Accuracy = 98.69%: 100%|██████████| 572/572 [01:10<00:00,  8.62it/s]\n",
      "[21 / 50]   Val: Loss = 0.10814, Accuracy = 96.91%: 100%|██████████| 13/13 [00:05<00:00,  2.90it/s]\n",
      "[22 / 50] Train: Loss = 0.03347, Accuracy = 98.74%: 100%|██████████| 572/572 [01:10<00:00,  8.12it/s]\n",
      "[22 / 50]   Val: Loss = 0.10795, Accuracy = 96.93%: 100%|██████████| 13/13 [00:04<00:00,  2.72it/s]\n",
      "[23 / 50] Train: Loss = 0.03208, Accuracy = 98.77%: 100%|██████████| 572/572 [01:09<00:00,  8.19it/s]\n",
      "[23 / 50]   Val: Loss = 0.11260, Accuracy = 96.83%: 100%|██████████| 13/13 [00:04<00:00,  2.72it/s]\n",
      "[24 / 50] Train: Loss = 0.03072, Accuracy = 98.82%: 100%|██████████| 572/572 [01:10<00:00,  8.15it/s]\n",
      "[24 / 50]   Val: Loss = 0.10869, Accuracy = 96.95%: 100%|██████████| 13/13 [00:04<00:00,  3.11it/s]\n",
      "[25 / 50] Train: Loss = 0.02921, Accuracy = 98.87%: 100%|██████████| 572/572 [01:10<00:00,  8.16it/s]\n",
      "[25 / 50]   Val: Loss = 0.11142, Accuracy = 96.92%: 100%|██████████| 13/13 [00:04<00:00,  3.12it/s]\n",
      "[26 / 50] Train: Loss = 0.02796, Accuracy = 98.92%: 100%|██████████| 572/572 [01:10<00:00,  8.14it/s]\n",
      "[26 / 50]   Val: Loss = 0.11621, Accuracy = 96.88%: 100%|██████████| 13/13 [00:04<00:00,  3.03it/s]\n",
      "[27 / 50] Train: Loss = 0.02683, Accuracy = 98.96%: 100%|██████████| 572/572 [01:10<00:00,  8.13it/s]\n",
      "[27 / 50]   Val: Loss = 0.11362, Accuracy = 96.89%: 100%|██████████| 13/13 [00:04<00:00,  2.65it/s]\n",
      "[28 / 50] Train: Loss = 0.02574, Accuracy = 98.99%: 100%|██████████| 572/572 [01:10<00:00,  8.12it/s]\n",
      "[28 / 50]   Val: Loss = 0.11436, Accuracy = 96.81%: 100%|██████████| 13/13 [00:04<00:00,  2.72it/s]\n",
      "[29 / 50] Train: Loss = 0.02464, Accuracy = 99.03%: 100%|██████████| 572/572 [01:10<00:00,  8.64it/s]\n",
      "[29 / 50]   Val: Loss = 0.11676, Accuracy = 96.83%: 100%|██████████| 13/13 [00:04<00:00,  3.51it/s]\n",
      "[30 / 50] Train: Loss = 0.02354, Accuracy = 99.08%: 100%|██████████| 572/572 [01:10<00:00, 10.15it/s]\n",
      "[30 / 50]   Val: Loss = 0.11918, Accuracy = 96.82%: 100%|██████████| 13/13 [00:04<00:00,  3.34it/s]\n",
      "[31 / 50] Train: Loss = 0.02253, Accuracy = 99.11%: 100%|██████████| 572/572 [01:09<00:00,  8.18it/s]\n",
      "[31 / 50]   Val: Loss = 0.11813, Accuracy = 96.81%: 100%|██████████| 13/13 [00:04<00:00,  3.36it/s]\n",
      "[32 / 50] Train: Loss = 0.02145, Accuracy = 99.16%: 100%|██████████| 572/572 [01:10<00:00,  9.68it/s]\n",
      "[32 / 50]   Val: Loss = 0.12061, Accuracy = 96.88%: 100%|██████████| 13/13 [00:04<00:00,  3.04it/s]\n",
      "[33 / 50] Train: Loss = 0.02051, Accuracy = 99.21%: 100%|██████████| 572/572 [01:10<00:00,  8.12it/s]\n",
      "[33 / 50]   Val: Loss = 0.12420, Accuracy = 96.80%: 100%|██████████| 13/13 [00:04<00:00,  2.69it/s]\n",
      "[34 / 50] Train: Loss = 0.01970, Accuracy = 99.23%: 100%|██████████| 572/572 [01:10<00:00,  8.06it/s]\n",
      "[34 / 50]   Val: Loss = 0.12451, Accuracy = 96.75%: 100%|██████████| 13/13 [00:04<00:00,  3.42it/s]\n",
      "[35 / 50] Train: Loss = 0.01890, Accuracy = 99.26%: 100%|██████████| 572/572 [01:10<00:00,  8.37it/s] \n",
      "[35 / 50]   Val: Loss = 0.12991, Accuracy = 96.80%: 100%|██████████| 13/13 [00:04<00:00,  2.41it/s]\n",
      "[36 / 50] Train: Loss = 0.01800, Accuracy = 99.30%: 100%|██████████| 572/572 [01:10<00:00,  8.13it/s]\n",
      "[36 / 50]   Val: Loss = 0.12853, Accuracy = 96.76%: 100%|██████████| 13/13 [00:04<00:00,  2.74it/s]\n",
      "[37 / 50] Train: Loss = 0.01701, Accuracy = 99.34%: 100%|██████████| 572/572 [01:10<00:00,  8.13it/s] \n",
      "[37 / 50]   Val: Loss = 0.13016, Accuracy = 96.80%: 100%|██████████| 13/13 [00:04<00:00,  3.11it/s]\n",
      "[38 / 50] Train: Loss = 0.01643, Accuracy = 99.36%: 100%|██████████| 572/572 [01:10<00:00,  8.14it/s]\n",
      "[38 / 50]   Val: Loss = 0.13579, Accuracy = 96.74%: 100%|██████████| 13/13 [00:04<00:00,  2.64it/s]\n",
      "[39 / 50] Train: Loss = 0.01552, Accuracy = 99.39%: 100%|██████████| 572/572 [01:10<00:00,  8.13it/s]\n",
      "[39 / 50]   Val: Loss = 0.13825, Accuracy = 96.67%: 100%|██████████| 13/13 [00:04<00:00,  3.21it/s]\n",
      "[40 / 50] Train: Loss = 0.01481, Accuracy = 99.43%: 100%|██████████| 572/572 [01:10<00:00,  8.13it/s] \n",
      "[40 / 50]   Val: Loss = 0.13914, Accuracy = 96.63%: 100%|██████████| 13/13 [00:05<00:00,  2.62it/s]\n",
      "[41 / 50] Train: Loss = 0.01438, Accuracy = 99.45%: 100%|██████████| 572/572 [01:10<00:00,  9.35it/s] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[41 / 50]   Val: Loss = 0.14127, Accuracy = 96.69%: 100%|██████████| 13/13 [00:04<00:00,  3.28it/s]\n",
      "[42 / 50] Train: Loss = 0.01352, Accuracy = 99.48%: 100%|██████████| 572/572 [01:10<00:00,  9.21it/s] \n",
      "[42 / 50]   Val: Loss = 0.14277, Accuracy = 96.72%: 100%|██████████| 13/13 [00:04<00:00,  3.46it/s]\n",
      "[43 / 50] Train: Loss = 0.01295, Accuracy = 99.51%: 100%|██████████| 572/572 [01:10<00:00,  8.12it/s] \n",
      "[43 / 50]   Val: Loss = 0.14475, Accuracy = 96.65%: 100%|██████████| 13/13 [00:04<00:00,  3.05it/s]\n",
      "[44 / 50] Train: Loss = 0.01246, Accuracy = 99.53%: 100%|██████████| 572/572 [01:10<00:00,  8.16it/s] \n",
      "[44 / 50]   Val: Loss = 0.14792, Accuracy = 96.69%: 100%|██████████| 13/13 [00:04<00:00,  3.30it/s]\n",
      "[45 / 50] Train: Loss = 0.01175, Accuracy = 99.56%: 100%|██████████| 572/572 [01:09<00:00,  8.18it/s] \n",
      "[45 / 50]   Val: Loss = 0.14952, Accuracy = 96.67%: 100%|██████████| 13/13 [00:04<00:00,  3.38it/s]\n",
      "[46 / 50] Train: Loss = 0.01136, Accuracy = 99.57%: 100%|██████████| 572/572 [01:10<00:00,  8.13it/s] \n",
      "[46 / 50]   Val: Loss = 0.15210, Accuracy = 96.68%: 100%|██████████| 13/13 [00:04<00:00,  2.73it/s]\n",
      "[47 / 50] Train: Loss = 0.01077, Accuracy = 99.60%: 100%|██████████| 572/572 [01:10<00:00,  8.05it/s] \n",
      "[47 / 50]   Val: Loss = 0.15467, Accuracy = 96.57%: 100%|██████████| 13/13 [00:04<00:00,  3.53it/s]\n",
      "[48 / 50] Train: Loss = 0.01000, Accuracy = 99.63%: 100%|██████████| 572/572 [01:10<00:00,  8.14it/s] \n",
      "[48 / 50]   Val: Loss = 0.15906, Accuracy = 96.59%: 100%|██████████| 13/13 [00:04<00:00,  3.02it/s]\n",
      "[49 / 50] Train: Loss = 0.00977, Accuracy = 99.64%: 100%|██████████| 572/572 [01:10<00:00,  8.09it/s] \n",
      "[49 / 50]   Val: Loss = 0.15827, Accuracy = 96.59%: 100%|██████████| 13/13 [00:05<00:00,  2.87it/s]\n",
      "[50 / 50] Train: Loss = 0.00927, Accuracy = 99.66%: 100%|██████████| 572/572 [01:10<00:00,  8.17it/s] \n",
      "[50 / 50]   Val: Loss = 0.16327, Accuracy = 96.57%: 100%|██████████| 13/13 [00:05<00:00,  2.57it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPUuAPGhEGVR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: Loss = 0.16442, Accuracy = 96.62%: 100%|██████████| 28/28 [00:10<00:00,  2.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1644236674266202, 0.9661547359614967)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# <calc test accuracy>\n",
    "test_loss, test_acc = do_epoch(model, criterion, (X_test, y_test), 512, None, 'Test:')\n",
    "test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 06 - RNNs, part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
