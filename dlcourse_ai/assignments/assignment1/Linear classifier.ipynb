{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients are different at (0,). Analytic: 1.00000, Numeric: 0.50000\n"
     ]
    }
   ],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, np.array([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), np.array([1]))\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, np.array([1])), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.219017\n",
      "Epoch 1, loss: 0.065851\n",
      "Epoch 2, loss: 0.020561\n",
      "Epoch 3, loss: 0.007310\n",
      "Epoch 4, loss: 0.003426\n",
      "Epoch 5, loss: 0.002230\n",
      "Epoch 6, loss: 0.001933\n",
      "Epoch 7, loss: 0.001859\n",
      "Epoch 8, loss: 0.001776\n",
      "Epoch 9, loss: 0.001811\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11a408b00>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.167\n",
      "Epoch 0, loss: 0.001744\n",
      "Epoch 1, loss: 0.001800\n",
      "Epoch 2, loss: 0.001794\n",
      "Epoch 3, loss: 0.001834\n",
      "Epoch 4, loss: 0.001807\n",
      "Epoch 5, loss: 0.001812\n",
      "Epoch 6, loss: 0.001819\n",
      "Epoch 7, loss: 0.001843\n",
      "Epoch 8, loss: 0.001850\n",
      "Epoch 9, loss: 0.001760\n",
      "Epoch 10, loss: 0.001785\n",
      "Epoch 11, loss: 0.001830\n",
      "Epoch 12, loss: 0.001801\n",
      "Epoch 13, loss: 0.001770\n",
      "Epoch 14, loss: 0.001813\n",
      "Epoch 15, loss: 0.001817\n",
      "Epoch 16, loss: 0.001864\n",
      "Epoch 17, loss: 0.001815\n",
      "Epoch 18, loss: 0.001844\n",
      "Epoch 19, loss: 0.001793\n",
      "Epoch 20, loss: 0.001800\n",
      "Epoch 21, loss: 0.001791\n",
      "Epoch 22, loss: 0.001770\n",
      "Epoch 23, loss: 0.001863\n",
      "Epoch 24, loss: 0.001765\n",
      "Epoch 25, loss: 0.001816\n",
      "Epoch 26, loss: 0.001749\n",
      "Epoch 27, loss: 0.001772\n",
      "Epoch 28, loss: 0.001826\n",
      "Epoch 29, loss: 0.001841\n",
      "Epoch 30, loss: 0.001817\n",
      "Epoch 31, loss: 0.001801\n",
      "Epoch 32, loss: 0.001836\n",
      "Epoch 33, loss: 0.001775\n",
      "Epoch 34, loss: 0.001797\n",
      "Epoch 35, loss: 0.001800\n",
      "Epoch 36, loss: 0.001799\n",
      "Epoch 37, loss: 0.001794\n",
      "Epoch 38, loss: 0.001814\n",
      "Epoch 39, loss: 0.001782\n",
      "Epoch 40, loss: 0.001818\n",
      "Epoch 41, loss: 0.001812\n",
      "Epoch 42, loss: 0.001775\n",
      "Epoch 43, loss: 0.001824\n",
      "Epoch 44, loss: 0.001823\n",
      "Epoch 45, loss: 0.001868\n",
      "Epoch 46, loss: 0.001815\n",
      "Epoch 47, loss: 0.001830\n",
      "Epoch 48, loss: 0.001793\n",
      "Epoch 49, loss: 0.001833\n",
      "Epoch 50, loss: 0.001875\n",
      "Epoch 51, loss: 0.001925\n",
      "Epoch 52, loss: 0.001824\n",
      "Epoch 53, loss: 0.001773\n",
      "Epoch 54, loss: 0.001844\n",
      "Epoch 55, loss: 0.001860\n",
      "Epoch 56, loss: 0.001837\n",
      "Epoch 57, loss: 0.001813\n",
      "Epoch 58, loss: 0.001770\n",
      "Epoch 59, loss: 0.001829\n",
      "Epoch 60, loss: 0.001813\n",
      "Epoch 61, loss: 0.001848\n",
      "Epoch 62, loss: 0.001830\n",
      "Epoch 63, loss: 0.001790\n",
      "Epoch 64, loss: 0.001853\n",
      "Epoch 65, loss: 0.001854\n",
      "Epoch 66, loss: 0.001839\n",
      "Epoch 67, loss: 0.001844\n",
      "Epoch 68, loss: 0.001866\n",
      "Epoch 69, loss: 0.001850\n",
      "Epoch 70, loss: 0.001768\n",
      "Epoch 71, loss: 0.001763\n",
      "Epoch 72, loss: 0.001787\n",
      "Epoch 73, loss: 0.001859\n",
      "Epoch 74, loss: 0.001792\n",
      "Epoch 75, loss: 0.001757\n",
      "Epoch 76, loss: 0.001817\n",
      "Epoch 77, loss: 0.001842\n",
      "Epoch 78, loss: 0.001886\n",
      "Epoch 79, loss: 0.001871\n",
      "Epoch 80, loss: 0.001804\n",
      "Epoch 81, loss: 0.001821\n",
      "Epoch 82, loss: 0.001804\n",
      "Epoch 83, loss: 0.001807\n",
      "Epoch 84, loss: 0.001807\n",
      "Epoch 85, loss: 0.001828\n",
      "Epoch 86, loss: 0.001807\n",
      "Epoch 87, loss: 0.001819\n",
      "Epoch 88, loss: 0.001772\n",
      "Epoch 89, loss: 0.001780\n",
      "Epoch 90, loss: 0.001780\n",
      "Epoch 91, loss: 0.001754\n",
      "Epoch 92, loss: 0.001831\n",
      "Epoch 93, loss: 0.001840\n",
      "Epoch 94, loss: 0.001813\n",
      "Epoch 95, loss: 0.001832\n",
      "Epoch 96, loss: 0.001820\n",
      "Epoch 97, loss: 0.001830\n",
      "Epoch 98, loss: 0.001828\n",
      "Epoch 99, loss: 0.001813\n",
      "Accuracy after training for 100 epochs:  0.17\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate = 0.01, reg_strength = 10.0\n",
      "Epoch 0, loss: 0.002703\n",
      "Epoch 1, loss: 0.003998\n",
      "Epoch 2, loss: 0.004226\n",
      "Epoch 3, loss: 0.003936\n",
      "Epoch 4, loss: 0.003484\n",
      "Epoch 5, loss: 0.003230\n",
      "Epoch 6, loss: 0.002936\n",
      "Epoch 7, loss: 0.003428\n",
      "Epoch 8, loss: 0.003030\n",
      "Epoch 9, loss: 0.004600\n",
      "Epoch 10, loss: 0.003168\n",
      "Epoch 11, loss: 0.003635\n",
      "Epoch 12, loss: 0.004070\n",
      "Epoch 13, loss: 0.003192\n",
      "Epoch 14, loss: 0.002876\n",
      "Epoch 15, loss: 0.003066\n",
      "Epoch 16, loss: 0.003240\n",
      "Epoch 17, loss: 0.003202\n",
      "Epoch 18, loss: 0.003224\n",
      "Epoch 19, loss: 0.003713\n",
      "Epoch 20, loss: 0.003394\n",
      "Epoch 21, loss: 0.003276\n",
      "Epoch 22, loss: 0.003196\n",
      "Epoch 23, loss: 0.002239\n",
      "Epoch 24, loss: 0.004053\n",
      "Epoch 25, loss: 0.003286\n",
      "Epoch 26, loss: 0.003287\n",
      "Epoch 27, loss: 0.004069\n",
      "Epoch 28, loss: 0.003077\n",
      "Epoch 29, loss: 0.003833\n",
      "Epoch 30, loss: 0.003838\n",
      "Epoch 31, loss: 0.002866\n",
      "Epoch 32, loss: 0.003249\n",
      "Epoch 33, loss: 0.003134\n",
      "Epoch 34, loss: 0.003766\n",
      "Epoch 35, loss: 0.003227\n",
      "Epoch 36, loss: 0.003037\n",
      "Epoch 37, loss: 0.003338\n",
      "Epoch 38, loss: 0.003658\n",
      "Epoch 39, loss: 0.002841\n",
      "Epoch 40, loss: 0.004593\n",
      "Epoch 41, loss: 0.004096\n",
      "Epoch 42, loss: 0.003078\n",
      "Epoch 43, loss: 0.002711\n",
      "Epoch 44, loss: 0.003230\n",
      "Epoch 45, loss: 0.004289\n",
      "Epoch 46, loss: 0.003599\n",
      "Epoch 47, loss: 0.004232\n",
      "Epoch 48, loss: 0.004351\n",
      "Epoch 49, loss: 0.003282\n",
      "Epoch 50, loss: 0.003323\n",
      "Epoch 51, loss: 0.003322\n",
      "Epoch 52, loss: 0.003657\n",
      "Epoch 53, loss: 0.003830\n",
      "Epoch 54, loss: 0.002706\n",
      "Epoch 55, loss: 0.002562\n",
      "Epoch 56, loss: 0.003529\n",
      "Epoch 57, loss: 0.003619\n",
      "Epoch 58, loss: 0.003631\n",
      "Epoch 59, loss: 0.003686\n",
      "Epoch 60, loss: 0.002792\n",
      "Epoch 61, loss: 0.002834\n",
      "Epoch 62, loss: 0.003000\n",
      "Epoch 63, loss: 0.003634\n",
      "Epoch 64, loss: 0.003457\n",
      "Epoch 65, loss: 0.003758\n",
      "Epoch 66, loss: 0.003506\n",
      "Epoch 67, loss: 0.004493\n",
      "Epoch 68, loss: 0.003405\n",
      "Epoch 69, loss: 0.003896\n",
      "Epoch 70, loss: 0.003471\n",
      "Epoch 71, loss: 0.002922\n",
      "Epoch 72, loss: 0.002779\n",
      "Epoch 73, loss: 0.003166\n",
      "Epoch 74, loss: 0.004240\n",
      "Epoch 75, loss: 0.002854\n",
      "Epoch 76, loss: 0.004808\n",
      "Epoch 77, loss: 0.003034\n",
      "Epoch 78, loss: 0.003162\n",
      "Epoch 79, loss: 0.002804\n",
      "Epoch 80, loss: 0.002549\n",
      "Epoch 81, loss: 0.002932\n",
      "Epoch 82, loss: 0.002586\n",
      "Epoch 83, loss: 0.003033\n",
      "Epoch 84, loss: 0.003170\n",
      "Epoch 85, loss: 0.002950\n",
      "Epoch 86, loss: 0.003750\n",
      "Epoch 87, loss: 0.003415\n",
      "Epoch 88, loss: 0.003174\n",
      "Epoch 89, loss: 0.003556\n",
      "Epoch 90, loss: 0.003802\n",
      "Epoch 91, loss: 0.003686\n",
      "Epoch 92, loss: 0.002770\n",
      "Epoch 93, loss: 0.006755\n",
      "Epoch 94, loss: 0.003722\n",
      "Epoch 95, loss: 0.003779\n",
      "Epoch 96, loss: 0.002717\n",
      "Epoch 97, loss: 0.003701\n",
      "Epoch 98, loss: 0.003952\n",
      "Epoch 99, loss: 0.003473\n",
      "accuracy = 0.11\n",
      "learning_rate = 0.01, reg_strength = 1\n",
      "Epoch 0, loss: 0.031050\n",
      "Epoch 1, loss: 0.017127\n",
      "Epoch 2, loss: 0.013839\n",
      "Epoch 3, loss: 0.013311\n",
      "Epoch 4, loss: 0.013870\n",
      "Epoch 5, loss: 0.014135\n",
      "Epoch 6, loss: 0.014845\n",
      "Epoch 7, loss: 0.014694\n",
      "Epoch 8, loss: 0.015371\n",
      "Epoch 9, loss: 0.015075\n",
      "Epoch 10, loss: 0.014822\n",
      "Epoch 11, loss: 0.015110\n",
      "Epoch 12, loss: 0.015345\n",
      "Epoch 13, loss: 0.015042\n",
      "Epoch 14, loss: 0.015433\n",
      "Epoch 15, loss: 0.015254\n",
      "Epoch 16, loss: 0.015137\n",
      "Epoch 17, loss: 0.015274\n",
      "Epoch 18, loss: 0.015294\n",
      "Epoch 19, loss: 0.015331\n",
      "Epoch 20, loss: 0.015191\n",
      "Epoch 21, loss: 0.015508\n",
      "Epoch 22, loss: 0.015552\n",
      "Epoch 23, loss: 0.015465\n",
      "Epoch 24, loss: 0.015297\n",
      "Epoch 25, loss: 0.014839\n",
      "Epoch 26, loss: 0.015332\n",
      "Epoch 27, loss: 0.014866\n",
      "Epoch 28, loss: 0.015189\n",
      "Epoch 29, loss: 0.015097\n",
      "Epoch 30, loss: 0.015356\n",
      "Epoch 31, loss: 0.015319\n",
      "Epoch 32, loss: 0.014885\n",
      "Epoch 33, loss: 0.015915\n",
      "Epoch 34, loss: 0.015210\n",
      "Epoch 35, loss: 0.015123\n",
      "Epoch 36, loss: 0.015130\n",
      "Epoch 37, loss: 0.015214\n",
      "Epoch 38, loss: 0.015051\n",
      "Epoch 39, loss: 0.015235\n",
      "Epoch 40, loss: 0.015066\n",
      "Epoch 41, loss: 0.015370\n",
      "Epoch 42, loss: 0.014991\n",
      "Epoch 43, loss: 0.015361\n",
      "Epoch 44, loss: 0.015273\n",
      "Epoch 45, loss: 0.015260\n",
      "Epoch 46, loss: 0.014983\n",
      "Epoch 47, loss: 0.015225\n",
      "Epoch 48, loss: 0.015133\n",
      "Epoch 49, loss: 0.014915\n",
      "Epoch 50, loss: 0.015428\n",
      "Epoch 51, loss: 0.015367\n",
      "Epoch 52, loss: 0.015268\n",
      "Epoch 53, loss: 0.015444\n",
      "Epoch 54, loss: 0.015113\n",
      "Epoch 55, loss: 0.014994\n",
      "Epoch 56, loss: 0.014967\n",
      "Epoch 57, loss: 0.015139\n",
      "Epoch 58, loss: 0.015600\n",
      "Epoch 59, loss: 0.015564\n",
      "Epoch 60, loss: 0.015066\n",
      "Epoch 61, loss: 0.015136\n",
      "Epoch 62, loss: 0.015171\n",
      "Epoch 63, loss: 0.015296\n",
      "Epoch 64, loss: 0.015253\n",
      "Epoch 65, loss: 0.015306\n",
      "Epoch 66, loss: 0.015378\n",
      "Epoch 67, loss: 0.015087\n",
      "Epoch 68, loss: 0.015062\n",
      "Epoch 69, loss: 0.015072\n",
      "Epoch 70, loss: 0.015331\n",
      "Epoch 71, loss: 0.015026\n",
      "Epoch 72, loss: 0.015237\n",
      "Epoch 73, loss: 0.015348\n",
      "Epoch 74, loss: 0.015647\n",
      "Epoch 75, loss: 0.015234\n",
      "Epoch 76, loss: 0.015243\n",
      "Epoch 77, loss: 0.015010\n",
      "Epoch 78, loss: 0.015206\n",
      "Epoch 79, loss: 0.015622\n",
      "Epoch 80, loss: 0.015502\n",
      "Epoch 81, loss: 0.014965\n",
      "Epoch 82, loss: 0.015364\n",
      "Epoch 83, loss: 0.015125\n",
      "Epoch 84, loss: 0.014985\n",
      "Epoch 85, loss: 0.015159\n",
      "Epoch 86, loss: 0.015211\n",
      "Epoch 87, loss: 0.014890\n",
      "Epoch 88, loss: 0.015264\n",
      "Epoch 89, loss: 0.015586\n",
      "Epoch 90, loss: 0.014984\n",
      "Epoch 91, loss: 0.015302\n",
      "Epoch 92, loss: 0.015434\n",
      "Epoch 93, loss: 0.015448\n",
      "Epoch 94, loss: 0.015076\n",
      "Epoch 95, loss: 0.015272\n",
      "Epoch 96, loss: 0.014836\n",
      "Epoch 97, loss: 0.015057\n",
      "Epoch 98, loss: 0.015349\n",
      "Epoch 99, loss: 0.015065\n",
      "accuracy = 0.16277777777777777\n",
      "learning_rate = 0.01, reg_strength = 0.1\n",
      "Epoch 0, loss: 0.006842\n",
      "Epoch 1, loss: 0.007220\n",
      "Epoch 2, loss: 0.008051\n",
      "Epoch 3, loss: 0.009250\n",
      "Epoch 4, loss: 0.010758\n",
      "Epoch 5, loss: 0.012514\n",
      "Epoch 6, loss: 0.014522\n",
      "Epoch 7, loss: 0.016294\n",
      "Epoch 8, loss: 0.018398\n",
      "Epoch 9, loss: 0.020661\n",
      "Epoch 10, loss: 0.022481\n",
      "Epoch 11, loss: 0.024630\n",
      "Epoch 12, loss: 0.026755\n",
      "Epoch 13, loss: 0.028606\n",
      "Epoch 14, loss: 0.030310\n",
      "Epoch 15, loss: 0.032690\n",
      "Epoch 16, loss: 0.034756\n",
      "Epoch 17, loss: 0.036437\n",
      "Epoch 18, loss: 0.038052\n",
      "Epoch 19, loss: 0.039793\n",
      "Epoch 20, loss: 0.041174\n",
      "Epoch 21, loss: 0.042690\n",
      "Epoch 22, loss: 0.043305\n",
      "Epoch 23, loss: 0.045136\n",
      "Epoch 24, loss: 0.046234\n",
      "Epoch 25, loss: 0.048208\n",
      "Epoch 26, loss: 0.048833\n",
      "Epoch 27, loss: 0.050108\n",
      "Epoch 28, loss: 0.051017\n",
      "Epoch 29, loss: 0.052418\n",
      "Epoch 30, loss: 0.053504\n",
      "Epoch 31, loss: 0.053673\n",
      "Epoch 32, loss: 0.055104\n",
      "Epoch 33, loss: 0.055904\n",
      "Epoch 34, loss: 0.057002\n",
      "Epoch 35, loss: 0.056643\n",
      "Epoch 36, loss: 0.057695\n",
      "Epoch 37, loss: 0.058084\n",
      "Epoch 38, loss: 0.059293\n",
      "Epoch 39, loss: 0.059532\n",
      "Epoch 40, loss: 0.059515\n",
      "Epoch 41, loss: 0.060319\n",
      "Epoch 42, loss: 0.061040\n",
      "Epoch 43, loss: 0.061300\n",
      "Epoch 44, loss: 0.061612\n",
      "Epoch 45, loss: 0.062989\n",
      "Epoch 46, loss: 0.062632\n",
      "Epoch 47, loss: 0.062539\n",
      "Epoch 48, loss: 0.062919\n",
      "Epoch 49, loss: 0.064008\n",
      "Epoch 50, loss: 0.063052\n",
      "Epoch 51, loss: 0.064095\n",
      "Epoch 52, loss: 0.064398\n",
      "Epoch 53, loss: 0.065335\n",
      "Epoch 54, loss: 0.065035\n",
      "Epoch 55, loss: 0.065271\n",
      "Epoch 56, loss: 0.064885\n",
      "Epoch 57, loss: 0.065777\n",
      "Epoch 58, loss: 0.065318\n",
      "Epoch 59, loss: 0.065314\n",
      "Epoch 60, loss: 0.065995\n",
      "Epoch 61, loss: 0.065936\n",
      "Epoch 62, loss: 0.066097\n",
      "Epoch 63, loss: 0.066291\n",
      "Epoch 64, loss: 0.066787\n",
      "Epoch 65, loss: 0.066308\n",
      "Epoch 66, loss: 0.067179\n",
      "Epoch 67, loss: 0.066889\n",
      "Epoch 68, loss: 0.066553\n",
      "Epoch 69, loss: 0.067544\n",
      "Epoch 70, loss: 0.066991\n",
      "Epoch 71, loss: 0.067220\n",
      "Epoch 72, loss: 0.066443\n",
      "Epoch 73, loss: 0.067556\n",
      "Epoch 74, loss: 0.066697\n",
      "Epoch 75, loss: 0.067140\n",
      "Epoch 76, loss: 0.067445\n",
      "Epoch 77, loss: 0.067861\n",
      "Epoch 78, loss: 0.067540\n",
      "Epoch 79, loss: 0.066626\n",
      "Epoch 80, loss: 0.067665\n",
      "Epoch 81, loss: 0.068174\n",
      "Epoch 82, loss: 0.067363\n",
      "Epoch 83, loss: 0.067889\n",
      "Epoch 84, loss: 0.067790\n",
      "Epoch 85, loss: 0.067270\n",
      "Epoch 86, loss: 0.067775\n",
      "Epoch 87, loss: 0.066484\n",
      "Epoch 88, loss: 0.067993\n",
      "Epoch 89, loss: 0.068128\n",
      "Epoch 90, loss: 0.067822\n",
      "Epoch 91, loss: 0.067906\n",
      "Epoch 92, loss: 0.068074\n",
      "Epoch 93, loss: 0.068077\n",
      "Epoch 94, loss: 0.068315\n",
      "Epoch 95, loss: 0.068307\n",
      "Epoch 96, loss: 0.068318\n",
      "Epoch 97, loss: 0.068043\n",
      "Epoch 98, loss: 0.067963\n",
      "Epoch 99, loss: 0.068194\n",
      "accuracy = 0.19111111111111112\n",
      "learning_rate = 0.01, reg_strength = 0.01\n",
      "Epoch 0, loss: 0.000741\n",
      "Epoch 1, loss: 0.000848\n",
      "Epoch 2, loss: 0.001000\n",
      "Epoch 3, loss: 0.001216\n",
      "Epoch 4, loss: 0.001463\n",
      "Epoch 5, loss: 0.001765\n",
      "Epoch 6, loss: 0.002091\n",
      "Epoch 7, loss: 0.002456\n",
      "Epoch 8, loss: 0.002868\n",
      "Epoch 9, loss: 0.003272\n",
      "Epoch 10, loss: 0.003715\n",
      "Epoch 11, loss: 0.004160\n",
      "Epoch 12, loss: 0.004691\n",
      "Epoch 13, loss: 0.005140\n",
      "Epoch 14, loss: 0.005625\n",
      "Epoch 15, loss: 0.006201\n",
      "Epoch 16, loss: 0.006769\n",
      "Epoch 17, loss: 0.007204\n",
      "Epoch 18, loss: 0.007916\n",
      "Epoch 19, loss: 0.008406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, loss: 0.008898\n",
      "Epoch 21, loss: 0.009474\n",
      "Epoch 22, loss: 0.010000\n",
      "Epoch 23, loss: 0.010635\n",
      "Epoch 24, loss: 0.011206\n",
      "Epoch 25, loss: 0.011698\n",
      "Epoch 26, loss: 0.012521\n",
      "Epoch 27, loss: 0.012893\n",
      "Epoch 28, loss: 0.013681\n",
      "Epoch 29, loss: 0.014079\n",
      "Epoch 30, loss: 0.014604\n",
      "Epoch 31, loss: 0.015493\n",
      "Epoch 32, loss: 0.015953\n",
      "Epoch 33, loss: 0.016316\n",
      "Epoch 34, loss: 0.017158\n",
      "Epoch 35, loss: 0.017404\n",
      "Epoch 36, loss: 0.018359\n",
      "Epoch 37, loss: 0.018693\n",
      "Epoch 38, loss: 0.019271\n",
      "Epoch 39, loss: 0.019901\n",
      "Epoch 40, loss: 0.020375\n",
      "Epoch 41, loss: 0.020917\n",
      "Epoch 42, loss: 0.021474\n",
      "Epoch 43, loss: 0.022074\n",
      "Epoch 44, loss: 0.022619\n",
      "Epoch 45, loss: 0.022895\n",
      "Epoch 46, loss: 0.023666\n",
      "Epoch 47, loss: 0.023677\n",
      "Epoch 48, loss: 0.024669\n",
      "Epoch 49, loss: 0.025128\n",
      "Epoch 50, loss: 0.025759\n",
      "Epoch 51, loss: 0.026161\n",
      "Epoch 52, loss: 0.026177\n",
      "Epoch 53, loss: 0.026811\n",
      "Epoch 54, loss: 0.027449\n",
      "Epoch 55, loss: 0.028232\n",
      "Epoch 56, loss: 0.028418\n",
      "Epoch 57, loss: 0.028789\n",
      "Epoch 58, loss: 0.029650\n",
      "Epoch 59, loss: 0.030485\n",
      "Epoch 60, loss: 0.030235\n",
      "Epoch 61, loss: 0.031090\n",
      "Epoch 62, loss: 0.031048\n",
      "Epoch 63, loss: 0.031346\n",
      "Epoch 64, loss: 0.031256\n",
      "Epoch 65, loss: 0.033217\n",
      "Epoch 66, loss: 0.033092\n",
      "Epoch 67, loss: 0.033506\n",
      "Epoch 68, loss: 0.033690\n",
      "Epoch 69, loss: 0.033845\n",
      "Epoch 70, loss: 0.034688\n",
      "Epoch 71, loss: 0.034962\n",
      "Epoch 72, loss: 0.035661\n",
      "Epoch 73, loss: 0.036050\n",
      "Epoch 74, loss: 0.036254\n",
      "Epoch 75, loss: 0.037211\n",
      "Epoch 76, loss: 0.037503\n",
      "Epoch 77, loss: 0.037485\n",
      "Epoch 78, loss: 0.037706\n",
      "Epoch 79, loss: 0.038820\n",
      "Epoch 80, loss: 0.038288\n",
      "Epoch 81, loss: 0.038802\n",
      "Epoch 82, loss: 0.039370\n",
      "Epoch 83, loss: 0.039090\n",
      "Epoch 84, loss: 0.039711\n",
      "Epoch 85, loss: 0.039773\n",
      "Epoch 86, loss: 0.040047\n",
      "Epoch 87, loss: 0.040979\n",
      "Epoch 88, loss: 0.039861\n",
      "Epoch 89, loss: 0.041910\n",
      "Epoch 90, loss: 0.041284\n",
      "Epoch 91, loss: 0.042720\n",
      "Epoch 92, loss: 0.043063\n",
      "Epoch 93, loss: 0.042777\n",
      "Epoch 94, loss: 0.043507\n",
      "Epoch 95, loss: 0.043480\n",
      "Epoch 96, loss: 0.043603\n",
      "Epoch 97, loss: 0.044735\n",
      "Epoch 98, loss: 0.044719\n",
      "Epoch 99, loss: 0.044867\n",
      "accuracy = 0.20333333333333334\n",
      "learning_rate = 0.01, reg_strength = 0.001\n",
      "Epoch 0, loss: 0.000073\n",
      "Epoch 1, loss: 0.000083\n",
      "Epoch 2, loss: 0.000101\n",
      "Epoch 3, loss: 0.000122\n",
      "Epoch 4, loss: 0.000150\n",
      "Epoch 5, loss: 0.000179\n",
      "Epoch 6, loss: 0.000217\n",
      "Epoch 7, loss: 0.000253\n",
      "Epoch 8, loss: 0.000294\n",
      "Epoch 9, loss: 0.000340\n",
      "Epoch 10, loss: 0.000391\n",
      "Epoch 11, loss: 0.000438\n",
      "Epoch 12, loss: 0.000491\n",
      "Epoch 13, loss: 0.000554\n",
      "Epoch 14, loss: 0.000607\n",
      "Epoch 15, loss: 0.000665\n",
      "Epoch 16, loss: 0.000715\n",
      "Epoch 17, loss: 0.000779\n",
      "Epoch 18, loss: 0.000847\n",
      "Epoch 19, loss: 0.000907\n",
      "Epoch 20, loss: 0.000963\n",
      "Epoch 21, loss: 0.001040\n",
      "Epoch 22, loss: 0.001097\n",
      "Epoch 23, loss: 0.001162\n",
      "Epoch 24, loss: 0.001253\n",
      "Epoch 25, loss: 0.001309\n",
      "Epoch 26, loss: 0.001387\n",
      "Epoch 27, loss: 0.001440\n",
      "Epoch 28, loss: 0.001515\n",
      "Epoch 29, loss: 0.001617\n",
      "Epoch 30, loss: 0.001657\n",
      "Epoch 31, loss: 0.001707\n",
      "Epoch 32, loss: 0.001807\n",
      "Epoch 33, loss: 0.001865\n",
      "Epoch 34, loss: 0.001919\n",
      "Epoch 35, loss: 0.002013\n",
      "Epoch 36, loss: 0.002038\n",
      "Epoch 37, loss: 0.002154\n",
      "Epoch 38, loss: 0.002234\n",
      "Epoch 39, loss: 0.002310\n",
      "Epoch 40, loss: 0.002350\n",
      "Epoch 41, loss: 0.002438\n",
      "Epoch 42, loss: 0.002484\n",
      "Epoch 43, loss: 0.002587\n",
      "Epoch 44, loss: 0.002678\n",
      "Epoch 45, loss: 0.002701\n",
      "Epoch 46, loss: 0.002782\n",
      "Epoch 47, loss: 0.002883\n",
      "Epoch 48, loss: 0.002898\n",
      "Epoch 49, loss: 0.002992\n",
      "Epoch 50, loss: 0.003056\n",
      "Epoch 51, loss: 0.003116\n",
      "Epoch 52, loss: 0.003252\n",
      "Epoch 53, loss: 0.003263\n",
      "Epoch 54, loss: 0.003254\n",
      "Epoch 55, loss: 0.003392\n",
      "Epoch 56, loss: 0.003421\n",
      "Epoch 57, loss: 0.003558\n",
      "Epoch 58, loss: 0.003600\n",
      "Epoch 59, loss: 0.003627\n",
      "Epoch 60, loss: 0.003713\n",
      "Epoch 61, loss: 0.003776\n",
      "Epoch 62, loss: 0.003838\n",
      "Epoch 63, loss: 0.003952\n",
      "Epoch 64, loss: 0.004001\n",
      "Epoch 65, loss: 0.004058\n",
      "Epoch 66, loss: 0.004142\n",
      "Epoch 67, loss: 0.004202\n",
      "Epoch 68, loss: 0.004158\n",
      "Epoch 69, loss: 0.004294\n",
      "Epoch 70, loss: 0.004385\n",
      "Epoch 71, loss: 0.004530\n",
      "Epoch 72, loss: 0.004496\n",
      "Epoch 73, loss: 0.004530\n",
      "Epoch 74, loss: 0.004661\n",
      "Epoch 75, loss: 0.004721\n",
      "Epoch 76, loss: 0.004693\n",
      "Epoch 77, loss: 0.004820\n",
      "Epoch 78, loss: 0.004934\n",
      "Epoch 79, loss: 0.005008\n",
      "Epoch 80, loss: 0.005048\n",
      "Epoch 81, loss: 0.005033\n",
      "Epoch 82, loss: 0.005215\n",
      "Epoch 83, loss: 0.005253\n",
      "Epoch 84, loss: 0.005339\n",
      "Epoch 85, loss: 0.005357\n",
      "Epoch 86, loss: 0.005336\n",
      "Epoch 87, loss: 0.005549\n",
      "Epoch 88, loss: 0.005467\n",
      "Epoch 89, loss: 0.005670\n",
      "Epoch 90, loss: 0.005590\n",
      "Epoch 91, loss: 0.005605\n",
      "Epoch 92, loss: 0.005788\n",
      "Epoch 93, loss: 0.005729\n",
      "Epoch 94, loss: 0.005883\n",
      "Epoch 95, loss: 0.006105\n",
      "Epoch 96, loss: 0.005782\n",
      "Epoch 97, loss: 0.006201\n",
      "Epoch 98, loss: 0.006228\n",
      "Epoch 99, loss: 0.006147\n",
      "accuracy = 0.20333333333333334\n",
      "learning_rate = 0.01, reg_strength = 0.0001\n",
      "Epoch 0, loss: 0.000007\n",
      "Epoch 1, loss: 0.000008\n",
      "Epoch 2, loss: 0.000010\n",
      "Epoch 3, loss: 0.000012\n",
      "Epoch 4, loss: 0.000015\n",
      "Epoch 5, loss: 0.000018\n",
      "Epoch 6, loss: 0.000022\n",
      "Epoch 7, loss: 0.000025\n",
      "Epoch 8, loss: 0.000030\n",
      "Epoch 9, loss: 0.000034\n",
      "Epoch 10, loss: 0.000039\n",
      "Epoch 11, loss: 0.000044\n",
      "Epoch 12, loss: 0.000049\n",
      "Epoch 13, loss: 0.000055\n",
      "Epoch 14, loss: 0.000061\n",
      "Epoch 15, loss: 0.000066\n",
      "Epoch 16, loss: 0.000072\n",
      "Epoch 17, loss: 0.000078\n",
      "Epoch 18, loss: 0.000085\n",
      "Epoch 19, loss: 0.000093\n",
      "Epoch 20, loss: 0.000098\n",
      "Epoch 21, loss: 0.000104\n",
      "Epoch 22, loss: 0.000112\n",
      "Epoch 23, loss: 0.000118\n",
      "Epoch 24, loss: 0.000125\n",
      "Epoch 25, loss: 0.000133\n",
      "Epoch 26, loss: 0.000138\n",
      "Epoch 27, loss: 0.000146\n",
      "Epoch 28, loss: 0.000151\n",
      "Epoch 29, loss: 0.000159\n",
      "Epoch 30, loss: 0.000166\n",
      "Epoch 31, loss: 0.000174\n",
      "Epoch 32, loss: 0.000180\n",
      "Epoch 33, loss: 0.000190\n",
      "Epoch 34, loss: 0.000198\n",
      "Epoch 35, loss: 0.000205\n",
      "Epoch 36, loss: 0.000214\n",
      "Epoch 37, loss: 0.000220\n",
      "Epoch 38, loss: 0.000225\n",
      "Epoch 39, loss: 0.000234\n",
      "Epoch 40, loss: 0.000239\n",
      "Epoch 41, loss: 0.000248\n",
      "Epoch 42, loss: 0.000256\n",
      "Epoch 43, loss: 0.000255\n",
      "Epoch 44, loss: 0.000266\n",
      "Epoch 45, loss: 0.000276\n",
      "Epoch 46, loss: 0.000284\n",
      "Epoch 47, loss: 0.000289\n",
      "Epoch 48, loss: 0.000297\n",
      "Epoch 49, loss: 0.000303\n",
      "Epoch 50, loss: 0.000309\n",
      "Epoch 51, loss: 0.000319\n",
      "Epoch 52, loss: 0.000330\n",
      "Epoch 53, loss: 0.000335\n",
      "Epoch 54, loss: 0.000343\n",
      "Epoch 55, loss: 0.000338\n",
      "Epoch 56, loss: 0.000356\n",
      "Epoch 57, loss: 0.000362\n",
      "Epoch 58, loss: 0.000369\n",
      "Epoch 59, loss: 0.000372\n",
      "Epoch 60, loss: 0.000379\n",
      "Epoch 61, loss: 0.000378\n",
      "Epoch 62, loss: 0.000396\n",
      "Epoch 63, loss: 0.000401\n",
      "Epoch 64, loss: 0.000410\n",
      "Epoch 65, loss: 0.000413\n",
      "Epoch 66, loss: 0.000422\n",
      "Epoch 67, loss: 0.000429\n",
      "Epoch 68, loss: 0.000436\n",
      "Epoch 69, loss: 0.000438\n",
      "Epoch 70, loss: 0.000448\n",
      "Epoch 71, loss: 0.000452\n",
      "Epoch 72, loss: 0.000468\n",
      "Epoch 73, loss: 0.000467\n",
      "Epoch 74, loss: 0.000481\n",
      "Epoch 75, loss: 0.000487\n",
      "Epoch 76, loss: 0.000485\n",
      "Epoch 77, loss: 0.000494\n",
      "Epoch 78, loss: 0.000494\n",
      "Epoch 79, loss: 0.000509\n",
      "Epoch 80, loss: 0.000505\n",
      "Epoch 81, loss: 0.000512\n",
      "Epoch 82, loss: 0.000526\n",
      "Epoch 83, loss: 0.000537\n",
      "Epoch 84, loss: 0.000541\n",
      "Epoch 85, loss: 0.000552\n",
      "Epoch 86, loss: 0.000542\n",
      "Epoch 87, loss: 0.000557\n",
      "Epoch 88, loss: 0.000552\n",
      "Epoch 89, loss: 0.000579\n",
      "Epoch 90, loss: 0.000592\n",
      "Epoch 91, loss: 0.000590\n",
      "Epoch 92, loss: 0.000594\n",
      "Epoch 93, loss: 0.000595\n",
      "Epoch 94, loss: 0.000596\n",
      "Epoch 95, loss: 0.000600\n",
      "Epoch 96, loss: 0.000613\n",
      "Epoch 97, loss: 0.000624\n",
      "Epoch 98, loss: 0.000623\n",
      "Epoch 99, loss: 0.000634\n",
      "accuracy = 0.20722222222222222\n",
      "learning_rate = 0.01, reg_strength = 1e-05\n",
      "Epoch 0, loss: 0.000001\n",
      "Epoch 1, loss: 0.000001\n",
      "Epoch 2, loss: 0.000001\n",
      "Epoch 3, loss: 0.000001\n",
      "Epoch 4, loss: 0.000001\n",
      "Epoch 5, loss: 0.000002\n",
      "Epoch 6, loss: 0.000002\n",
      "Epoch 7, loss: 0.000003\n",
      "Epoch 8, loss: 0.000003\n",
      "Epoch 9, loss: 0.000003\n",
      "Epoch 10, loss: 0.000004\n",
      "Epoch 11, loss: 0.000004\n",
      "Epoch 12, loss: 0.000005\n",
      "Epoch 13, loss: 0.000005\n",
      "Epoch 14, loss: 0.000006\n",
      "Epoch 15, loss: 0.000007\n",
      "Epoch 16, loss: 0.000007\n",
      "Epoch 17, loss: 0.000008\n",
      "Epoch 18, loss: 0.000009\n",
      "Epoch 19, loss: 0.000009\n",
      "Epoch 20, loss: 0.000010\n",
      "Epoch 21, loss: 0.000011\n",
      "Epoch 22, loss: 0.000011\n",
      "Epoch 23, loss: 0.000012\n",
      "Epoch 24, loss: 0.000013\n",
      "Epoch 25, loss: 0.000013\n",
      "Epoch 26, loss: 0.000014\n",
      "Epoch 27, loss: 0.000015\n",
      "Epoch 28, loss: 0.000015\n",
      "Epoch 29, loss: 0.000016\n",
      "Epoch 30, loss: 0.000017\n",
      "Epoch 31, loss: 0.000017\n",
      "Epoch 32, loss: 0.000018\n",
      "Epoch 33, loss: 0.000019\n",
      "Epoch 34, loss: 0.000020\n",
      "Epoch 35, loss: 0.000021\n",
      "Epoch 36, loss: 0.000021\n",
      "Epoch 37, loss: 0.000022\n",
      "Epoch 38, loss: 0.000023\n",
      "Epoch 39, loss: 0.000023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, loss: 0.000024\n",
      "Epoch 41, loss: 0.000025\n",
      "Epoch 42, loss: 0.000025\n",
      "Epoch 43, loss: 0.000026\n",
      "Epoch 44, loss: 0.000027\n",
      "Epoch 45, loss: 0.000027\n",
      "Epoch 46, loss: 0.000029\n",
      "Epoch 47, loss: 0.000029\n",
      "Epoch 48, loss: 0.000029\n",
      "Epoch 49, loss: 0.000030\n",
      "Epoch 50, loss: 0.000031\n",
      "Epoch 51, loss: 0.000032\n",
      "Epoch 52, loss: 0.000032\n",
      "Epoch 53, loss: 0.000033\n",
      "Epoch 54, loss: 0.000034\n",
      "Epoch 55, loss: 0.000035\n",
      "Epoch 56, loss: 0.000035\n",
      "Epoch 57, loss: 0.000036\n",
      "Epoch 58, loss: 0.000037\n",
      "Epoch 59, loss: 0.000038\n",
      "Epoch 60, loss: 0.000038\n",
      "Epoch 61, loss: 0.000040\n",
      "Epoch 62, loss: 0.000040\n",
      "Epoch 63, loss: 0.000041\n",
      "Epoch 64, loss: 0.000041\n",
      "Epoch 65, loss: 0.000042\n",
      "Epoch 66, loss: 0.000041\n",
      "Epoch 67, loss: 0.000044\n",
      "Epoch 68, loss: 0.000044\n",
      "Epoch 69, loss: 0.000044\n",
      "Epoch 70, loss: 0.000045\n",
      "Epoch 71, loss: 0.000046\n",
      "Epoch 72, loss: 0.000045\n",
      "Epoch 73, loss: 0.000047\n",
      "Epoch 74, loss: 0.000048\n",
      "Epoch 75, loss: 0.000048\n",
      "Epoch 76, loss: 0.000049\n",
      "Epoch 77, loss: 0.000049\n",
      "Epoch 78, loss: 0.000050\n",
      "Epoch 79, loss: 0.000052\n",
      "Epoch 80, loss: 0.000051\n",
      "Epoch 81, loss: 0.000052\n",
      "Epoch 82, loss: 0.000052\n",
      "Epoch 83, loss: 0.000053\n",
      "Epoch 84, loss: 0.000053\n",
      "Epoch 85, loss: 0.000055\n",
      "Epoch 86, loss: 0.000055\n",
      "Epoch 87, loss: 0.000056\n",
      "Epoch 88, loss: 0.000057\n",
      "Epoch 89, loss: 0.000057\n",
      "Epoch 90, loss: 0.000058\n",
      "Epoch 91, loss: 0.000059\n",
      "Epoch 92, loss: 0.000060\n",
      "Epoch 93, loss: 0.000060\n",
      "Epoch 94, loss: 0.000059\n",
      "Epoch 95, loss: 0.000060\n",
      "Epoch 96, loss: 0.000061\n",
      "Epoch 97, loss: 0.000063\n",
      "Epoch 98, loss: 0.000063\n",
      "Epoch 99, loss: 0.000064\n",
      "accuracy = 0.2061111111111111\n",
      "learning_rate = 0.01, reg_strength = 1e-06\n",
      "Epoch 0, loss: 0.000000\n",
      "Epoch 1, loss: 0.000000\n",
      "Epoch 2, loss: 0.000000\n",
      "Epoch 3, loss: 0.000000\n",
      "Epoch 4, loss: 0.000000\n",
      "Epoch 5, loss: 0.000000\n",
      "Epoch 6, loss: 0.000000\n",
      "Epoch 7, loss: 0.000000\n",
      "Epoch 8, loss: 0.000000\n",
      "Epoch 9, loss: 0.000000\n",
      "Epoch 10, loss: 0.000000\n",
      "Epoch 11, loss: 0.000000\n",
      "Epoch 12, loss: 0.000000\n",
      "Epoch 13, loss: 0.000001\n",
      "Epoch 14, loss: 0.000001\n",
      "Epoch 15, loss: 0.000001\n",
      "Epoch 16, loss: 0.000001\n",
      "Epoch 17, loss: 0.000001\n",
      "Epoch 18, loss: 0.000001\n",
      "Epoch 19, loss: 0.000001\n",
      "Epoch 20, loss: 0.000001\n",
      "Epoch 21, loss: 0.000001\n",
      "Epoch 22, loss: 0.000001\n",
      "Epoch 23, loss: 0.000001\n",
      "Epoch 24, loss: 0.000001\n",
      "Epoch 25, loss: 0.000001\n",
      "Epoch 26, loss: 0.000001\n",
      "Epoch 27, loss: 0.000001\n",
      "Epoch 28, loss: 0.000002\n",
      "Epoch 29, loss: 0.000002\n",
      "Epoch 30, loss: 0.000002\n",
      "Epoch 31, loss: 0.000002\n",
      "Epoch 32, loss: 0.000002\n",
      "Epoch 33, loss: 0.000002\n",
      "Epoch 34, loss: 0.000002\n",
      "Epoch 35, loss: 0.000002\n",
      "Epoch 36, loss: 0.000002\n",
      "Epoch 37, loss: 0.000002\n",
      "Epoch 38, loss: 0.000002\n",
      "Epoch 39, loss: 0.000002\n",
      "Epoch 40, loss: 0.000002\n",
      "Epoch 41, loss: 0.000002\n",
      "Epoch 42, loss: 0.000003\n",
      "Epoch 43, loss: 0.000003\n",
      "Epoch 44, loss: 0.000003\n",
      "Epoch 45, loss: 0.000003\n",
      "Epoch 46, loss: 0.000003\n",
      "Epoch 47, loss: 0.000003\n",
      "Epoch 48, loss: 0.000003\n",
      "Epoch 49, loss: 0.000003\n",
      "Epoch 50, loss: 0.000003\n",
      "Epoch 51, loss: 0.000003\n",
      "Epoch 52, loss: 0.000003\n",
      "Epoch 53, loss: 0.000003\n",
      "Epoch 54, loss: 0.000003\n",
      "Epoch 55, loss: 0.000003\n",
      "Epoch 56, loss: 0.000004\n",
      "Epoch 57, loss: 0.000004\n",
      "Epoch 58, loss: 0.000004\n",
      "Epoch 59, loss: 0.000004\n",
      "Epoch 60, loss: 0.000004\n",
      "Epoch 61, loss: 0.000004\n",
      "Epoch 62, loss: 0.000004\n",
      "Epoch 63, loss: 0.000004\n",
      "Epoch 64, loss: 0.000004\n",
      "Epoch 65, loss: 0.000004\n",
      "Epoch 66, loss: 0.000004\n",
      "Epoch 67, loss: 0.000004\n",
      "Epoch 68, loss: 0.000004\n",
      "Epoch 69, loss: 0.000004\n",
      "Epoch 70, loss: 0.000005\n",
      "Epoch 71, loss: 0.000005\n",
      "Epoch 72, loss: 0.000005\n",
      "Epoch 73, loss: 0.000005\n",
      "Epoch 74, loss: 0.000005\n",
      "Epoch 75, loss: 0.000005\n",
      "Epoch 76, loss: 0.000005\n",
      "Epoch 77, loss: 0.000005\n",
      "Epoch 78, loss: 0.000005\n",
      "Epoch 79, loss: 0.000005\n",
      "Epoch 80, loss: 0.000005\n",
      "Epoch 81, loss: 0.000005\n",
      "Epoch 82, loss: 0.000005\n",
      "Epoch 83, loss: 0.000005\n",
      "Epoch 84, loss: 0.000006\n",
      "Epoch 85, loss: 0.000005\n",
      "Epoch 86, loss: 0.000006\n",
      "Epoch 87, loss: 0.000006\n",
      "Epoch 88, loss: 0.000006\n",
      "Epoch 89, loss: 0.000006\n",
      "Epoch 90, loss: 0.000006\n",
      "Epoch 91, loss: 0.000006\n",
      "Epoch 92, loss: 0.000006\n",
      "Epoch 93, loss: 0.000006\n",
      "Epoch 94, loss: 0.000006\n",
      "Epoch 95, loss: 0.000006\n",
      "Epoch 96, loss: 0.000006\n",
      "Epoch 97, loss: 0.000006\n",
      "Epoch 98, loss: 0.000006\n",
      "Epoch 99, loss: 0.000006\n",
      "accuracy = 0.20722222222222222\n",
      "learning_rate = 0.001, reg_strength = 10.0\n",
      "Epoch 0, loss: 0.279943\n",
      "Epoch 1, loss: 0.106709\n",
      "Epoch 2, loss: 0.041379\n",
      "Epoch 3, loss: 0.016698\n",
      "Epoch 4, loss: 0.007485\n",
      "Epoch 5, loss: 0.004025\n",
      "Epoch 6, loss: 0.002792\n",
      "Epoch 7, loss: 0.002343\n",
      "Epoch 8, loss: 0.002098\n",
      "Epoch 9, loss: 0.002065\n",
      "Epoch 10, loss: 0.002029\n",
      "Epoch 11, loss: 0.002027\n",
      "Epoch 12, loss: 0.002013\n",
      "Epoch 13, loss: 0.002057\n",
      "Epoch 14, loss: 0.002045\n",
      "Epoch 15, loss: 0.002093\n",
      "Epoch 16, loss: 0.002054\n",
      "Epoch 17, loss: 0.002048\n",
      "Epoch 18, loss: 0.002053\n",
      "Epoch 19, loss: 0.002110\n",
      "Epoch 20, loss: 0.002142\n",
      "Epoch 21, loss: 0.002101\n",
      "Epoch 22, loss: 0.002083\n",
      "Epoch 23, loss: 0.002011\n",
      "Epoch 24, loss: 0.001972\n",
      "Epoch 25, loss: 0.002022\n",
      "Epoch 26, loss: 0.002017\n",
      "Epoch 27, loss: 0.002077\n",
      "Epoch 28, loss: 0.001990\n",
      "Epoch 29, loss: 0.001985\n",
      "Epoch 30, loss: 0.002019\n",
      "Epoch 31, loss: 0.002055\n",
      "Epoch 32, loss: 0.002057\n",
      "Epoch 33, loss: 0.002031\n",
      "Epoch 34, loss: 0.002066\n",
      "Epoch 35, loss: 0.002042\n",
      "Epoch 36, loss: 0.002115\n",
      "Epoch 37, loss: 0.002052\n",
      "Epoch 38, loss: 0.002071\n",
      "Epoch 39, loss: 0.002057\n",
      "Epoch 40, loss: 0.001960\n",
      "Epoch 41, loss: 0.002016\n",
      "Epoch 42, loss: 0.002041\n",
      "Epoch 43, loss: 0.002020\n",
      "Epoch 44, loss: 0.002051\n",
      "Epoch 45, loss: 0.002064\n",
      "Epoch 46, loss: 0.002146\n",
      "Epoch 47, loss: 0.002041\n",
      "Epoch 48, loss: 0.002011\n",
      "Epoch 49, loss: 0.002066\n",
      "Epoch 50, loss: 0.002049\n",
      "Epoch 51, loss: 0.002064\n",
      "Epoch 52, loss: 0.002075\n",
      "Epoch 53, loss: 0.002055\n",
      "Epoch 54, loss: 0.002075\n",
      "Epoch 55, loss: 0.002073\n",
      "Epoch 56, loss: 0.002054\n",
      "Epoch 57, loss: 0.002074\n",
      "Epoch 58, loss: 0.002041\n",
      "Epoch 59, loss: 0.002053\n",
      "Epoch 60, loss: 0.002049\n",
      "Epoch 61, loss: 0.002068\n",
      "Epoch 62, loss: 0.002075\n",
      "Epoch 63, loss: 0.002069\n",
      "Epoch 64, loss: 0.002044\n",
      "Epoch 65, loss: 0.002047\n",
      "Epoch 66, loss: 0.002099\n",
      "Epoch 67, loss: 0.002113\n",
      "Epoch 68, loss: 0.002036\n",
      "Epoch 69, loss: 0.002081\n",
      "Epoch 70, loss: 0.002103\n",
      "Epoch 71, loss: 0.002041\n",
      "Epoch 72, loss: 0.002063\n",
      "Epoch 73, loss: 0.002024\n",
      "Epoch 74, loss: 0.002100\n",
      "Epoch 75, loss: 0.002019\n",
      "Epoch 76, loss: 0.002017\n",
      "Epoch 77, loss: 0.001963\n",
      "Epoch 78, loss: 0.001986\n",
      "Epoch 79, loss: 0.002022\n",
      "Epoch 80, loss: 0.002097\n",
      "Epoch 81, loss: 0.002022\n",
      "Epoch 82, loss: 0.002037\n",
      "Epoch 83, loss: 0.002083\n",
      "Epoch 84, loss: 0.002048\n",
      "Epoch 85, loss: 0.002062\n",
      "Epoch 86, loss: 0.002075\n",
      "Epoch 87, loss: 0.002101\n",
      "Epoch 88, loss: 0.002088\n",
      "Epoch 89, loss: 0.002055\n",
      "Epoch 90, loss: 0.002061\n",
      "Epoch 91, loss: 0.001993\n",
      "Epoch 92, loss: 0.002098\n",
      "Epoch 93, loss: 0.002043\n",
      "Epoch 94, loss: 0.002010\n",
      "Epoch 95, loss: 0.002109\n",
      "Epoch 96, loss: 0.002003\n",
      "Epoch 97, loss: 0.002059\n",
      "Epoch 98, loss: 0.002074\n",
      "Epoch 99, loss: 0.002019\n",
      "accuracy = 0.13055555555555556\n",
      "learning_rate = 0.001, reg_strength = 1\n",
      "Epoch 0, loss: 0.065189\n",
      "Epoch 1, loss: 0.059368\n",
      "Epoch 2, loss: 0.054101\n",
      "Epoch 3, loss: 0.049439\n",
      "Epoch 4, loss: 0.045236\n",
      "Epoch 5, loss: 0.041486\n",
      "Epoch 6, loss: 0.038105\n",
      "Epoch 7, loss: 0.035162\n",
      "Epoch 8, loss: 0.032492\n",
      "Epoch 9, loss: 0.030073\n",
      "Epoch 10, loss: 0.027934\n",
      "Epoch 11, loss: 0.026027\n",
      "Epoch 12, loss: 0.024429\n",
      "Epoch 13, loss: 0.022876\n",
      "Epoch 14, loss: 0.021555\n",
      "Epoch 15, loss: 0.020403\n",
      "Epoch 16, loss: 0.019362\n",
      "Epoch 17, loss: 0.018443\n",
      "Epoch 18, loss: 0.017639\n",
      "Epoch 19, loss: 0.016962\n",
      "Epoch 20, loss: 0.016331\n",
      "Epoch 21, loss: 0.015822\n",
      "Epoch 22, loss: 0.015295\n",
      "Epoch 23, loss: 0.014960\n",
      "Epoch 24, loss: 0.014553\n",
      "Epoch 25, loss: 0.014293\n",
      "Epoch 26, loss: 0.014059\n",
      "Epoch 27, loss: 0.013906\n",
      "Epoch 28, loss: 0.013676\n",
      "Epoch 29, loss: 0.013504\n",
      "Epoch 30, loss: 0.013399\n",
      "Epoch 31, loss: 0.013320\n",
      "Epoch 32, loss: 0.013212\n",
      "Epoch 33, loss: 0.013150\n",
      "Epoch 34, loss: 0.013135\n",
      "Epoch 35, loss: 0.013112\n",
      "Epoch 36, loss: 0.013106\n",
      "Epoch 37, loss: 0.013079\n",
      "Epoch 38, loss: 0.013098\n",
      "Epoch 39, loss: 0.013109\n",
      "Epoch 40, loss: 0.013129\n",
      "Epoch 41, loss: 0.013179\n",
      "Epoch 42, loss: 0.013202\n",
      "Epoch 43, loss: 0.013247\n",
      "Epoch 44, loss: 0.013317\n",
      "Epoch 45, loss: 0.013353\n",
      "Epoch 46, loss: 0.013324\n",
      "Epoch 47, loss: 0.013406\n",
      "Epoch 48, loss: 0.013459\n",
      "Epoch 49, loss: 0.013506\n",
      "Epoch 50, loss: 0.013576\n",
      "Epoch 51, loss: 0.013572\n",
      "Epoch 52, loss: 0.013679\n",
      "Epoch 53, loss: 0.013697\n",
      "Epoch 54, loss: 0.013733\n",
      "Epoch 55, loss: 0.013769\n",
      "Epoch 56, loss: 0.013835\n",
      "Epoch 57, loss: 0.013908\n",
      "Epoch 58, loss: 0.013902\n",
      "Epoch 59, loss: 0.013979\n",
      "Epoch 60, loss: 0.014069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61, loss: 0.014112\n",
      "Epoch 62, loss: 0.014096\n",
      "Epoch 63, loss: 0.014189\n",
      "Epoch 64, loss: 0.014187\n",
      "Epoch 65, loss: 0.014258\n",
      "Epoch 66, loss: 0.014281\n",
      "Epoch 67, loss: 0.014269\n",
      "Epoch 68, loss: 0.014405\n",
      "Epoch 69, loss: 0.014367\n",
      "Epoch 70, loss: 0.014491\n",
      "Epoch 71, loss: 0.014356\n",
      "Epoch 72, loss: 0.014434\n",
      "Epoch 73, loss: 0.014512\n",
      "Epoch 74, loss: 0.014474\n",
      "Epoch 75, loss: 0.014549\n",
      "Epoch 76, loss: 0.014559\n",
      "Epoch 77, loss: 0.014627\n",
      "Epoch 78, loss: 0.014619\n",
      "Epoch 79, loss: 0.014657\n",
      "Epoch 80, loss: 0.014625\n",
      "Epoch 81, loss: 0.014697\n",
      "Epoch 82, loss: 0.014655\n",
      "Epoch 83, loss: 0.014734\n",
      "Epoch 84, loss: 0.014688\n",
      "Epoch 85, loss: 0.014754\n",
      "Epoch 86, loss: 0.014785\n",
      "Epoch 87, loss: 0.014753\n",
      "Epoch 88, loss: 0.014792\n",
      "Epoch 89, loss: 0.014821\n",
      "Epoch 90, loss: 0.014806\n",
      "Epoch 91, loss: 0.014884\n",
      "Epoch 92, loss: 0.014817\n",
      "Epoch 93, loss: 0.014858\n",
      "Epoch 94, loss: 0.014813\n",
      "Epoch 95, loss: 0.014839\n",
      "Epoch 96, loss: 0.014925\n",
      "Epoch 97, loss: 0.014904\n",
      "Epoch 98, loss: 0.014984\n",
      "Epoch 99, loss: 0.014917\n",
      "accuracy = 0.15944444444444444\n",
      "learning_rate = 0.001, reg_strength = 0.1\n",
      "Epoch 0, loss: 0.006979\n",
      "Epoch 1, loss: 0.006922\n",
      "Epoch 2, loss: 0.006877\n",
      "Epoch 3, loss: 0.006832\n",
      "Epoch 4, loss: 0.006796\n",
      "Epoch 5, loss: 0.006788\n",
      "Epoch 6, loss: 0.006757\n",
      "Epoch 7, loss: 0.006763\n",
      "Epoch 8, loss: 0.006743\n",
      "Epoch 9, loss: 0.006764\n",
      "Epoch 10, loss: 0.006782\n",
      "Epoch 11, loss: 0.006801\n",
      "Epoch 12, loss: 0.006801\n",
      "Epoch 13, loss: 0.006831\n",
      "Epoch 14, loss: 0.006880\n",
      "Epoch 15, loss: 0.006916\n",
      "Epoch 16, loss: 0.006968\n",
      "Epoch 17, loss: 0.007023\n",
      "Epoch 18, loss: 0.007092\n",
      "Epoch 19, loss: 0.007119\n",
      "Epoch 20, loss: 0.007194\n",
      "Epoch 21, loss: 0.007270\n",
      "Epoch 22, loss: 0.007336\n",
      "Epoch 23, loss: 0.007429\n",
      "Epoch 24, loss: 0.007522\n",
      "Epoch 25, loss: 0.007610\n",
      "Epoch 26, loss: 0.007710\n",
      "Epoch 27, loss: 0.007769\n",
      "Epoch 28, loss: 0.007909\n",
      "Epoch 29, loss: 0.008019\n",
      "Epoch 30, loss: 0.008118\n",
      "Epoch 31, loss: 0.008246\n",
      "Epoch 32, loss: 0.008375\n",
      "Epoch 33, loss: 0.008448\n",
      "Epoch 34, loss: 0.008596\n",
      "Epoch 35, loss: 0.008714\n",
      "Epoch 36, loss: 0.008856\n",
      "Epoch 37, loss: 0.008965\n",
      "Epoch 38, loss: 0.009128\n",
      "Epoch 39, loss: 0.009262\n",
      "Epoch 40, loss: 0.009404\n",
      "Epoch 41, loss: 0.009519\n",
      "Epoch 42, loss: 0.009687\n",
      "Epoch 43, loss: 0.009863\n",
      "Epoch 44, loss: 0.009972\n",
      "Epoch 45, loss: 0.010096\n",
      "Epoch 46, loss: 0.010306\n",
      "Epoch 47, loss: 0.010473\n",
      "Epoch 48, loss: 0.010609\n",
      "Epoch 49, loss: 0.010777\n",
      "Epoch 50, loss: 0.010998\n",
      "Epoch 51, loss: 0.011155\n",
      "Epoch 52, loss: 0.011278\n",
      "Epoch 53, loss: 0.011493\n",
      "Epoch 54, loss: 0.011629\n",
      "Epoch 55, loss: 0.011810\n",
      "Epoch 56, loss: 0.012059\n",
      "Epoch 57, loss: 0.012177\n",
      "Epoch 58, loss: 0.012337\n",
      "Epoch 59, loss: 0.012432\n",
      "Epoch 60, loss: 0.012744\n",
      "Epoch 61, loss: 0.012964\n",
      "Epoch 62, loss: 0.013093\n",
      "Epoch 63, loss: 0.013299\n",
      "Epoch 64, loss: 0.013445\n",
      "Epoch 65, loss: 0.013637\n",
      "Epoch 66, loss: 0.013791\n",
      "Epoch 67, loss: 0.014056\n",
      "Epoch 68, loss: 0.014197\n",
      "Epoch 69, loss: 0.014419\n",
      "Epoch 70, loss: 0.014684\n",
      "Epoch 71, loss: 0.014773\n",
      "Epoch 72, loss: 0.015068\n",
      "Epoch 73, loss: 0.015307\n",
      "Epoch 74, loss: 0.015439\n",
      "Epoch 75, loss: 0.015655\n",
      "Epoch 76, loss: 0.015784\n",
      "Epoch 77, loss: 0.016069\n",
      "Epoch 78, loss: 0.016268\n",
      "Epoch 79, loss: 0.016393\n",
      "Epoch 80, loss: 0.016721\n",
      "Epoch 81, loss: 0.016795\n",
      "Epoch 82, loss: 0.017004\n",
      "Epoch 83, loss: 0.017238\n",
      "Epoch 84, loss: 0.017324\n",
      "Epoch 85, loss: 0.017729\n",
      "Epoch 86, loss: 0.017837\n",
      "Epoch 87, loss: 0.018063\n",
      "Epoch 88, loss: 0.018295\n",
      "Epoch 89, loss: 0.018479\n",
      "Epoch 90, loss: 0.018645\n",
      "Epoch 91, loss: 0.018899\n",
      "Epoch 92, loss: 0.019014\n",
      "Epoch 93, loss: 0.019316\n",
      "Epoch 94, loss: 0.019514\n",
      "Epoch 95, loss: 0.019780\n",
      "Epoch 96, loss: 0.020064\n",
      "Epoch 97, loss: 0.020163\n",
      "Epoch 98, loss: 0.020405\n",
      "Epoch 99, loss: 0.020535\n",
      "accuracy = 0.18166666666666667\n",
      "learning_rate = 0.001, reg_strength = 0.01\n",
      "Epoch 0, loss: 0.000710\n",
      "Epoch 1, loss: 0.000710\n",
      "Epoch 2, loss: 0.000711\n",
      "Epoch 3, loss: 0.000713\n",
      "Epoch 4, loss: 0.000718\n",
      "Epoch 5, loss: 0.000720\n",
      "Epoch 6, loss: 0.000724\n",
      "Epoch 7, loss: 0.000730\n",
      "Epoch 8, loss: 0.000737\n",
      "Epoch 9, loss: 0.000744\n",
      "Epoch 10, loss: 0.000749\n",
      "Epoch 11, loss: 0.000756\n",
      "Epoch 12, loss: 0.000766\n",
      "Epoch 13, loss: 0.000775\n",
      "Epoch 14, loss: 0.000785\n",
      "Epoch 15, loss: 0.000794\n",
      "Epoch 16, loss: 0.000806\n",
      "Epoch 17, loss: 0.000818\n",
      "Epoch 18, loss: 0.000829\n",
      "Epoch 19, loss: 0.000842\n",
      "Epoch 20, loss: 0.000856\n",
      "Epoch 21, loss: 0.000872\n",
      "Epoch 22, loss: 0.000885\n",
      "Epoch 23, loss: 0.000900\n",
      "Epoch 24, loss: 0.000912\n",
      "Epoch 25, loss: 0.000930\n",
      "Epoch 26, loss: 0.000949\n",
      "Epoch 27, loss: 0.000965\n",
      "Epoch 28, loss: 0.000979\n",
      "Epoch 29, loss: 0.001001\n",
      "Epoch 30, loss: 0.001019\n",
      "Epoch 31, loss: 0.001041\n",
      "Epoch 32, loss: 0.001060\n",
      "Epoch 33, loss: 0.001081\n",
      "Epoch 34, loss: 0.001094\n",
      "Epoch 35, loss: 0.001123\n",
      "Epoch 36, loss: 0.001142\n",
      "Epoch 37, loss: 0.001164\n",
      "Epoch 38, loss: 0.001189\n",
      "Epoch 39, loss: 0.001206\n",
      "Epoch 40, loss: 0.001233\n",
      "Epoch 41, loss: 0.001257\n",
      "Epoch 42, loss: 0.001282\n",
      "Epoch 43, loss: 0.001309\n",
      "Epoch 44, loss: 0.001336\n",
      "Epoch 45, loss: 0.001358\n",
      "Epoch 46, loss: 0.001386\n",
      "Epoch 47, loss: 0.001415\n",
      "Epoch 48, loss: 0.001442\n",
      "Epoch 49, loss: 0.001464\n",
      "Epoch 50, loss: 0.001501\n",
      "Epoch 51, loss: 0.001521\n",
      "Epoch 52, loss: 0.001549\n",
      "Epoch 53, loss: 0.001585\n",
      "Epoch 54, loss: 0.001611\n",
      "Epoch 55, loss: 0.001643\n",
      "Epoch 56, loss: 0.001664\n",
      "Epoch 57, loss: 0.001702\n",
      "Epoch 58, loss: 0.001724\n",
      "Epoch 59, loss: 0.001764\n",
      "Epoch 60, loss: 0.001809\n",
      "Epoch 61, loss: 0.001825\n",
      "Epoch 62, loss: 0.001860\n",
      "Epoch 63, loss: 0.001894\n",
      "Epoch 64, loss: 0.001923\n",
      "Epoch 65, loss: 0.001953\n",
      "Epoch 66, loss: 0.001993\n",
      "Epoch 67, loss: 0.002031\n",
      "Epoch 68, loss: 0.002070\n",
      "Epoch 69, loss: 0.002098\n",
      "Epoch 70, loss: 0.002141\n",
      "Epoch 71, loss: 0.002185\n",
      "Epoch 72, loss: 0.002207\n",
      "Epoch 73, loss: 0.002247\n",
      "Epoch 74, loss: 0.002261\n",
      "Epoch 75, loss: 0.002313\n",
      "Epoch 76, loss: 0.002354\n",
      "Epoch 77, loss: 0.002383\n",
      "Epoch 78, loss: 0.002418\n",
      "Epoch 79, loss: 0.002447\n",
      "Epoch 80, loss: 0.002497\n",
      "Epoch 81, loss: 0.002536\n",
      "Epoch 82, loss: 0.002568\n",
      "Epoch 83, loss: 0.002614\n",
      "Epoch 84, loss: 0.002655\n",
      "Epoch 85, loss: 0.002693\n",
      "Epoch 86, loss: 0.002739\n",
      "Epoch 87, loss: 0.002776\n",
      "Epoch 88, loss: 0.002796\n",
      "Epoch 89, loss: 0.002842\n",
      "Epoch 90, loss: 0.002891\n",
      "Epoch 91, loss: 0.002955\n",
      "Epoch 92, loss: 0.002978\n",
      "Epoch 93, loss: 0.003025\n",
      "Epoch 94, loss: 0.003049\n",
      "Epoch 95, loss: 0.003114\n",
      "Epoch 96, loss: 0.003157\n",
      "Epoch 97, loss: 0.003199\n",
      "Epoch 98, loss: 0.003259\n",
      "Epoch 99, loss: 0.003267\n",
      "accuracy = 0.18222222222222223\n",
      "learning_rate = 0.001, reg_strength = 0.001\n",
      "Epoch 0, loss: 0.000071\n",
      "Epoch 1, loss: 0.000071\n",
      "Epoch 2, loss: 0.000072\n",
      "Epoch 3, loss: 0.000072\n",
      "Epoch 4, loss: 0.000072\n",
      "Epoch 5, loss: 0.000073\n",
      "Epoch 6, loss: 0.000073\n",
      "Epoch 7, loss: 0.000074\n",
      "Epoch 8, loss: 0.000074\n",
      "Epoch 9, loss: 0.000075\n",
      "Epoch 10, loss: 0.000076\n",
      "Epoch 11, loss: 0.000076\n",
      "Epoch 12, loss: 0.000077\n",
      "Epoch 13, loss: 0.000078\n",
      "Epoch 14, loss: 0.000079\n",
      "Epoch 15, loss: 0.000080\n",
      "Epoch 16, loss: 0.000082\n",
      "Epoch 17, loss: 0.000083\n",
      "Epoch 18, loss: 0.000084\n",
      "Epoch 19, loss: 0.000086\n",
      "Epoch 20, loss: 0.000087\n",
      "Epoch 21, loss: 0.000088\n",
      "Epoch 22, loss: 0.000090\n",
      "Epoch 23, loss: 0.000091\n",
      "Epoch 24, loss: 0.000093\n",
      "Epoch 25, loss: 0.000095\n",
      "Epoch 26, loss: 0.000097\n",
      "Epoch 27, loss: 0.000098\n",
      "Epoch 28, loss: 0.000101\n",
      "Epoch 29, loss: 0.000102\n",
      "Epoch 30, loss: 0.000104\n",
      "Epoch 31, loss: 0.000106\n",
      "Epoch 32, loss: 0.000108\n",
      "Epoch 33, loss: 0.000111\n",
      "Epoch 34, loss: 0.000113\n",
      "Epoch 35, loss: 0.000115\n",
      "Epoch 36, loss: 0.000117\n",
      "Epoch 37, loss: 0.000119\n",
      "Epoch 38, loss: 0.000122\n",
      "Epoch 39, loss: 0.000124\n",
      "Epoch 40, loss: 0.000127\n",
      "Epoch 41, loss: 0.000129\n",
      "Epoch 42, loss: 0.000132\n",
      "Epoch 43, loss: 0.000134\n",
      "Epoch 44, loss: 0.000137\n",
      "Epoch 45, loss: 0.000140\n",
      "Epoch 46, loss: 0.000142\n",
      "Epoch 47, loss: 0.000145\n",
      "Epoch 48, loss: 0.000148\n",
      "Epoch 49, loss: 0.000151\n",
      "Epoch 50, loss: 0.000154\n",
      "Epoch 51, loss: 0.000157\n",
      "Epoch 52, loss: 0.000159\n",
      "Epoch 53, loss: 0.000163\n",
      "Epoch 54, loss: 0.000166\n",
      "Epoch 55, loss: 0.000169\n",
      "Epoch 56, loss: 0.000172\n",
      "Epoch 57, loss: 0.000176\n",
      "Epoch 58, loss: 0.000179\n",
      "Epoch 59, loss: 0.000182\n",
      "Epoch 60, loss: 0.000185\n",
      "Epoch 61, loss: 0.000189\n",
      "Epoch 62, loss: 0.000192\n",
      "Epoch 63, loss: 0.000195\n",
      "Epoch 64, loss: 0.000198\n",
      "Epoch 65, loss: 0.000203\n",
      "Epoch 66, loss: 0.000206\n",
      "Epoch 67, loss: 0.000210\n",
      "Epoch 68, loss: 0.000214\n",
      "Epoch 69, loss: 0.000218\n",
      "Epoch 70, loss: 0.000220\n",
      "Epoch 71, loss: 0.000224\n",
      "Epoch 72, loss: 0.000228\n",
      "Epoch 73, loss: 0.000231\n",
      "Epoch 74, loss: 0.000237\n",
      "Epoch 75, loss: 0.000241\n",
      "Epoch 76, loss: 0.000244\n",
      "Epoch 77, loss: 0.000248\n",
      "Epoch 78, loss: 0.000251\n",
      "Epoch 79, loss: 0.000257\n",
      "Epoch 80, loss: 0.000259\n",
      "Epoch 81, loss: 0.000265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82, loss: 0.000267\n",
      "Epoch 83, loss: 0.000272\n",
      "Epoch 84, loss: 0.000277\n",
      "Epoch 85, loss: 0.000280\n",
      "Epoch 86, loss: 0.000287\n",
      "Epoch 87, loss: 0.000290\n",
      "Epoch 88, loss: 0.000293\n",
      "Epoch 89, loss: 0.000299\n",
      "Epoch 90, loss: 0.000302\n",
      "Epoch 91, loss: 0.000307\n",
      "Epoch 92, loss: 0.000312\n",
      "Epoch 93, loss: 0.000316\n",
      "Epoch 94, loss: 0.000319\n",
      "Epoch 95, loss: 0.000323\n",
      "Epoch 96, loss: 0.000329\n",
      "Epoch 97, loss: 0.000332\n",
      "Epoch 98, loss: 0.000339\n",
      "Epoch 99, loss: 0.000342\n",
      "accuracy = 0.185\n",
      "learning_rate = 0.001, reg_strength = 0.0001\n",
      "Epoch 0, loss: 0.000007\n",
      "Epoch 1, loss: 0.000007\n",
      "Epoch 2, loss: 0.000007\n",
      "Epoch 3, loss: 0.000007\n",
      "Epoch 4, loss: 0.000007\n",
      "Epoch 5, loss: 0.000007\n",
      "Epoch 6, loss: 0.000007\n",
      "Epoch 7, loss: 0.000007\n",
      "Epoch 8, loss: 0.000007\n",
      "Epoch 9, loss: 0.000008\n",
      "Epoch 10, loss: 0.000008\n",
      "Epoch 11, loss: 0.000008\n",
      "Epoch 12, loss: 0.000008\n",
      "Epoch 13, loss: 0.000008\n",
      "Epoch 14, loss: 0.000008\n",
      "Epoch 15, loss: 0.000008\n",
      "Epoch 16, loss: 0.000008\n",
      "Epoch 17, loss: 0.000008\n",
      "Epoch 18, loss: 0.000008\n",
      "Epoch 19, loss: 0.000009\n",
      "Epoch 20, loss: 0.000009\n",
      "Epoch 21, loss: 0.000009\n",
      "Epoch 22, loss: 0.000009\n",
      "Epoch 23, loss: 0.000009\n",
      "Epoch 24, loss: 0.000009\n",
      "Epoch 25, loss: 0.000009\n",
      "Epoch 26, loss: 0.000010\n",
      "Epoch 27, loss: 0.000010\n",
      "Epoch 28, loss: 0.000010\n",
      "Epoch 29, loss: 0.000010\n",
      "Epoch 30, loss: 0.000010\n",
      "Epoch 31, loss: 0.000011\n",
      "Epoch 32, loss: 0.000011\n",
      "Epoch 33, loss: 0.000011\n",
      "Epoch 34, loss: 0.000011\n",
      "Epoch 35, loss: 0.000011\n",
      "Epoch 36, loss: 0.000012\n",
      "Epoch 37, loss: 0.000012\n",
      "Epoch 38, loss: 0.000012\n",
      "Epoch 39, loss: 0.000012\n",
      "Epoch 40, loss: 0.000013\n",
      "Epoch 41, loss: 0.000013\n",
      "Epoch 42, loss: 0.000013\n",
      "Epoch 43, loss: 0.000013\n",
      "Epoch 44, loss: 0.000014\n",
      "Epoch 45, loss: 0.000014\n",
      "Epoch 46, loss: 0.000014\n",
      "Epoch 47, loss: 0.000015\n",
      "Epoch 48, loss: 0.000015\n",
      "Epoch 49, loss: 0.000015\n",
      "Epoch 50, loss: 0.000015\n",
      "Epoch 51, loss: 0.000016\n",
      "Epoch 52, loss: 0.000016\n",
      "Epoch 53, loss: 0.000016\n",
      "Epoch 54, loss: 0.000017\n",
      "Epoch 55, loss: 0.000017\n",
      "Epoch 56, loss: 0.000017\n",
      "Epoch 57, loss: 0.000017\n",
      "Epoch 58, loss: 0.000018\n",
      "Epoch 59, loss: 0.000018\n",
      "Epoch 60, loss: 0.000019\n",
      "Epoch 61, loss: 0.000019\n",
      "Epoch 62, loss: 0.000019\n",
      "Epoch 63, loss: 0.000020\n",
      "Epoch 64, loss: 0.000020\n",
      "Epoch 65, loss: 0.000020\n",
      "Epoch 66, loss: 0.000021\n",
      "Epoch 67, loss: 0.000021\n",
      "Epoch 68, loss: 0.000021\n",
      "Epoch 69, loss: 0.000022\n",
      "Epoch 70, loss: 0.000022\n",
      "Epoch 71, loss: 0.000023\n",
      "Epoch 72, loss: 0.000023\n",
      "Epoch 73, loss: 0.000023\n",
      "Epoch 74, loss: 0.000024\n",
      "Epoch 75, loss: 0.000024\n",
      "Epoch 76, loss: 0.000024\n",
      "Epoch 77, loss: 0.000025\n",
      "Epoch 78, loss: 0.000025\n",
      "Epoch 79, loss: 0.000026\n",
      "Epoch 80, loss: 0.000026\n",
      "Epoch 81, loss: 0.000026\n",
      "Epoch 82, loss: 0.000027\n",
      "Epoch 83, loss: 0.000027\n",
      "Epoch 84, loss: 0.000028\n",
      "Epoch 85, loss: 0.000028\n",
      "Epoch 86, loss: 0.000029\n",
      "Epoch 87, loss: 0.000029\n",
      "Epoch 88, loss: 0.000029\n",
      "Epoch 89, loss: 0.000030\n",
      "Epoch 90, loss: 0.000030\n",
      "Epoch 91, loss: 0.000031\n",
      "Epoch 92, loss: 0.000031\n",
      "Epoch 93, loss: 0.000031\n",
      "Epoch 94, loss: 0.000032\n",
      "Epoch 95, loss: 0.000033\n",
      "Epoch 96, loss: 0.000033\n",
      "Epoch 97, loss: 0.000033\n",
      "Epoch 98, loss: 0.000034\n",
      "Epoch 99, loss: 0.000035\n",
      "accuracy = 0.18555555555555556\n",
      "learning_rate = 0.001, reg_strength = 1e-05\n",
      "Epoch 0, loss: 0.000001\n",
      "Epoch 1, loss: 0.000001\n",
      "Epoch 2, loss: 0.000001\n",
      "Epoch 3, loss: 0.000001\n",
      "Epoch 4, loss: 0.000001\n",
      "Epoch 5, loss: 0.000001\n",
      "Epoch 6, loss: 0.000001\n",
      "Epoch 7, loss: 0.000001\n",
      "Epoch 8, loss: 0.000001\n",
      "Epoch 9, loss: 0.000001\n",
      "Epoch 10, loss: 0.000001\n",
      "Epoch 11, loss: 0.000001\n",
      "Epoch 12, loss: 0.000001\n",
      "Epoch 13, loss: 0.000001\n",
      "Epoch 14, loss: 0.000001\n",
      "Epoch 15, loss: 0.000001\n",
      "Epoch 16, loss: 0.000001\n",
      "Epoch 17, loss: 0.000001\n",
      "Epoch 18, loss: 0.000001\n",
      "Epoch 19, loss: 0.000001\n",
      "Epoch 20, loss: 0.000001\n",
      "Epoch 21, loss: 0.000001\n",
      "Epoch 22, loss: 0.000001\n",
      "Epoch 23, loss: 0.000001\n",
      "Epoch 24, loss: 0.000001\n",
      "Epoch 25, loss: 0.000001\n",
      "Epoch 26, loss: 0.000001\n",
      "Epoch 27, loss: 0.000001\n",
      "Epoch 28, loss: 0.000001\n",
      "Epoch 29, loss: 0.000001\n",
      "Epoch 30, loss: 0.000001\n",
      "Epoch 31, loss: 0.000001\n",
      "Epoch 32, loss: 0.000001\n",
      "Epoch 33, loss: 0.000001\n",
      "Epoch 34, loss: 0.000001\n",
      "Epoch 35, loss: 0.000001\n",
      "Epoch 36, loss: 0.000001\n",
      "Epoch 37, loss: 0.000001\n",
      "Epoch 38, loss: 0.000001\n",
      "Epoch 39, loss: 0.000001\n",
      "Epoch 40, loss: 0.000001\n",
      "Epoch 41, loss: 0.000001\n",
      "Epoch 42, loss: 0.000001\n",
      "Epoch 43, loss: 0.000001\n",
      "Epoch 44, loss: 0.000001\n",
      "Epoch 45, loss: 0.000001\n",
      "Epoch 46, loss: 0.000001\n",
      "Epoch 47, loss: 0.000001\n",
      "Epoch 48, loss: 0.000001\n",
      "Epoch 49, loss: 0.000001\n",
      "Epoch 50, loss: 0.000002\n",
      "Epoch 51, loss: 0.000002\n",
      "Epoch 52, loss: 0.000002\n",
      "Epoch 53, loss: 0.000002\n",
      "Epoch 54, loss: 0.000002\n",
      "Epoch 55, loss: 0.000002\n",
      "Epoch 56, loss: 0.000002\n",
      "Epoch 57, loss: 0.000002\n",
      "Epoch 58, loss: 0.000002\n",
      "Epoch 59, loss: 0.000002\n",
      "Epoch 60, loss: 0.000002\n",
      "Epoch 61, loss: 0.000002\n",
      "Epoch 62, loss: 0.000002\n",
      "Epoch 63, loss: 0.000002\n",
      "Epoch 64, loss: 0.000002\n",
      "Epoch 65, loss: 0.000002\n",
      "Epoch 66, loss: 0.000002\n",
      "Epoch 67, loss: 0.000002\n",
      "Epoch 68, loss: 0.000002\n",
      "Epoch 69, loss: 0.000002\n",
      "Epoch 70, loss: 0.000002\n",
      "Epoch 71, loss: 0.000002\n",
      "Epoch 72, loss: 0.000002\n",
      "Epoch 73, loss: 0.000002\n",
      "Epoch 74, loss: 0.000002\n",
      "Epoch 75, loss: 0.000002\n",
      "Epoch 76, loss: 0.000002\n",
      "Epoch 77, loss: 0.000002\n",
      "Epoch 78, loss: 0.000003\n",
      "Epoch 79, loss: 0.000003\n",
      "Epoch 80, loss: 0.000003\n",
      "Epoch 81, loss: 0.000003\n",
      "Epoch 82, loss: 0.000003\n",
      "Epoch 83, loss: 0.000003\n",
      "Epoch 84, loss: 0.000003\n",
      "Epoch 85, loss: 0.000003\n",
      "Epoch 86, loss: 0.000003\n",
      "Epoch 87, loss: 0.000003\n",
      "Epoch 88, loss: 0.000003\n",
      "Epoch 89, loss: 0.000003\n",
      "Epoch 90, loss: 0.000003\n",
      "Epoch 91, loss: 0.000003\n",
      "Epoch 92, loss: 0.000003\n",
      "Epoch 93, loss: 0.000003\n",
      "Epoch 94, loss: 0.000003\n",
      "Epoch 95, loss: 0.000003\n",
      "Epoch 96, loss: 0.000003\n",
      "Epoch 97, loss: 0.000003\n",
      "Epoch 98, loss: 0.000003\n",
      "Epoch 99, loss: 0.000003\n",
      "accuracy = 0.1827777777777778\n",
      "learning_rate = 0.001, reg_strength = 1e-06\n",
      "Epoch 0, loss: 0.000000\n",
      "Epoch 1, loss: 0.000000\n",
      "Epoch 2, loss: 0.000000\n",
      "Epoch 3, loss: 0.000000\n",
      "Epoch 4, loss: 0.000000\n",
      "Epoch 5, loss: 0.000000\n",
      "Epoch 6, loss: 0.000000\n",
      "Epoch 7, loss: 0.000000\n",
      "Epoch 8, loss: 0.000000\n",
      "Epoch 9, loss: 0.000000\n",
      "Epoch 10, loss: 0.000000\n",
      "Epoch 11, loss: 0.000000\n",
      "Epoch 12, loss: 0.000000\n",
      "Epoch 13, loss: 0.000000\n",
      "Epoch 14, loss: 0.000000\n",
      "Epoch 15, loss: 0.000000\n",
      "Epoch 16, loss: 0.000000\n",
      "Epoch 17, loss: 0.000000\n",
      "Epoch 18, loss: 0.000000\n",
      "Epoch 19, loss: 0.000000\n",
      "Epoch 20, loss: 0.000000\n",
      "Epoch 21, loss: 0.000000\n",
      "Epoch 22, loss: 0.000000\n",
      "Epoch 23, loss: 0.000000\n",
      "Epoch 24, loss: 0.000000\n",
      "Epoch 25, loss: 0.000000\n",
      "Epoch 26, loss: 0.000000\n",
      "Epoch 27, loss: 0.000000\n",
      "Epoch 28, loss: 0.000000\n",
      "Epoch 29, loss: 0.000000\n",
      "Epoch 30, loss: 0.000000\n",
      "Epoch 31, loss: 0.000000\n",
      "Epoch 32, loss: 0.000000\n",
      "Epoch 33, loss: 0.000000\n",
      "Epoch 34, loss: 0.000000\n",
      "Epoch 35, loss: 0.000000\n",
      "Epoch 36, loss: 0.000000\n",
      "Epoch 37, loss: 0.000000\n",
      "Epoch 38, loss: 0.000000\n",
      "Epoch 39, loss: 0.000000\n",
      "Epoch 40, loss: 0.000000\n",
      "Epoch 41, loss: 0.000000\n",
      "Epoch 42, loss: 0.000000\n",
      "Epoch 43, loss: 0.000000\n",
      "Epoch 44, loss: 0.000000\n",
      "Epoch 45, loss: 0.000000\n",
      "Epoch 46, loss: 0.000000\n",
      "Epoch 47, loss: 0.000000\n",
      "Epoch 48, loss: 0.000000\n",
      "Epoch 49, loss: 0.000000\n",
      "Epoch 50, loss: 0.000000\n",
      "Epoch 51, loss: 0.000000\n",
      "Epoch 52, loss: 0.000000\n",
      "Epoch 53, loss: 0.000000\n",
      "Epoch 54, loss: 0.000000\n",
      "Epoch 55, loss: 0.000000\n",
      "Epoch 56, loss: 0.000000\n",
      "Epoch 57, loss: 0.000000\n",
      "Epoch 58, loss: 0.000000\n",
      "Epoch 59, loss: 0.000000\n",
      "Epoch 60, loss: 0.000000\n",
      "Epoch 61, loss: 0.000000\n",
      "Epoch 62, loss: 0.000000\n",
      "Epoch 63, loss: 0.000000\n",
      "Epoch 64, loss: 0.000000\n",
      "Epoch 65, loss: 0.000000\n",
      "Epoch 66, loss: 0.000000\n",
      "Epoch 67, loss: 0.000000\n",
      "Epoch 68, loss: 0.000000\n",
      "Epoch 69, loss: 0.000000\n",
      "Epoch 70, loss: 0.000000\n",
      "Epoch 71, loss: 0.000000\n",
      "Epoch 72, loss: 0.000000\n",
      "Epoch 73, loss: 0.000000\n",
      "Epoch 74, loss: 0.000000\n",
      "Epoch 75, loss: 0.000000\n",
      "Epoch 76, loss: 0.000000\n",
      "Epoch 77, loss: 0.000000\n",
      "Epoch 78, loss: 0.000000\n",
      "Epoch 79, loss: 0.000000\n",
      "Epoch 80, loss: 0.000000\n",
      "Epoch 81, loss: 0.000000\n",
      "Epoch 82, loss: 0.000000\n",
      "Epoch 83, loss: 0.000000\n",
      "Epoch 84, loss: 0.000000\n",
      "Epoch 85, loss: 0.000000\n",
      "Epoch 86, loss: 0.000000\n",
      "Epoch 87, loss: 0.000000\n",
      "Epoch 88, loss: 0.000000\n",
      "Epoch 89, loss: 0.000000\n",
      "Epoch 90, loss: 0.000000\n",
      "Epoch 91, loss: 0.000000\n",
      "Epoch 92, loss: 0.000000\n",
      "Epoch 93, loss: 0.000000\n",
      "Epoch 94, loss: 0.000000\n",
      "Epoch 95, loss: 0.000000\n",
      "Epoch 96, loss: 0.000000\n",
      "Epoch 97, loss: 0.000000\n",
      "Epoch 98, loss: 0.000000\n",
      "Epoch 99, loss: 0.000000\n",
      "accuracy = 0.18666666666666668\n",
      "learning_rate = 0.0001, reg_strength = 10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.640755\n",
      "Epoch 1, loss: 0.582189\n",
      "Epoch 2, loss: 0.528841\n",
      "Epoch 3, loss: 0.480371\n",
      "Epoch 4, loss: 0.436269\n",
      "Epoch 5, loss: 0.396312\n",
      "Epoch 6, loss: 0.360044\n",
      "Epoch 7, loss: 0.327080\n",
      "Epoch 8, loss: 0.297229\n",
      "Epoch 9, loss: 0.270063\n",
      "Epoch 10, loss: 0.245311\n",
      "Epoch 11, loss: 0.222974\n",
      "Epoch 12, loss: 0.202624\n",
      "Epoch 13, loss: 0.184143\n",
      "Epoch 14, loss: 0.167307\n",
      "Epoch 15, loss: 0.152094\n",
      "Epoch 16, loss: 0.138251\n",
      "Epoch 17, loss: 0.125706\n",
      "Epoch 18, loss: 0.114313\n",
      "Epoch 19, loss: 0.103906\n",
      "Epoch 20, loss: 0.094513\n",
      "Epoch 21, loss: 0.085921\n",
      "Epoch 22, loss: 0.078182\n",
      "Epoch 23, loss: 0.071103\n",
      "Epoch 24, loss: 0.064760\n",
      "Epoch 25, loss: 0.058956\n",
      "Epoch 26, loss: 0.053703\n",
      "Epoch 27, loss: 0.048905\n",
      "Epoch 28, loss: 0.044564\n",
      "Epoch 29, loss: 0.040629\n",
      "Epoch 30, loss: 0.037040\n",
      "Epoch 31, loss: 0.033775\n",
      "Epoch 32, loss: 0.030871\n",
      "Epoch 33, loss: 0.028180\n",
      "Epoch 34, loss: 0.025762\n",
      "Epoch 35, loss: 0.023544\n",
      "Epoch 36, loss: 0.021542\n",
      "Epoch 37, loss: 0.019721\n",
      "Epoch 38, loss: 0.018071\n",
      "Epoch 39, loss: 0.016577\n",
      "Epoch 40, loss: 0.015211\n",
      "Epoch 41, loss: 0.013976\n",
      "Epoch 42, loss: 0.012873\n",
      "Epoch 43, loss: 0.011848\n",
      "Epoch 44, loss: 0.010926\n",
      "Epoch 45, loss: 0.010102\n",
      "Epoch 46, loss: 0.009341\n",
      "Epoch 47, loss: 0.008655\n",
      "Epoch 48, loss: 0.008029\n",
      "Epoch 49, loss: 0.007461\n",
      "Epoch 50, loss: 0.006962\n",
      "Epoch 51, loss: 0.006482\n",
      "Epoch 52, loss: 0.006072\n",
      "Epoch 53, loss: 0.005686\n",
      "Epoch 54, loss: 0.005338\n",
      "Epoch 55, loss: 0.005024\n",
      "Epoch 56, loss: 0.004737\n",
      "Epoch 57, loss: 0.004485\n",
      "Epoch 58, loss: 0.004251\n",
      "Epoch 59, loss: 0.004029\n",
      "Epoch 60, loss: 0.003844\n",
      "Epoch 61, loss: 0.003666\n",
      "Epoch 62, loss: 0.003515\n",
      "Epoch 63, loss: 0.003367\n",
      "Epoch 64, loss: 0.003241\n",
      "Epoch 65, loss: 0.003115\n",
      "Epoch 66, loss: 0.003015\n",
      "Epoch 67, loss: 0.002922\n",
      "Epoch 68, loss: 0.002839\n",
      "Epoch 69, loss: 0.002757\n",
      "Epoch 70, loss: 0.002688\n",
      "Epoch 71, loss: 0.002621\n",
      "Epoch 72, loss: 0.002571\n",
      "Epoch 73, loss: 0.002509\n",
      "Epoch 74, loss: 0.002460\n",
      "Epoch 75, loss: 0.002417\n",
      "Epoch 76, loss: 0.002381\n",
      "Epoch 77, loss: 0.002341\n",
      "Epoch 78, loss: 0.002315\n",
      "Epoch 79, loss: 0.002287\n",
      "Epoch 80, loss: 0.002263\n",
      "Epoch 81, loss: 0.002235\n",
      "Epoch 82, loss: 0.002214\n",
      "Epoch 83, loss: 0.002193\n",
      "Epoch 84, loss: 0.002179\n",
      "Epoch 85, loss: 0.002166\n",
      "Epoch 86, loss: 0.002153\n",
      "Epoch 87, loss: 0.002138\n",
      "Epoch 88, loss: 0.002128\n",
      "Epoch 89, loss: 0.002120\n",
      "Epoch 90, loss: 0.002105\n",
      "Epoch 91, loss: 0.002100\n",
      "Epoch 92, loss: 0.002099\n",
      "Epoch 93, loss: 0.002082\n",
      "Epoch 94, loss: 0.002080\n",
      "Epoch 95, loss: 0.002080\n",
      "Epoch 96, loss: 0.002073\n",
      "Epoch 97, loss: 0.002066\n",
      "Epoch 98, loss: 0.002068\n",
      "Epoch 99, loss: 0.002062\n",
      "accuracy = 0.13166666666666665\n",
      "learning_rate = 0.0001, reg_strength = 1\n",
      "Epoch 0, loss: 0.069482\n",
      "Epoch 1, loss: 0.068827\n",
      "Epoch 2, loss: 0.068168\n",
      "Epoch 3, loss: 0.067538\n",
      "Epoch 4, loss: 0.066856\n",
      "Epoch 5, loss: 0.066213\n",
      "Epoch 6, loss: 0.065589\n",
      "Epoch 7, loss: 0.065009\n",
      "Epoch 8, loss: 0.064377\n",
      "Epoch 9, loss: 0.063760\n",
      "Epoch 10, loss: 0.063166\n",
      "Epoch 11, loss: 0.062592\n",
      "Epoch 12, loss: 0.061976\n",
      "Epoch 13, loss: 0.061380\n",
      "Epoch 14, loss: 0.060834\n",
      "Epoch 15, loss: 0.060272\n",
      "Epoch 16, loss: 0.059684\n",
      "Epoch 17, loss: 0.059156\n",
      "Epoch 18, loss: 0.058621\n",
      "Epoch 19, loss: 0.058081\n",
      "Epoch 20, loss: 0.057495\n",
      "Epoch 21, loss: 0.056964\n",
      "Epoch 22, loss: 0.056436\n",
      "Epoch 23, loss: 0.055951\n",
      "Epoch 24, loss: 0.055384\n",
      "Epoch 25, loss: 0.054962\n",
      "Epoch 26, loss: 0.054444\n",
      "Epoch 27, loss: 0.053910\n",
      "Epoch 28, loss: 0.053453\n",
      "Epoch 29, loss: 0.052979\n",
      "Epoch 30, loss: 0.052439\n",
      "Epoch 31, loss: 0.052003\n",
      "Epoch 32, loss: 0.051526\n",
      "Epoch 33, loss: 0.051044\n",
      "Epoch 34, loss: 0.050578\n",
      "Epoch 35, loss: 0.050136\n",
      "Epoch 36, loss: 0.049690\n",
      "Epoch 37, loss: 0.049312\n",
      "Epoch 38, loss: 0.048772\n",
      "Epoch 39, loss: 0.048354\n",
      "Epoch 40, loss: 0.047903\n",
      "Epoch 41, loss: 0.047481\n",
      "Epoch 42, loss: 0.047109\n",
      "Epoch 43, loss: 0.046647\n",
      "Epoch 44, loss: 0.046244\n",
      "Epoch 45, loss: 0.045863\n",
      "Epoch 46, loss: 0.045457\n",
      "Epoch 47, loss: 0.045014\n",
      "Epoch 48, loss: 0.044647\n",
      "Epoch 49, loss: 0.044251\n",
      "Epoch 50, loss: 0.043920\n",
      "Epoch 51, loss: 0.043549\n",
      "Epoch 52, loss: 0.043136\n",
      "Epoch 53, loss: 0.042754\n",
      "Epoch 54, loss: 0.042464\n",
      "Epoch 55, loss: 0.041959\n",
      "Epoch 56, loss: 0.041687\n",
      "Epoch 57, loss: 0.041285\n",
      "Epoch 58, loss: 0.040965\n",
      "Epoch 59, loss: 0.040617\n",
      "Epoch 60, loss: 0.040255\n",
      "Epoch 61, loss: 0.039919\n",
      "Epoch 62, loss: 0.039551\n",
      "Epoch 63, loss: 0.039256\n",
      "Epoch 64, loss: 0.038897\n",
      "Epoch 65, loss: 0.038638\n",
      "Epoch 66, loss: 0.038283\n",
      "Epoch 67, loss: 0.038011\n",
      "Epoch 68, loss: 0.037647\n",
      "Epoch 69, loss: 0.037327\n",
      "Epoch 70, loss: 0.036997\n",
      "Epoch 71, loss: 0.036715\n",
      "Epoch 72, loss: 0.036409\n",
      "Epoch 73, loss: 0.036093\n",
      "Epoch 74, loss: 0.035814\n",
      "Epoch 75, loss: 0.035527\n",
      "Epoch 76, loss: 0.035245\n",
      "Epoch 77, loss: 0.034945\n",
      "Epoch 78, loss: 0.034668\n",
      "Epoch 79, loss: 0.034463\n",
      "Epoch 80, loss: 0.034140\n",
      "Epoch 81, loss: 0.033886\n",
      "Epoch 82, loss: 0.033590\n",
      "Epoch 83, loss: 0.033341\n",
      "Epoch 84, loss: 0.033077\n",
      "Epoch 85, loss: 0.032752\n",
      "Epoch 86, loss: 0.032551\n",
      "Epoch 87, loss: 0.032277\n",
      "Epoch 88, loss: 0.032018\n",
      "Epoch 89, loss: 0.031794\n",
      "Epoch 90, loss: 0.031510\n",
      "Epoch 91, loss: 0.031318\n",
      "Epoch 92, loss: 0.031045\n",
      "Epoch 93, loss: 0.030853\n",
      "Epoch 94, loss: 0.030615\n",
      "Epoch 95, loss: 0.030347\n",
      "Epoch 96, loss: 0.030131\n",
      "Epoch 97, loss: 0.029906\n",
      "Epoch 98, loss: 0.029706\n",
      "Epoch 99, loss: 0.029464\n",
      "accuracy = 0.13777777777777778\n",
      "learning_rate = 0.0001, reg_strength = 0.1\n",
      "Epoch 0, loss: 0.007047\n",
      "Epoch 1, loss: 0.007040\n",
      "Epoch 2, loss: 0.007032\n",
      "Epoch 3, loss: 0.007026\n",
      "Epoch 4, loss: 0.007017\n",
      "Epoch 5, loss: 0.007015\n",
      "Epoch 6, loss: 0.007010\n",
      "Epoch 7, loss: 0.006994\n",
      "Epoch 8, loss: 0.006994\n",
      "Epoch 9, loss: 0.006984\n",
      "Epoch 10, loss: 0.006978\n",
      "Epoch 11, loss: 0.006974\n",
      "Epoch 12, loss: 0.006968\n",
      "Epoch 13, loss: 0.006963\n",
      "Epoch 14, loss: 0.006957\n",
      "Epoch 15, loss: 0.006948\n",
      "Epoch 16, loss: 0.006942\n",
      "Epoch 17, loss: 0.006935\n",
      "Epoch 18, loss: 0.006936\n",
      "Epoch 19, loss: 0.006930\n",
      "Epoch 20, loss: 0.006924\n",
      "Epoch 21, loss: 0.006925\n",
      "Epoch 22, loss: 0.006909\n",
      "Epoch 23, loss: 0.006906\n",
      "Epoch 24, loss: 0.006903\n",
      "Epoch 25, loss: 0.006893\n",
      "Epoch 26, loss: 0.006894\n",
      "Epoch 27, loss: 0.006888\n",
      "Epoch 28, loss: 0.006884\n",
      "Epoch 29, loss: 0.006883\n",
      "Epoch 30, loss: 0.006872\n",
      "Epoch 31, loss: 0.006873\n",
      "Epoch 32, loss: 0.006865\n",
      "Epoch 33, loss: 0.006866\n",
      "Epoch 34, loss: 0.006853\n",
      "Epoch 35, loss: 0.006858\n",
      "Epoch 36, loss: 0.006848\n",
      "Epoch 37, loss: 0.006849\n",
      "Epoch 38, loss: 0.006846\n",
      "Epoch 39, loss: 0.006845\n",
      "Epoch 40, loss: 0.006843\n",
      "Epoch 41, loss: 0.006826\n",
      "Epoch 42, loss: 0.006838\n",
      "Epoch 43, loss: 0.006824\n",
      "Epoch 44, loss: 0.006818\n",
      "Epoch 45, loss: 0.006813\n",
      "Epoch 46, loss: 0.006814\n",
      "Epoch 47, loss: 0.006816\n",
      "Epoch 48, loss: 0.006821\n",
      "Epoch 49, loss: 0.006806\n",
      "Epoch 50, loss: 0.006806\n",
      "Epoch 51, loss: 0.006807\n",
      "Epoch 52, loss: 0.006802\n",
      "Epoch 53, loss: 0.006804\n",
      "Epoch 54, loss: 0.006796\n",
      "Epoch 55, loss: 0.006788\n",
      "Epoch 56, loss: 0.006795\n",
      "Epoch 57, loss: 0.006794\n",
      "Epoch 58, loss: 0.006787\n",
      "Epoch 59, loss: 0.006787\n",
      "Epoch 60, loss: 0.006795\n",
      "Epoch 61, loss: 0.006784\n",
      "Epoch 62, loss: 0.006793\n",
      "Epoch 63, loss: 0.006779\n",
      "Epoch 64, loss: 0.006774\n",
      "Epoch 65, loss: 0.006781\n",
      "Epoch 66, loss: 0.006774\n",
      "Epoch 67, loss: 0.006770\n",
      "Epoch 68, loss: 0.006761\n",
      "Epoch 69, loss: 0.006769\n",
      "Epoch 70, loss: 0.006773\n",
      "Epoch 71, loss: 0.006768\n",
      "Epoch 72, loss: 0.006763\n",
      "Epoch 73, loss: 0.006758\n",
      "Epoch 74, loss: 0.006759\n",
      "Epoch 75, loss: 0.006764\n",
      "Epoch 76, loss: 0.006757\n",
      "Epoch 77, loss: 0.006770\n",
      "Epoch 78, loss: 0.006762\n",
      "Epoch 79, loss: 0.006752\n",
      "Epoch 80, loss: 0.006761\n",
      "Epoch 81, loss: 0.006761\n",
      "Epoch 82, loss: 0.006760\n",
      "Epoch 83, loss: 0.006768\n",
      "Epoch 84, loss: 0.006751\n",
      "Epoch 85, loss: 0.006758\n",
      "Epoch 86, loss: 0.006748\n",
      "Epoch 87, loss: 0.006755\n",
      "Epoch 88, loss: 0.006764\n",
      "Epoch 89, loss: 0.006744\n",
      "Epoch 90, loss: 0.006766\n",
      "Epoch 91, loss: 0.006766\n",
      "Epoch 92, loss: 0.006760\n",
      "Epoch 93, loss: 0.006754\n",
      "Epoch 94, loss: 0.006762\n",
      "Epoch 95, loss: 0.006752\n",
      "Epoch 96, loss: 0.006759\n",
      "Epoch 97, loss: 0.006768\n",
      "Epoch 98, loss: 0.006751\n",
      "Epoch 99, loss: 0.006771\n",
      "accuracy = 0.14\n",
      "learning_rate = 0.0001, reg_strength = 0.01\n",
      "Epoch 0, loss: 0.000707\n",
      "Epoch 1, loss: 0.000707\n",
      "Epoch 2, loss: 0.000707\n",
      "Epoch 3, loss: 0.000707\n",
      "Epoch 4, loss: 0.000707\n",
      "Epoch 5, loss: 0.000707\n",
      "Epoch 6, loss: 0.000706\n",
      "Epoch 7, loss: 0.000706\n",
      "Epoch 8, loss: 0.000706\n",
      "Epoch 9, loss: 0.000706\n",
      "Epoch 10, loss: 0.000706\n",
      "Epoch 11, loss: 0.000706\n",
      "Epoch 12, loss: 0.000707\n",
      "Epoch 13, loss: 0.000706\n",
      "Epoch 14, loss: 0.000707\n",
      "Epoch 15, loss: 0.000706\n",
      "Epoch 16, loss: 0.000706\n",
      "Epoch 17, loss: 0.000707\n",
      "Epoch 18, loss: 0.000706\n",
      "Epoch 19, loss: 0.000707\n",
      "Epoch 20, loss: 0.000707\n",
      "Epoch 21, loss: 0.000708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, loss: 0.000707\n",
      "Epoch 23, loss: 0.000707\n",
      "Epoch 24, loss: 0.000708\n",
      "Epoch 25, loss: 0.000708\n",
      "Epoch 26, loss: 0.000708\n",
      "Epoch 27, loss: 0.000708\n",
      "Epoch 28, loss: 0.000708\n",
      "Epoch 29, loss: 0.000708\n",
      "Epoch 30, loss: 0.000709\n",
      "Epoch 31, loss: 0.000709\n",
      "Epoch 32, loss: 0.000709\n",
      "Epoch 33, loss: 0.000710\n",
      "Epoch 34, loss: 0.000709\n",
      "Epoch 35, loss: 0.000710\n",
      "Epoch 36, loss: 0.000710\n",
      "Epoch 37, loss: 0.000710\n",
      "Epoch 38, loss: 0.000710\n",
      "Epoch 39, loss: 0.000710\n",
      "Epoch 40, loss: 0.000711\n",
      "Epoch 41, loss: 0.000711\n",
      "Epoch 42, loss: 0.000711\n",
      "Epoch 43, loss: 0.000713\n",
      "Epoch 44, loss: 0.000712\n",
      "Epoch 45, loss: 0.000712\n",
      "Epoch 46, loss: 0.000712\n",
      "Epoch 47, loss: 0.000712\n",
      "Epoch 48, loss: 0.000713\n",
      "Epoch 49, loss: 0.000713\n",
      "Epoch 50, loss: 0.000714\n",
      "Epoch 51, loss: 0.000714\n",
      "Epoch 52, loss: 0.000714\n",
      "Epoch 53, loss: 0.000715\n",
      "Epoch 54, loss: 0.000716\n",
      "Epoch 55, loss: 0.000715\n",
      "Epoch 56, loss: 0.000716\n",
      "Epoch 57, loss: 0.000716\n",
      "Epoch 58, loss: 0.000718\n",
      "Epoch 59, loss: 0.000718\n",
      "Epoch 60, loss: 0.000718\n",
      "Epoch 61, loss: 0.000719\n",
      "Epoch 62, loss: 0.000717\n",
      "Epoch 63, loss: 0.000719\n",
      "Epoch 64, loss: 0.000720\n",
      "Epoch 65, loss: 0.000720\n",
      "Epoch 66, loss: 0.000720\n",
      "Epoch 67, loss: 0.000720\n",
      "Epoch 68, loss: 0.000720\n",
      "Epoch 69, loss: 0.000721\n",
      "Epoch 70, loss: 0.000723\n",
      "Epoch 71, loss: 0.000723\n",
      "Epoch 72, loss: 0.000722\n",
      "Epoch 73, loss: 0.000725\n",
      "Epoch 74, loss: 0.000724\n",
      "Epoch 75, loss: 0.000724\n",
      "Epoch 76, loss: 0.000724\n",
      "Epoch 77, loss: 0.000724\n",
      "Epoch 78, loss: 0.000726\n",
      "Epoch 79, loss: 0.000726\n",
      "Epoch 80, loss: 0.000726\n",
      "Epoch 81, loss: 0.000728\n",
      "Epoch 82, loss: 0.000729\n",
      "Epoch 83, loss: 0.000728\n",
      "Epoch 84, loss: 0.000729\n",
      "Epoch 85, loss: 0.000732\n",
      "Epoch 86, loss: 0.000730\n",
      "Epoch 87, loss: 0.000731\n",
      "Epoch 88, loss: 0.000733\n",
      "Epoch 89, loss: 0.000733\n",
      "Epoch 90, loss: 0.000733\n",
      "Epoch 91, loss: 0.000734\n",
      "Epoch 92, loss: 0.000734\n",
      "Epoch 93, loss: 0.000735\n",
      "Epoch 94, loss: 0.000735\n",
      "Epoch 95, loss: 0.000735\n",
      "Epoch 96, loss: 0.000737\n",
      "Epoch 97, loss: 0.000737\n",
      "Epoch 98, loss: 0.000739\n",
      "Epoch 99, loss: 0.000739\n",
      "accuracy = 0.1461111111111111\n",
      "learning_rate = 0.0001, reg_strength = 0.001\n",
      "Epoch 0, loss: 0.000070\n",
      "Epoch 1, loss: 0.000070\n",
      "Epoch 2, loss: 0.000070\n",
      "Epoch 3, loss: 0.000070\n",
      "Epoch 4, loss: 0.000070\n",
      "Epoch 5, loss: 0.000070\n",
      "Epoch 6, loss: 0.000070\n",
      "Epoch 7, loss: 0.000070\n",
      "Epoch 8, loss: 0.000070\n",
      "Epoch 9, loss: 0.000070\n",
      "Epoch 10, loss: 0.000070\n",
      "Epoch 11, loss: 0.000070\n",
      "Epoch 12, loss: 0.000070\n",
      "Epoch 13, loss: 0.000070\n",
      "Epoch 14, loss: 0.000070\n",
      "Epoch 15, loss: 0.000070\n",
      "Epoch 16, loss: 0.000070\n",
      "Epoch 17, loss: 0.000070\n",
      "Epoch 18, loss: 0.000070\n",
      "Epoch 19, loss: 0.000070\n",
      "Epoch 20, loss: 0.000070\n",
      "Epoch 21, loss: 0.000070\n",
      "Epoch 22, loss: 0.000070\n",
      "Epoch 23, loss: 0.000070\n",
      "Epoch 24, loss: 0.000070\n",
      "Epoch 25, loss: 0.000070\n",
      "Epoch 26, loss: 0.000070\n",
      "Epoch 27, loss: 0.000070\n",
      "Epoch 28, loss: 0.000070\n",
      "Epoch 29, loss: 0.000070\n",
      "Epoch 30, loss: 0.000070\n",
      "Epoch 31, loss: 0.000070\n",
      "Epoch 32, loss: 0.000070\n",
      "Epoch 33, loss: 0.000070\n",
      "Epoch 34, loss: 0.000070\n",
      "Epoch 35, loss: 0.000070\n",
      "Epoch 36, loss: 0.000070\n",
      "Epoch 37, loss: 0.000070\n",
      "Epoch 38, loss: 0.000070\n",
      "Epoch 39, loss: 0.000070\n",
      "Epoch 40, loss: 0.000070\n",
      "Epoch 41, loss: 0.000070\n",
      "Epoch 42, loss: 0.000070\n",
      "Epoch 43, loss: 0.000070\n",
      "Epoch 44, loss: 0.000070\n",
      "Epoch 45, loss: 0.000071\n",
      "Epoch 46, loss: 0.000071\n",
      "Epoch 47, loss: 0.000071\n",
      "Epoch 48, loss: 0.000071\n",
      "Epoch 49, loss: 0.000071\n",
      "Epoch 50, loss: 0.000071\n",
      "Epoch 51, loss: 0.000071\n",
      "Epoch 52, loss: 0.000071\n",
      "Epoch 53, loss: 0.000071\n",
      "Epoch 54, loss: 0.000071\n",
      "Epoch 55, loss: 0.000071\n",
      "Epoch 56, loss: 0.000071\n",
      "Epoch 57, loss: 0.000071\n",
      "Epoch 58, loss: 0.000071\n",
      "Epoch 59, loss: 0.000071\n",
      "Epoch 60, loss: 0.000071\n",
      "Epoch 61, loss: 0.000071\n",
      "Epoch 62, loss: 0.000071\n",
      "Epoch 63, loss: 0.000071\n",
      "Epoch 64, loss: 0.000071\n",
      "Epoch 65, loss: 0.000071\n",
      "Epoch 66, loss: 0.000071\n",
      "Epoch 67, loss: 0.000072\n",
      "Epoch 68, loss: 0.000071\n",
      "Epoch 69, loss: 0.000071\n",
      "Epoch 70, loss: 0.000072\n",
      "Epoch 71, loss: 0.000072\n",
      "Epoch 72, loss: 0.000072\n",
      "Epoch 73, loss: 0.000072\n",
      "Epoch 74, loss: 0.000072\n",
      "Epoch 75, loss: 0.000072\n",
      "Epoch 76, loss: 0.000072\n",
      "Epoch 77, loss: 0.000072\n",
      "Epoch 78, loss: 0.000072\n",
      "Epoch 79, loss: 0.000072\n",
      "Epoch 80, loss: 0.000072\n",
      "Epoch 81, loss: 0.000072\n",
      "Epoch 82, loss: 0.000072\n",
      "Epoch 83, loss: 0.000072\n",
      "Epoch 84, loss: 0.000072\n",
      "Epoch 85, loss: 0.000072\n",
      "Epoch 86, loss: 0.000072\n",
      "Epoch 87, loss: 0.000073\n",
      "Epoch 88, loss: 0.000073\n",
      "Epoch 89, loss: 0.000073\n",
      "Epoch 90, loss: 0.000073\n",
      "Epoch 91, loss: 0.000073\n",
      "Epoch 92, loss: 0.000073\n",
      "Epoch 93, loss: 0.000073\n",
      "Epoch 94, loss: 0.000073\n",
      "Epoch 95, loss: 0.000073\n",
      "Epoch 96, loss: 0.000073\n",
      "Epoch 97, loss: 0.000073\n",
      "Epoch 98, loss: 0.000073\n",
      "Epoch 99, loss: 0.000073\n",
      "accuracy = 0.14333333333333334\n",
      "learning_rate = 0.0001, reg_strength = 0.0001\n",
      "Epoch 0, loss: 0.000007\n",
      "Epoch 1, loss: 0.000007\n",
      "Epoch 2, loss: 0.000007\n",
      "Epoch 3, loss: 0.000007\n",
      "Epoch 4, loss: 0.000007\n",
      "Epoch 5, loss: 0.000007\n",
      "Epoch 6, loss: 0.000007\n",
      "Epoch 7, loss: 0.000007\n",
      "Epoch 8, loss: 0.000007\n",
      "Epoch 9, loss: 0.000007\n",
      "Epoch 10, loss: 0.000007\n",
      "Epoch 11, loss: 0.000007\n",
      "Epoch 12, loss: 0.000007\n",
      "Epoch 13, loss: 0.000007\n",
      "Epoch 14, loss: 0.000007\n",
      "Epoch 15, loss: 0.000007\n",
      "Epoch 16, loss: 0.000007\n",
      "Epoch 17, loss: 0.000007\n",
      "Epoch 18, loss: 0.000007\n",
      "Epoch 19, loss: 0.000007\n",
      "Epoch 20, loss: 0.000007\n",
      "Epoch 21, loss: 0.000007\n",
      "Epoch 22, loss: 0.000007\n",
      "Epoch 23, loss: 0.000007\n",
      "Epoch 24, loss: 0.000007\n",
      "Epoch 25, loss: 0.000007\n",
      "Epoch 26, loss: 0.000007\n",
      "Epoch 27, loss: 0.000007\n",
      "Epoch 28, loss: 0.000007\n",
      "Epoch 29, loss: 0.000007\n",
      "Epoch 30, loss: 0.000007\n",
      "Epoch 31, loss: 0.000007\n",
      "Epoch 32, loss: 0.000007\n",
      "Epoch 33, loss: 0.000007\n",
      "Epoch 34, loss: 0.000007\n",
      "Epoch 35, loss: 0.000007\n",
      "Epoch 36, loss: 0.000007\n",
      "Epoch 37, loss: 0.000007\n",
      "Epoch 38, loss: 0.000007\n",
      "Epoch 39, loss: 0.000007\n",
      "Epoch 40, loss: 0.000007\n",
      "Epoch 41, loss: 0.000007\n",
      "Epoch 42, loss: 0.000007\n",
      "Epoch 43, loss: 0.000007\n",
      "Epoch 44, loss: 0.000007\n",
      "Epoch 45, loss: 0.000007\n",
      "Epoch 46, loss: 0.000007\n",
      "Epoch 47, loss: 0.000007\n",
      "Epoch 48, loss: 0.000007\n",
      "Epoch 49, loss: 0.000007\n",
      "Epoch 50, loss: 0.000007\n",
      "Epoch 51, loss: 0.000007\n",
      "Epoch 52, loss: 0.000007\n",
      "Epoch 53, loss: 0.000007\n",
      "Epoch 54, loss: 0.000007\n",
      "Epoch 55, loss: 0.000007\n",
      "Epoch 56, loss: 0.000007\n",
      "Epoch 57, loss: 0.000007\n",
      "Epoch 58, loss: 0.000007\n",
      "Epoch 59, loss: 0.000007\n",
      "Epoch 60, loss: 0.000007\n",
      "Epoch 61, loss: 0.000007\n",
      "Epoch 62, loss: 0.000007\n",
      "Epoch 63, loss: 0.000007\n",
      "Epoch 64, loss: 0.000007\n",
      "Epoch 65, loss: 0.000007\n",
      "Epoch 66, loss: 0.000007\n",
      "Epoch 67, loss: 0.000007\n",
      "Epoch 68, loss: 0.000007\n",
      "Epoch 69, loss: 0.000007\n",
      "Epoch 70, loss: 0.000007\n",
      "Epoch 71, loss: 0.000007\n",
      "Epoch 72, loss: 0.000007\n",
      "Epoch 73, loss: 0.000007\n",
      "Epoch 74, loss: 0.000007\n",
      "Epoch 75, loss: 0.000007\n",
      "Epoch 76, loss: 0.000007\n",
      "Epoch 77, loss: 0.000007\n",
      "Epoch 78, loss: 0.000007\n",
      "Epoch 79, loss: 0.000007\n",
      "Epoch 80, loss: 0.000007\n",
      "Epoch 81, loss: 0.000007\n",
      "Epoch 82, loss: 0.000007\n",
      "Epoch 83, loss: 0.000007\n",
      "Epoch 84, loss: 0.000007\n",
      "Epoch 85, loss: 0.000007\n",
      "Epoch 86, loss: 0.000007\n",
      "Epoch 87, loss: 0.000007\n",
      "Epoch 88, loss: 0.000007\n",
      "Epoch 89, loss: 0.000007\n",
      "Epoch 90, loss: 0.000007\n",
      "Epoch 91, loss: 0.000007\n",
      "Epoch 92, loss: 0.000007\n",
      "Epoch 93, loss: 0.000007\n",
      "Epoch 94, loss: 0.000007\n",
      "Epoch 95, loss: 0.000007\n",
      "Epoch 96, loss: 0.000007\n",
      "Epoch 97, loss: 0.000007\n",
      "Epoch 98, loss: 0.000007\n",
      "Epoch 99, loss: 0.000007\n",
      "accuracy = 0.1438888888888889\n",
      "learning_rate = 0.0001, reg_strength = 1e-05\n",
      "Epoch 0, loss: 0.000001\n",
      "Epoch 1, loss: 0.000001\n",
      "Epoch 2, loss: 0.000001\n",
      "Epoch 3, loss: 0.000001\n",
      "Epoch 4, loss: 0.000001\n",
      "Epoch 5, loss: 0.000001\n",
      "Epoch 6, loss: 0.000001\n",
      "Epoch 7, loss: 0.000001\n",
      "Epoch 8, loss: 0.000001\n",
      "Epoch 9, loss: 0.000001\n",
      "Epoch 10, loss: 0.000001\n",
      "Epoch 11, loss: 0.000001\n",
      "Epoch 12, loss: 0.000001\n",
      "Epoch 13, loss: 0.000001\n",
      "Epoch 14, loss: 0.000001\n",
      "Epoch 15, loss: 0.000001\n",
      "Epoch 16, loss: 0.000001\n",
      "Epoch 17, loss: 0.000001\n",
      "Epoch 18, loss: 0.000001\n",
      "Epoch 19, loss: 0.000001\n",
      "Epoch 20, loss: 0.000001\n",
      "Epoch 21, loss: 0.000001\n",
      "Epoch 22, loss: 0.000001\n",
      "Epoch 23, loss: 0.000001\n",
      "Epoch 24, loss: 0.000001\n",
      "Epoch 25, loss: 0.000001\n",
      "Epoch 26, loss: 0.000001\n",
      "Epoch 27, loss: 0.000001\n",
      "Epoch 28, loss: 0.000001\n",
      "Epoch 29, loss: 0.000001\n",
      "Epoch 30, loss: 0.000001\n",
      "Epoch 31, loss: 0.000001\n",
      "Epoch 32, loss: 0.000001\n",
      "Epoch 33, loss: 0.000001\n",
      "Epoch 34, loss: 0.000001\n",
      "Epoch 35, loss: 0.000001\n",
      "Epoch 36, loss: 0.000001\n",
      "Epoch 37, loss: 0.000001\n",
      "Epoch 38, loss: 0.000001\n",
      "Epoch 39, loss: 0.000001\n",
      "Epoch 40, loss: 0.000001\n",
      "Epoch 41, loss: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, loss: 0.000001\n",
      "Epoch 43, loss: 0.000001\n",
      "Epoch 44, loss: 0.000001\n",
      "Epoch 45, loss: 0.000001\n",
      "Epoch 46, loss: 0.000001\n",
      "Epoch 47, loss: 0.000001\n",
      "Epoch 48, loss: 0.000001\n",
      "Epoch 49, loss: 0.000001\n",
      "Epoch 50, loss: 0.000001\n",
      "Epoch 51, loss: 0.000001\n",
      "Epoch 52, loss: 0.000001\n",
      "Epoch 53, loss: 0.000001\n",
      "Epoch 54, loss: 0.000001\n",
      "Epoch 55, loss: 0.000001\n",
      "Epoch 56, loss: 0.000001\n",
      "Epoch 57, loss: 0.000001\n",
      "Epoch 58, loss: 0.000001\n",
      "Epoch 59, loss: 0.000001\n",
      "Epoch 60, loss: 0.000001\n",
      "Epoch 61, loss: 0.000001\n",
      "Epoch 62, loss: 0.000001\n",
      "Epoch 63, loss: 0.000001\n",
      "Epoch 64, loss: 0.000001\n",
      "Epoch 65, loss: 0.000001\n",
      "Epoch 66, loss: 0.000001\n",
      "Epoch 67, loss: 0.000001\n",
      "Epoch 68, loss: 0.000001\n",
      "Epoch 69, loss: 0.000001\n",
      "Epoch 70, loss: 0.000001\n",
      "Epoch 71, loss: 0.000001\n",
      "Epoch 72, loss: 0.000001\n",
      "Epoch 73, loss: 0.000001\n",
      "Epoch 74, loss: 0.000001\n",
      "Epoch 75, loss: 0.000001\n",
      "Epoch 76, loss: 0.000001\n",
      "Epoch 77, loss: 0.000001\n",
      "Epoch 78, loss: 0.000001\n",
      "Epoch 79, loss: 0.000001\n",
      "Epoch 80, loss: 0.000001\n",
      "Epoch 81, loss: 0.000001\n",
      "Epoch 82, loss: 0.000001\n",
      "Epoch 83, loss: 0.000001\n",
      "Epoch 84, loss: 0.000001\n",
      "Epoch 85, loss: 0.000001\n",
      "Epoch 86, loss: 0.000001\n",
      "Epoch 87, loss: 0.000001\n",
      "Epoch 88, loss: 0.000001\n",
      "Epoch 89, loss: 0.000001\n",
      "Epoch 90, loss: 0.000001\n",
      "Epoch 91, loss: 0.000001\n",
      "Epoch 92, loss: 0.000001\n",
      "Epoch 93, loss: 0.000001\n",
      "Epoch 94, loss: 0.000001\n",
      "Epoch 95, loss: 0.000001\n",
      "Epoch 96, loss: 0.000001\n",
      "Epoch 97, loss: 0.000001\n",
      "Epoch 98, loss: 0.000001\n",
      "Epoch 99, loss: 0.000001\n",
      "accuracy = 0.14722222222222223\n",
      "learning_rate = 0.0001, reg_strength = 1e-06\n",
      "Epoch 0, loss: 0.000000\n",
      "Epoch 1, loss: 0.000000\n",
      "Epoch 2, loss: 0.000000\n",
      "Epoch 3, loss: 0.000000\n",
      "Epoch 4, loss: 0.000000\n",
      "Epoch 5, loss: 0.000000\n",
      "Epoch 6, loss: 0.000000\n",
      "Epoch 7, loss: 0.000000\n",
      "Epoch 8, loss: 0.000000\n",
      "Epoch 9, loss: 0.000000\n",
      "Epoch 10, loss: 0.000000\n",
      "Epoch 11, loss: 0.000000\n",
      "Epoch 12, loss: 0.000000\n",
      "Epoch 13, loss: 0.000000\n",
      "Epoch 14, loss: 0.000000\n",
      "Epoch 15, loss: 0.000000\n",
      "Epoch 16, loss: 0.000000\n",
      "Epoch 17, loss: 0.000000\n",
      "Epoch 18, loss: 0.000000\n",
      "Epoch 19, loss: 0.000000\n",
      "Epoch 20, loss: 0.000000\n",
      "Epoch 21, loss: 0.000000\n",
      "Epoch 22, loss: 0.000000\n",
      "Epoch 23, loss: 0.000000\n",
      "Epoch 24, loss: 0.000000\n",
      "Epoch 25, loss: 0.000000\n",
      "Epoch 26, loss: 0.000000\n",
      "Epoch 27, loss: 0.000000\n",
      "Epoch 28, loss: 0.000000\n",
      "Epoch 29, loss: 0.000000\n",
      "Epoch 30, loss: 0.000000\n",
      "Epoch 31, loss: 0.000000\n",
      "Epoch 32, loss: 0.000000\n",
      "Epoch 33, loss: 0.000000\n",
      "Epoch 34, loss: 0.000000\n",
      "Epoch 35, loss: 0.000000\n",
      "Epoch 36, loss: 0.000000\n",
      "Epoch 37, loss: 0.000000\n",
      "Epoch 38, loss: 0.000000\n",
      "Epoch 39, loss: 0.000000\n",
      "Epoch 40, loss: 0.000000\n",
      "Epoch 41, loss: 0.000000\n",
      "Epoch 42, loss: 0.000000\n",
      "Epoch 43, loss: 0.000000\n",
      "Epoch 44, loss: 0.000000\n",
      "Epoch 45, loss: 0.000000\n",
      "Epoch 46, loss: 0.000000\n",
      "Epoch 47, loss: 0.000000\n",
      "Epoch 48, loss: 0.000000\n",
      "Epoch 49, loss: 0.000000\n",
      "Epoch 50, loss: 0.000000\n",
      "Epoch 51, loss: 0.000000\n",
      "Epoch 52, loss: 0.000000\n",
      "Epoch 53, loss: 0.000000\n",
      "Epoch 54, loss: 0.000000\n",
      "Epoch 55, loss: 0.000000\n",
      "Epoch 56, loss: 0.000000\n",
      "Epoch 57, loss: 0.000000\n",
      "Epoch 58, loss: 0.000000\n",
      "Epoch 59, loss: 0.000000\n",
      "Epoch 60, loss: 0.000000\n",
      "Epoch 61, loss: 0.000000\n",
      "Epoch 62, loss: 0.000000\n",
      "Epoch 63, loss: 0.000000\n",
      "Epoch 64, loss: 0.000000\n",
      "Epoch 65, loss: 0.000000\n",
      "Epoch 66, loss: 0.000000\n",
      "Epoch 67, loss: 0.000000\n",
      "Epoch 68, loss: 0.000000\n",
      "Epoch 69, loss: 0.000000\n",
      "Epoch 70, loss: 0.000000\n",
      "Epoch 71, loss: 0.000000\n",
      "Epoch 72, loss: 0.000000\n",
      "Epoch 73, loss: 0.000000\n",
      "Epoch 74, loss: 0.000000\n",
      "Epoch 75, loss: 0.000000\n",
      "Epoch 76, loss: 0.000000\n",
      "Epoch 77, loss: 0.000000\n",
      "Epoch 78, loss: 0.000000\n",
      "Epoch 79, loss: 0.000000\n",
      "Epoch 80, loss: 0.000000\n",
      "Epoch 81, loss: 0.000000\n",
      "Epoch 82, loss: 0.000000\n",
      "Epoch 83, loss: 0.000000\n",
      "Epoch 84, loss: 0.000000\n",
      "Epoch 85, loss: 0.000000\n",
      "Epoch 86, loss: 0.000000\n",
      "Epoch 87, loss: 0.000000\n",
      "Epoch 88, loss: 0.000000\n",
      "Epoch 89, loss: 0.000000\n",
      "Epoch 90, loss: 0.000000\n",
      "Epoch 91, loss: 0.000000\n",
      "Epoch 92, loss: 0.000000\n",
      "Epoch 93, loss: 0.000000\n",
      "Epoch 94, loss: 0.000000\n",
      "Epoch 95, loss: 0.000000\n",
      "Epoch 96, loss: 0.000000\n",
      "Epoch 97, loss: 0.000000\n",
      "Epoch 98, loss: 0.000000\n",
      "Epoch 99, loss: 0.000000\n",
      "accuracy = 0.14\n",
      "learning_rate = 1e-05, reg_strength = 10.0\n",
      "Epoch 0, loss: 0.708052\n",
      "Epoch 1, loss: 0.701038\n",
      "Epoch 2, loss: 0.694312\n",
      "Epoch 3, loss: 0.688081\n",
      "Epoch 4, loss: 0.681225\n",
      "Epoch 5, loss: 0.674706\n",
      "Epoch 6, loss: 0.668166\n",
      "Epoch 7, loss: 0.661775\n",
      "Epoch 8, loss: 0.655533\n",
      "Epoch 9, loss: 0.649384\n",
      "Epoch 10, loss: 0.642994\n",
      "Epoch 11, loss: 0.636982\n",
      "Epoch 12, loss: 0.630951\n",
      "Epoch 13, loss: 0.625064\n",
      "Epoch 14, loss: 0.619014\n",
      "Epoch 15, loss: 0.612942\n",
      "Epoch 16, loss: 0.607233\n",
      "Epoch 17, loss: 0.601177\n",
      "Epoch 18, loss: 0.595747\n",
      "Epoch 19, loss: 0.589835\n",
      "Epoch 20, loss: 0.584208\n",
      "Epoch 21, loss: 0.578558\n",
      "Epoch 22, loss: 0.573136\n",
      "Epoch 23, loss: 0.567631\n",
      "Epoch 24, loss: 0.562345\n",
      "Epoch 25, loss: 0.556989\n",
      "Epoch 26, loss: 0.551613\n",
      "Epoch 27, loss: 0.546009\n",
      "Epoch 28, loss: 0.541141\n",
      "Epoch 29, loss: 0.535879\n",
      "Epoch 30, loss: 0.530793\n",
      "Epoch 31, loss: 0.525830\n",
      "Epoch 32, loss: 0.520750\n",
      "Epoch 33, loss: 0.515549\n",
      "Epoch 34, loss: 0.510807\n",
      "Epoch 35, loss: 0.505921\n",
      "Epoch 36, loss: 0.501082\n",
      "Epoch 37, loss: 0.496272\n",
      "Epoch 38, loss: 0.491553\n",
      "Epoch 39, loss: 0.486856\n",
      "Epoch 40, loss: 0.482399\n",
      "Epoch 41, loss: 0.477558\n",
      "Epoch 42, loss: 0.473146\n",
      "Epoch 43, loss: 0.468377\n",
      "Epoch 44, loss: 0.464127\n",
      "Epoch 45, loss: 0.459541\n",
      "Epoch 46, loss: 0.455353\n",
      "Epoch 47, loss: 0.451011\n",
      "Epoch 48, loss: 0.446606\n",
      "Epoch 49, loss: 0.442336\n",
      "Epoch 50, loss: 0.438133\n",
      "Epoch 51, loss: 0.433902\n",
      "Epoch 52, loss: 0.429792\n",
      "Epoch 53, loss: 0.425765\n",
      "Epoch 54, loss: 0.421554\n",
      "Epoch 55, loss: 0.417618\n",
      "Epoch 56, loss: 0.413672\n",
      "Epoch 57, loss: 0.409756\n",
      "Epoch 58, loss: 0.405772\n",
      "Epoch 59, loss: 0.401872\n",
      "Epoch 60, loss: 0.398030\n",
      "Epoch 61, loss: 0.394347\n",
      "Epoch 62, loss: 0.390475\n",
      "Epoch 63, loss: 0.386733\n",
      "Epoch 64, loss: 0.383153\n",
      "Epoch 65, loss: 0.379474\n",
      "Epoch 66, loss: 0.375863\n",
      "Epoch 67, loss: 0.372266\n",
      "Epoch 68, loss: 0.368625\n",
      "Epoch 69, loss: 0.365216\n",
      "Epoch 70, loss: 0.361818\n",
      "Epoch 71, loss: 0.358232\n",
      "Epoch 72, loss: 0.354976\n",
      "Epoch 73, loss: 0.351476\n",
      "Epoch 74, loss: 0.348008\n",
      "Epoch 75, loss: 0.344716\n",
      "Epoch 76, loss: 0.341422\n",
      "Epoch 77, loss: 0.338194\n",
      "Epoch 78, loss: 0.335104\n",
      "Epoch 79, loss: 0.331770\n",
      "Epoch 80, loss: 0.328680\n",
      "Epoch 81, loss: 0.325408\n",
      "Epoch 82, loss: 0.322471\n",
      "Epoch 83, loss: 0.319470\n",
      "Epoch 84, loss: 0.316218\n",
      "Epoch 85, loss: 0.313227\n",
      "Epoch 86, loss: 0.310391\n",
      "Epoch 87, loss: 0.307427\n",
      "Epoch 88, loss: 0.304424\n",
      "Epoch 89, loss: 0.301503\n",
      "Epoch 90, loss: 0.298551\n",
      "Epoch 91, loss: 0.295808\n",
      "Epoch 92, loss: 0.292979\n",
      "Epoch 93, loss: 0.290133\n",
      "Epoch 94, loss: 0.287506\n",
      "Epoch 95, loss: 0.284749\n",
      "Epoch 96, loss: 0.281875\n",
      "Epoch 97, loss: 0.279304\n",
      "Epoch 98, loss: 0.276631\n",
      "Epoch 99, loss: 0.273927\n",
      "accuracy = 0.11222222222222222\n",
      "learning_rate = 1e-05, reg_strength = 1\n",
      "Epoch 0, loss: 0.070719\n",
      "Epoch 1, loss: 0.070640\n",
      "Epoch 2, loss: 0.070591\n",
      "Epoch 3, loss: 0.070525\n",
      "Epoch 4, loss: 0.070458\n",
      "Epoch 5, loss: 0.070403\n",
      "Epoch 6, loss: 0.070319\n",
      "Epoch 7, loss: 0.070262\n",
      "Epoch 8, loss: 0.070203\n",
      "Epoch 9, loss: 0.070124\n",
      "Epoch 10, loss: 0.070068\n",
      "Epoch 11, loss: 0.069982\n",
      "Epoch 12, loss: 0.069926\n",
      "Epoch 13, loss: 0.069853\n",
      "Epoch 14, loss: 0.069809\n",
      "Epoch 15, loss: 0.069745\n",
      "Epoch 16, loss: 0.069655\n",
      "Epoch 17, loss: 0.069573\n",
      "Epoch 18, loss: 0.069497\n",
      "Epoch 19, loss: 0.069470\n",
      "Epoch 20, loss: 0.069387\n",
      "Epoch 21, loss: 0.069312\n",
      "Epoch 22, loss: 0.069263\n",
      "Epoch 23, loss: 0.069209\n",
      "Epoch 24, loss: 0.069097\n",
      "Epoch 25, loss: 0.069087\n",
      "Epoch 26, loss: 0.069003\n",
      "Epoch 27, loss: 0.068924\n",
      "Epoch 28, loss: 0.068827\n",
      "Epoch 29, loss: 0.068788\n",
      "Epoch 30, loss: 0.068708\n",
      "Epoch 31, loss: 0.068671\n",
      "Epoch 32, loss: 0.068606\n",
      "Epoch 33, loss: 0.068551\n",
      "Epoch 34, loss: 0.068461\n",
      "Epoch 35, loss: 0.068400\n",
      "Epoch 36, loss: 0.068345\n",
      "Epoch 37, loss: 0.068290\n",
      "Epoch 38, loss: 0.068176\n",
      "Epoch 39, loss: 0.068153\n",
      "Epoch 40, loss: 0.068046\n",
      "Epoch 41, loss: 0.067991\n",
      "Epoch 42, loss: 0.067945\n",
      "Epoch 43, loss: 0.067870\n",
      "Epoch 44, loss: 0.067833\n",
      "Epoch 45, loss: 0.067705\n",
      "Epoch 46, loss: 0.067677\n",
      "Epoch 47, loss: 0.067642\n",
      "Epoch 48, loss: 0.067584\n",
      "Epoch 49, loss: 0.067500\n",
      "Epoch 50, loss: 0.067411\n",
      "Epoch 51, loss: 0.067350\n",
      "Epoch 52, loss: 0.067293\n",
      "Epoch 53, loss: 0.067224\n",
      "Epoch 54, loss: 0.067175\n",
      "Epoch 55, loss: 0.067110\n",
      "Epoch 56, loss: 0.067019\n",
      "Epoch 57, loss: 0.066975\n",
      "Epoch 58, loss: 0.066911\n",
      "Epoch 59, loss: 0.066828\n",
      "Epoch 60, loss: 0.066777\n",
      "Epoch 61, loss: 0.066734\n",
      "Epoch 62, loss: 0.066673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63, loss: 0.066554\n",
      "Epoch 64, loss: 0.066542\n",
      "Epoch 65, loss: 0.066471\n",
      "Epoch 66, loss: 0.066430\n",
      "Epoch 67, loss: 0.066334\n",
      "Epoch 68, loss: 0.066282\n",
      "Epoch 69, loss: 0.066191\n",
      "Epoch 70, loss: 0.066134\n",
      "Epoch 71, loss: 0.066101\n",
      "Epoch 72, loss: 0.066016\n",
      "Epoch 73, loss: 0.065964\n",
      "Epoch 74, loss: 0.065899\n",
      "Epoch 75, loss: 0.065823\n",
      "Epoch 76, loss: 0.065767\n",
      "Epoch 77, loss: 0.065723\n",
      "Epoch 78, loss: 0.065630\n",
      "Epoch 79, loss: 0.065591\n",
      "Epoch 80, loss: 0.065534\n",
      "Epoch 81, loss: 0.065470\n",
      "Epoch 82, loss: 0.065411\n",
      "Epoch 83, loss: 0.065333\n",
      "Epoch 84, loss: 0.065312\n",
      "Epoch 85, loss: 0.065247\n",
      "Epoch 86, loss: 0.065121\n",
      "Epoch 87, loss: 0.065133\n",
      "Epoch 88, loss: 0.065012\n",
      "Epoch 89, loss: 0.064949\n",
      "Epoch 90, loss: 0.064903\n",
      "Epoch 91, loss: 0.064855\n",
      "Epoch 92, loss: 0.064773\n",
      "Epoch 93, loss: 0.064725\n",
      "Epoch 94, loss: 0.064628\n",
      "Epoch 95, loss: 0.064602\n",
      "Epoch 96, loss: 0.064532\n",
      "Epoch 97, loss: 0.064486\n",
      "Epoch 98, loss: 0.064429\n",
      "Epoch 99, loss: 0.064367\n",
      "accuracy = 0.11055555555555556\n",
      "learning_rate = 1e-05, reg_strength = 0.1\n",
      "Epoch 0, loss: 0.007067\n",
      "Epoch 1, loss: 0.007065\n",
      "Epoch 2, loss: 0.007064\n",
      "Epoch 3, loss: 0.007069\n",
      "Epoch 4, loss: 0.007060\n",
      "Epoch 5, loss: 0.007060\n",
      "Epoch 6, loss: 0.007068\n",
      "Epoch 7, loss: 0.007062\n",
      "Epoch 8, loss: 0.007059\n",
      "Epoch 9, loss: 0.007059\n",
      "Epoch 10, loss: 0.007058\n",
      "Epoch 11, loss: 0.007061\n",
      "Epoch 12, loss: 0.007058\n",
      "Epoch 13, loss: 0.007059\n",
      "Epoch 14, loss: 0.007060\n",
      "Epoch 15, loss: 0.007056\n",
      "Epoch 16, loss: 0.007053\n",
      "Epoch 17, loss: 0.007057\n",
      "Epoch 18, loss: 0.007053\n",
      "Epoch 19, loss: 0.007055\n",
      "Epoch 20, loss: 0.007053\n",
      "Epoch 21, loss: 0.007052\n",
      "Epoch 22, loss: 0.007052\n",
      "Epoch 23, loss: 0.007048\n",
      "Epoch 24, loss: 0.007052\n",
      "Epoch 25, loss: 0.007044\n",
      "Epoch 26, loss: 0.007051\n",
      "Epoch 27, loss: 0.007048\n",
      "Epoch 28, loss: 0.007049\n",
      "Epoch 29, loss: 0.007046\n",
      "Epoch 30, loss: 0.007046\n",
      "Epoch 31, loss: 0.007042\n",
      "Epoch 32, loss: 0.007044\n",
      "Epoch 33, loss: 0.007036\n",
      "Epoch 34, loss: 0.007041\n",
      "Epoch 35, loss: 0.007041\n",
      "Epoch 36, loss: 0.007041\n",
      "Epoch 37, loss: 0.007041\n",
      "Epoch 38, loss: 0.007040\n",
      "Epoch 39, loss: 0.007041\n",
      "Epoch 40, loss: 0.007037\n",
      "Epoch 41, loss: 0.007039\n",
      "Epoch 42, loss: 0.007035\n",
      "Epoch 43, loss: 0.007038\n",
      "Epoch 44, loss: 0.007032\n",
      "Epoch 45, loss: 0.007033\n",
      "Epoch 46, loss: 0.007035\n",
      "Epoch 47, loss: 0.007033\n",
      "Epoch 48, loss: 0.007038\n",
      "Epoch 49, loss: 0.007036\n",
      "Epoch 50, loss: 0.007032\n",
      "Epoch 51, loss: 0.007031\n",
      "Epoch 52, loss: 0.007030\n",
      "Epoch 53, loss: 0.007031\n",
      "Epoch 54, loss: 0.007031\n",
      "Epoch 55, loss: 0.007030\n",
      "Epoch 56, loss: 0.007029\n",
      "Epoch 57, loss: 0.007029\n",
      "Epoch 58, loss: 0.007027\n",
      "Epoch 59, loss: 0.007027\n",
      "Epoch 60, loss: 0.007025\n",
      "Epoch 61, loss: 0.007024\n",
      "Epoch 62, loss: 0.007023\n",
      "Epoch 63, loss: 0.007023\n",
      "Epoch 64, loss: 0.007024\n",
      "Epoch 65, loss: 0.007019\n",
      "Epoch 66, loss: 0.007020\n",
      "Epoch 67, loss: 0.007024\n",
      "Epoch 68, loss: 0.007017\n",
      "Epoch 69, loss: 0.007018\n",
      "Epoch 70, loss: 0.007019\n",
      "Epoch 71, loss: 0.007019\n",
      "Epoch 72, loss: 0.007016\n",
      "Epoch 73, loss: 0.007014\n",
      "Epoch 74, loss: 0.007015\n",
      "Epoch 75, loss: 0.007014\n",
      "Epoch 76, loss: 0.007018\n",
      "Epoch 77, loss: 0.007014\n",
      "Epoch 78, loss: 0.007015\n",
      "Epoch 79, loss: 0.007010\n",
      "Epoch 80, loss: 0.007015\n",
      "Epoch 81, loss: 0.007010\n",
      "Epoch 82, loss: 0.007013\n",
      "Epoch 83, loss: 0.007008\n",
      "Epoch 84, loss: 0.007011\n",
      "Epoch 85, loss: 0.007007\n",
      "Epoch 86, loss: 0.007012\n",
      "Epoch 87, loss: 0.007010\n",
      "Epoch 88, loss: 0.007005\n",
      "Epoch 89, loss: 0.007006\n",
      "Epoch 90, loss: 0.007007\n",
      "Epoch 91, loss: 0.007005\n",
      "Epoch 92, loss: 0.007005\n",
      "Epoch 93, loss: 0.007004\n",
      "Epoch 94, loss: 0.007002\n",
      "Epoch 95, loss: 0.007002\n",
      "Epoch 96, loss: 0.007004\n",
      "Epoch 97, loss: 0.007003\n",
      "Epoch 98, loss: 0.007000\n",
      "Epoch 99, loss: 0.007001\n",
      "accuracy = 0.11333333333333333\n",
      "learning_rate = 1e-05, reg_strength = 0.01\n",
      "Epoch 0, loss: 0.000715\n",
      "Epoch 1, loss: 0.000715\n",
      "Epoch 2, loss: 0.000716\n",
      "Epoch 3, loss: 0.000715\n",
      "Epoch 4, loss: 0.000715\n",
      "Epoch 5, loss: 0.000716\n",
      "Epoch 6, loss: 0.000715\n",
      "Epoch 7, loss: 0.000715\n",
      "Epoch 8, loss: 0.000715\n",
      "Epoch 9, loss: 0.000715\n",
      "Epoch 10, loss: 0.000715\n",
      "Epoch 11, loss: 0.000715\n",
      "Epoch 12, loss: 0.000715\n",
      "Epoch 13, loss: 0.000715\n",
      "Epoch 14, loss: 0.000715\n",
      "Epoch 15, loss: 0.000715\n",
      "Epoch 16, loss: 0.000715\n",
      "Epoch 17, loss: 0.000715\n",
      "Epoch 18, loss: 0.000715\n",
      "Epoch 19, loss: 0.000715\n",
      "Epoch 20, loss: 0.000715\n",
      "Epoch 21, loss: 0.000715\n",
      "Epoch 22, loss: 0.000715\n",
      "Epoch 23, loss: 0.000715\n",
      "Epoch 24, loss: 0.000715\n",
      "Epoch 25, loss: 0.000715\n",
      "Epoch 26, loss: 0.000715\n",
      "Epoch 27, loss: 0.000714\n",
      "Epoch 28, loss: 0.000715\n",
      "Epoch 29, loss: 0.000715\n",
      "Epoch 30, loss: 0.000715\n",
      "Epoch 31, loss: 0.000715\n",
      "Epoch 32, loss: 0.000715\n",
      "Epoch 33, loss: 0.000715\n",
      "Epoch 34, loss: 0.000715\n",
      "Epoch 35, loss: 0.000715\n",
      "Epoch 36, loss: 0.000715\n",
      "Epoch 37, loss: 0.000715\n",
      "Epoch 38, loss: 0.000715\n",
      "Epoch 39, loss: 0.000715\n",
      "Epoch 40, loss: 0.000715\n",
      "Epoch 41, loss: 0.000715\n",
      "Epoch 42, loss: 0.000715\n",
      "Epoch 43, loss: 0.000715\n",
      "Epoch 44, loss: 0.000715\n",
      "Epoch 45, loss: 0.000715\n",
      "Epoch 46, loss: 0.000715\n",
      "Epoch 47, loss: 0.000715\n",
      "Epoch 48, loss: 0.000715\n",
      "Epoch 49, loss: 0.000715\n",
      "Epoch 50, loss: 0.000715\n",
      "Epoch 51, loss: 0.000715\n",
      "Epoch 52, loss: 0.000715\n",
      "Epoch 53, loss: 0.000715\n",
      "Epoch 54, loss: 0.000715\n",
      "Epoch 55, loss: 0.000715\n",
      "Epoch 56, loss: 0.000715\n",
      "Epoch 57, loss: 0.000715\n",
      "Epoch 58, loss: 0.000715\n",
      "Epoch 59, loss: 0.000715\n",
      "Epoch 60, loss: 0.000715\n",
      "Epoch 61, loss: 0.000714\n",
      "Epoch 62, loss: 0.000714\n",
      "Epoch 63, loss: 0.000714\n",
      "Epoch 64, loss: 0.000715\n",
      "Epoch 65, loss: 0.000715\n",
      "Epoch 66, loss: 0.000715\n",
      "Epoch 67, loss: 0.000714\n",
      "Epoch 68, loss: 0.000714\n",
      "Epoch 69, loss: 0.000715\n",
      "Epoch 70, loss: 0.000715\n",
      "Epoch 71, loss: 0.000715\n",
      "Epoch 72, loss: 0.000715\n",
      "Epoch 73, loss: 0.000715\n",
      "Epoch 74, loss: 0.000715\n",
      "Epoch 75, loss: 0.000715\n",
      "Epoch 76, loss: 0.000715\n",
      "Epoch 77, loss: 0.000715\n",
      "Epoch 78, loss: 0.000714\n",
      "Epoch 79, loss: 0.000715\n",
      "Epoch 80, loss: 0.000715\n",
      "Epoch 81, loss: 0.000715\n",
      "Epoch 82, loss: 0.000715\n",
      "Epoch 83, loss: 0.000715\n",
      "Epoch 84, loss: 0.000715\n",
      "Epoch 85, loss: 0.000715\n",
      "Epoch 86, loss: 0.000714\n",
      "Epoch 87, loss: 0.000715\n",
      "Epoch 88, loss: 0.000715\n",
      "Epoch 89, loss: 0.000715\n",
      "Epoch 90, loss: 0.000715\n",
      "Epoch 91, loss: 0.000714\n",
      "Epoch 92, loss: 0.000715\n",
      "Epoch 93, loss: 0.000714\n",
      "Epoch 94, loss: 0.000715\n",
      "Epoch 95, loss: 0.000715\n",
      "Epoch 96, loss: 0.000715\n",
      "Epoch 97, loss: 0.000714\n",
      "Epoch 98, loss: 0.000715\n",
      "Epoch 99, loss: 0.000714\n",
      "accuracy = 0.13444444444444445\n",
      "learning_rate = 1e-05, reg_strength = 0.001\n",
      "Epoch 0, loss: 0.000070\n",
      "Epoch 1, loss: 0.000070\n",
      "Epoch 2, loss: 0.000070\n",
      "Epoch 3, loss: 0.000070\n",
      "Epoch 4, loss: 0.000070\n",
      "Epoch 5, loss: 0.000070\n",
      "Epoch 6, loss: 0.000070\n",
      "Epoch 7, loss: 0.000070\n",
      "Epoch 8, loss: 0.000070\n",
      "Epoch 9, loss: 0.000070\n",
      "Epoch 10, loss: 0.000070\n",
      "Epoch 11, loss: 0.000070\n",
      "Epoch 12, loss: 0.000070\n",
      "Epoch 13, loss: 0.000070\n",
      "Epoch 14, loss: 0.000070\n",
      "Epoch 15, loss: 0.000070\n",
      "Epoch 16, loss: 0.000070\n",
      "Epoch 17, loss: 0.000070\n",
      "Epoch 18, loss: 0.000070\n",
      "Epoch 19, loss: 0.000070\n",
      "Epoch 20, loss: 0.000070\n",
      "Epoch 21, loss: 0.000070\n",
      "Epoch 22, loss: 0.000070\n",
      "Epoch 23, loss: 0.000070\n",
      "Epoch 24, loss: 0.000070\n",
      "Epoch 25, loss: 0.000070\n",
      "Epoch 26, loss: 0.000070\n",
      "Epoch 27, loss: 0.000070\n",
      "Epoch 28, loss: 0.000070\n",
      "Epoch 29, loss: 0.000070\n",
      "Epoch 30, loss: 0.000070\n",
      "Epoch 31, loss: 0.000070\n",
      "Epoch 32, loss: 0.000070\n",
      "Epoch 33, loss: 0.000070\n",
      "Epoch 34, loss: 0.000070\n",
      "Epoch 35, loss: 0.000070\n",
      "Epoch 36, loss: 0.000070\n",
      "Epoch 37, loss: 0.000070\n",
      "Epoch 38, loss: 0.000070\n",
      "Epoch 39, loss: 0.000070\n",
      "Epoch 40, loss: 0.000070\n",
      "Epoch 41, loss: 0.000070\n",
      "Epoch 42, loss: 0.000070\n",
      "Epoch 43, loss: 0.000070\n",
      "Epoch 44, loss: 0.000070\n",
      "Epoch 45, loss: 0.000070\n",
      "Epoch 46, loss: 0.000070\n",
      "Epoch 47, loss: 0.000070\n",
      "Epoch 48, loss: 0.000070\n",
      "Epoch 49, loss: 0.000070\n",
      "Epoch 50, loss: 0.000070\n",
      "Epoch 51, loss: 0.000070\n",
      "Epoch 52, loss: 0.000070\n",
      "Epoch 53, loss: 0.000070\n",
      "Epoch 54, loss: 0.000070\n",
      "Epoch 55, loss: 0.000070\n",
      "Epoch 56, loss: 0.000070\n",
      "Epoch 57, loss: 0.000070\n",
      "Epoch 58, loss: 0.000070\n",
      "Epoch 59, loss: 0.000070\n",
      "Epoch 60, loss: 0.000070\n",
      "Epoch 61, loss: 0.000070\n",
      "Epoch 62, loss: 0.000070\n",
      "Epoch 63, loss: 0.000070\n",
      "Epoch 64, loss: 0.000070\n",
      "Epoch 65, loss: 0.000070\n",
      "Epoch 66, loss: 0.000070\n",
      "Epoch 67, loss: 0.000070\n",
      "Epoch 68, loss: 0.000070\n",
      "Epoch 69, loss: 0.000070\n",
      "Epoch 70, loss: 0.000070\n",
      "Epoch 71, loss: 0.000070\n",
      "Epoch 72, loss: 0.000070\n",
      "Epoch 73, loss: 0.000070\n",
      "Epoch 74, loss: 0.000070\n",
      "Epoch 75, loss: 0.000070\n",
      "Epoch 76, loss: 0.000070\n",
      "Epoch 77, loss: 0.000070\n",
      "Epoch 78, loss: 0.000070\n",
      "Epoch 79, loss: 0.000070\n",
      "Epoch 80, loss: 0.000070\n",
      "Epoch 81, loss: 0.000070\n",
      "Epoch 82, loss: 0.000070\n",
      "Epoch 83, loss: 0.000070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84, loss: 0.000070\n",
      "Epoch 85, loss: 0.000070\n",
      "Epoch 86, loss: 0.000070\n",
      "Epoch 87, loss: 0.000070\n",
      "Epoch 88, loss: 0.000070\n",
      "Epoch 89, loss: 0.000070\n",
      "Epoch 90, loss: 0.000070\n",
      "Epoch 91, loss: 0.000070\n",
      "Epoch 92, loss: 0.000070\n",
      "Epoch 93, loss: 0.000070\n",
      "Epoch 94, loss: 0.000070\n",
      "Epoch 95, loss: 0.000070\n",
      "Epoch 96, loss: 0.000070\n",
      "Epoch 97, loss: 0.000070\n",
      "Epoch 98, loss: 0.000070\n",
      "Epoch 99, loss: 0.000070\n",
      "accuracy = 0.09666666666666666\n",
      "learning_rate = 1e-05, reg_strength = 0.0001\n",
      "Epoch 0, loss: 0.000007\n",
      "Epoch 1, loss: 0.000007\n",
      "Epoch 2, loss: 0.000007\n",
      "Epoch 3, loss: 0.000007\n",
      "Epoch 4, loss: 0.000007\n",
      "Epoch 5, loss: 0.000007\n",
      "Epoch 6, loss: 0.000007\n",
      "Epoch 7, loss: 0.000007\n",
      "Epoch 8, loss: 0.000007\n",
      "Epoch 9, loss: 0.000007\n",
      "Epoch 10, loss: 0.000007\n",
      "Epoch 11, loss: 0.000007\n",
      "Epoch 12, loss: 0.000007\n",
      "Epoch 13, loss: 0.000007\n",
      "Epoch 14, loss: 0.000007\n",
      "Epoch 15, loss: 0.000007\n",
      "Epoch 16, loss: 0.000007\n",
      "Epoch 17, loss: 0.000007\n",
      "Epoch 18, loss: 0.000007\n",
      "Epoch 19, loss: 0.000007\n",
      "Epoch 20, loss: 0.000007\n",
      "Epoch 21, loss: 0.000007\n",
      "Epoch 22, loss: 0.000007\n",
      "Epoch 23, loss: 0.000007\n",
      "Epoch 24, loss: 0.000007\n",
      "Epoch 25, loss: 0.000007\n",
      "Epoch 26, loss: 0.000007\n",
      "Epoch 27, loss: 0.000007\n",
      "Epoch 28, loss: 0.000007\n",
      "Epoch 29, loss: 0.000007\n",
      "Epoch 30, loss: 0.000007\n",
      "Epoch 31, loss: 0.000007\n",
      "Epoch 32, loss: 0.000007\n",
      "Epoch 33, loss: 0.000007\n",
      "Epoch 34, loss: 0.000007\n",
      "Epoch 35, loss: 0.000007\n",
      "Epoch 36, loss: 0.000007\n",
      "Epoch 37, loss: 0.000007\n",
      "Epoch 38, loss: 0.000007\n",
      "Epoch 39, loss: 0.000007\n",
      "Epoch 40, loss: 0.000007\n",
      "Epoch 41, loss: 0.000007\n",
      "Epoch 42, loss: 0.000007\n",
      "Epoch 43, loss: 0.000007\n",
      "Epoch 44, loss: 0.000007\n",
      "Epoch 45, loss: 0.000007\n",
      "Epoch 46, loss: 0.000007\n",
      "Epoch 47, loss: 0.000007\n",
      "Epoch 48, loss: 0.000007\n",
      "Epoch 49, loss: 0.000007\n",
      "Epoch 50, loss: 0.000007\n",
      "Epoch 51, loss: 0.000007\n",
      "Epoch 52, loss: 0.000007\n",
      "Epoch 53, loss: 0.000007\n",
      "Epoch 54, loss: 0.000007\n",
      "Epoch 55, loss: 0.000007\n",
      "Epoch 56, loss: 0.000007\n",
      "Epoch 57, loss: 0.000007\n",
      "Epoch 58, loss: 0.000007\n",
      "Epoch 59, loss: 0.000007\n",
      "Epoch 60, loss: 0.000007\n",
      "Epoch 61, loss: 0.000007\n",
      "Epoch 62, loss: 0.000007\n",
      "Epoch 63, loss: 0.000007\n",
      "Epoch 64, loss: 0.000007\n",
      "Epoch 65, loss: 0.000007\n",
      "Epoch 66, loss: 0.000007\n",
      "Epoch 67, loss: 0.000007\n",
      "Epoch 68, loss: 0.000007\n",
      "Epoch 69, loss: 0.000007\n",
      "Epoch 70, loss: 0.000007\n",
      "Epoch 71, loss: 0.000007\n",
      "Epoch 72, loss: 0.000007\n",
      "Epoch 73, loss: 0.000007\n",
      "Epoch 74, loss: 0.000007\n",
      "Epoch 75, loss: 0.000007\n",
      "Epoch 76, loss: 0.000007\n",
      "Epoch 77, loss: 0.000007\n",
      "Epoch 78, loss: 0.000007\n",
      "Epoch 79, loss: 0.000007\n",
      "Epoch 80, loss: 0.000007\n",
      "Epoch 81, loss: 0.000007\n",
      "Epoch 82, loss: 0.000007\n",
      "Epoch 83, loss: 0.000007\n",
      "Epoch 84, loss: 0.000007\n",
      "Epoch 85, loss: 0.000007\n",
      "Epoch 86, loss: 0.000007\n",
      "Epoch 87, loss: 0.000007\n",
      "Epoch 88, loss: 0.000007\n",
      "Epoch 89, loss: 0.000007\n",
      "Epoch 90, loss: 0.000007\n",
      "Epoch 91, loss: 0.000007\n",
      "Epoch 92, loss: 0.000007\n",
      "Epoch 93, loss: 0.000007\n",
      "Epoch 94, loss: 0.000007\n",
      "Epoch 95, loss: 0.000007\n",
      "Epoch 96, loss: 0.000007\n",
      "Epoch 97, loss: 0.000007\n",
      "Epoch 98, loss: 0.000007\n",
      "Epoch 99, loss: 0.000007\n",
      "accuracy = 0.12111111111111111\n",
      "learning_rate = 1e-05, reg_strength = 1e-05\n",
      "Epoch 0, loss: 0.000001\n",
      "Epoch 1, loss: 0.000001\n",
      "Epoch 2, loss: 0.000001\n",
      "Epoch 3, loss: 0.000001\n",
      "Epoch 4, loss: 0.000001\n",
      "Epoch 5, loss: 0.000001\n",
      "Epoch 6, loss: 0.000001\n",
      "Epoch 7, loss: 0.000001\n",
      "Epoch 8, loss: 0.000001\n",
      "Epoch 9, loss: 0.000001\n",
      "Epoch 10, loss: 0.000001\n",
      "Epoch 11, loss: 0.000001\n",
      "Epoch 12, loss: 0.000001\n",
      "Epoch 13, loss: 0.000001\n",
      "Epoch 14, loss: 0.000001\n",
      "Epoch 15, loss: 0.000001\n",
      "Epoch 16, loss: 0.000001\n",
      "Epoch 17, loss: 0.000001\n",
      "Epoch 18, loss: 0.000001\n",
      "Epoch 19, loss: 0.000001\n",
      "Epoch 20, loss: 0.000001\n",
      "Epoch 21, loss: 0.000001\n",
      "Epoch 22, loss: 0.000001\n",
      "Epoch 23, loss: 0.000001\n",
      "Epoch 24, loss: 0.000001\n",
      "Epoch 25, loss: 0.000001\n",
      "Epoch 26, loss: 0.000001\n",
      "Epoch 27, loss: 0.000001\n",
      "Epoch 28, loss: 0.000001\n",
      "Epoch 29, loss: 0.000001\n",
      "Epoch 30, loss: 0.000001\n",
      "Epoch 31, loss: 0.000001\n",
      "Epoch 32, loss: 0.000001\n",
      "Epoch 33, loss: 0.000001\n",
      "Epoch 34, loss: 0.000001\n",
      "Epoch 35, loss: 0.000001\n",
      "Epoch 36, loss: 0.000001\n",
      "Epoch 37, loss: 0.000001\n",
      "Epoch 38, loss: 0.000001\n",
      "Epoch 39, loss: 0.000001\n",
      "Epoch 40, loss: 0.000001\n",
      "Epoch 41, loss: 0.000001\n",
      "Epoch 42, loss: 0.000001\n",
      "Epoch 43, loss: 0.000001\n",
      "Epoch 44, loss: 0.000001\n",
      "Epoch 45, loss: 0.000001\n",
      "Epoch 46, loss: 0.000001\n",
      "Epoch 47, loss: 0.000001\n",
      "Epoch 48, loss: 0.000001\n",
      "Epoch 49, loss: 0.000001\n",
      "Epoch 50, loss: 0.000001\n",
      "Epoch 51, loss: 0.000001\n",
      "Epoch 52, loss: 0.000001\n",
      "Epoch 53, loss: 0.000001\n",
      "Epoch 54, loss: 0.000001\n",
      "Epoch 55, loss: 0.000001\n",
      "Epoch 56, loss: 0.000001\n",
      "Epoch 57, loss: 0.000001\n",
      "Epoch 58, loss: 0.000001\n",
      "Epoch 59, loss: 0.000001\n",
      "Epoch 60, loss: 0.000001\n",
      "Epoch 61, loss: 0.000001\n",
      "Epoch 62, loss: 0.000001\n",
      "Epoch 63, loss: 0.000001\n",
      "Epoch 64, loss: 0.000001\n",
      "Epoch 65, loss: 0.000001\n",
      "Epoch 66, loss: 0.000001\n",
      "Epoch 67, loss: 0.000001\n",
      "Epoch 68, loss: 0.000001\n",
      "Epoch 69, loss: 0.000001\n",
      "Epoch 70, loss: 0.000001\n",
      "Epoch 71, loss: 0.000001\n",
      "Epoch 72, loss: 0.000001\n",
      "Epoch 73, loss: 0.000001\n",
      "Epoch 74, loss: 0.000001\n",
      "Epoch 75, loss: 0.000001\n",
      "Epoch 76, loss: 0.000001\n",
      "Epoch 77, loss: 0.000001\n",
      "Epoch 78, loss: 0.000001\n",
      "Epoch 79, loss: 0.000001\n",
      "Epoch 80, loss: 0.000001\n",
      "Epoch 81, loss: 0.000001\n",
      "Epoch 82, loss: 0.000001\n",
      "Epoch 83, loss: 0.000001\n",
      "Epoch 84, loss: 0.000001\n",
      "Epoch 85, loss: 0.000001\n",
      "Epoch 86, loss: 0.000001\n",
      "Epoch 87, loss: 0.000001\n",
      "Epoch 88, loss: 0.000001\n",
      "Epoch 89, loss: 0.000001\n",
      "Epoch 90, loss: 0.000001\n",
      "Epoch 91, loss: 0.000001\n",
      "Epoch 92, loss: 0.000001\n",
      "Epoch 93, loss: 0.000001\n",
      "Epoch 94, loss: 0.000001\n",
      "Epoch 95, loss: 0.000001\n",
      "Epoch 96, loss: 0.000001\n",
      "Epoch 97, loss: 0.000001\n",
      "Epoch 98, loss: 0.000001\n",
      "Epoch 99, loss: 0.000001\n",
      "accuracy = 0.14777777777777779\n",
      "learning_rate = 1e-05, reg_strength = 1e-06\n",
      "Epoch 0, loss: 0.000000\n",
      "Epoch 1, loss: 0.000000\n",
      "Epoch 2, loss: 0.000000\n",
      "Epoch 3, loss: 0.000000\n",
      "Epoch 4, loss: 0.000000\n",
      "Epoch 5, loss: 0.000000\n",
      "Epoch 6, loss: 0.000000\n",
      "Epoch 7, loss: 0.000000\n",
      "Epoch 8, loss: 0.000000\n",
      "Epoch 9, loss: 0.000000\n",
      "Epoch 10, loss: 0.000000\n",
      "Epoch 11, loss: 0.000000\n",
      "Epoch 12, loss: 0.000000\n",
      "Epoch 13, loss: 0.000000\n",
      "Epoch 14, loss: 0.000000\n",
      "Epoch 15, loss: 0.000000\n",
      "Epoch 16, loss: 0.000000\n",
      "Epoch 17, loss: 0.000000\n",
      "Epoch 18, loss: 0.000000\n",
      "Epoch 19, loss: 0.000000\n",
      "Epoch 20, loss: 0.000000\n",
      "Epoch 21, loss: 0.000000\n",
      "Epoch 22, loss: 0.000000\n",
      "Epoch 23, loss: 0.000000\n",
      "Epoch 24, loss: 0.000000\n",
      "Epoch 25, loss: 0.000000\n",
      "Epoch 26, loss: 0.000000\n",
      "Epoch 27, loss: 0.000000\n",
      "Epoch 28, loss: 0.000000\n",
      "Epoch 29, loss: 0.000000\n",
      "Epoch 30, loss: 0.000000\n",
      "Epoch 31, loss: 0.000000\n",
      "Epoch 32, loss: 0.000000\n",
      "Epoch 33, loss: 0.000000\n",
      "Epoch 34, loss: 0.000000\n",
      "Epoch 35, loss: 0.000000\n",
      "Epoch 36, loss: 0.000000\n",
      "Epoch 37, loss: 0.000000\n",
      "Epoch 38, loss: 0.000000\n",
      "Epoch 39, loss: 0.000000\n",
      "Epoch 40, loss: 0.000000\n",
      "Epoch 41, loss: 0.000000\n",
      "Epoch 42, loss: 0.000000\n",
      "Epoch 43, loss: 0.000000\n",
      "Epoch 44, loss: 0.000000\n",
      "Epoch 45, loss: 0.000000\n",
      "Epoch 46, loss: 0.000000\n",
      "Epoch 47, loss: 0.000000\n",
      "Epoch 48, loss: 0.000000\n",
      "Epoch 49, loss: 0.000000\n",
      "Epoch 50, loss: 0.000000\n",
      "Epoch 51, loss: 0.000000\n",
      "Epoch 52, loss: 0.000000\n",
      "Epoch 53, loss: 0.000000\n",
      "Epoch 54, loss: 0.000000\n",
      "Epoch 55, loss: 0.000000\n",
      "Epoch 56, loss: 0.000000\n",
      "Epoch 57, loss: 0.000000\n",
      "Epoch 58, loss: 0.000000\n",
      "Epoch 59, loss: 0.000000\n",
      "Epoch 60, loss: 0.000000\n",
      "Epoch 61, loss: 0.000000\n",
      "Epoch 62, loss: 0.000000\n",
      "Epoch 63, loss: 0.000000\n",
      "Epoch 64, loss: 0.000000\n",
      "Epoch 65, loss: 0.000000\n",
      "Epoch 66, loss: 0.000000\n",
      "Epoch 67, loss: 0.000000\n",
      "Epoch 68, loss: 0.000000\n",
      "Epoch 69, loss: 0.000000\n",
      "Epoch 70, loss: 0.000000\n",
      "Epoch 71, loss: 0.000000\n",
      "Epoch 72, loss: 0.000000\n",
      "Epoch 73, loss: 0.000000\n",
      "Epoch 74, loss: 0.000000\n",
      "Epoch 75, loss: 0.000000\n",
      "Epoch 76, loss: 0.000000\n",
      "Epoch 77, loss: 0.000000\n",
      "Epoch 78, loss: 0.000000\n",
      "Epoch 79, loss: 0.000000\n",
      "Epoch 80, loss: 0.000000\n",
      "Epoch 81, loss: 0.000000\n",
      "Epoch 82, loss: 0.000000\n",
      "Epoch 83, loss: 0.000000\n",
      "Epoch 84, loss: 0.000000\n",
      "Epoch 85, loss: 0.000000\n",
      "Epoch 86, loss: 0.000000\n",
      "Epoch 87, loss: 0.000000\n",
      "Epoch 88, loss: 0.000000\n",
      "Epoch 89, loss: 0.000000\n",
      "Epoch 90, loss: 0.000000\n",
      "Epoch 91, loss: 0.000000\n",
      "Epoch 92, loss: 0.000000\n",
      "Epoch 93, loss: 0.000000\n",
      "Epoch 94, loss: 0.000000\n",
      "Epoch 95, loss: 0.000000\n",
      "Epoch 96, loss: 0.000000\n",
      "Epoch 97, loss: 0.000000\n",
      "Epoch 98, loss: 0.000000\n",
      "Epoch 99, loss: 0.000000\n",
      "accuracy = 0.11222222222222222\n",
      "learning_rate = 1e-06, reg_strength = 10.0\n",
      "Epoch 0, loss: 0.701926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.701233\n",
      "Epoch 2, loss: 0.700349\n",
      "Epoch 3, loss: 0.699661\n",
      "Epoch 4, loss: 0.699165\n",
      "Epoch 5, loss: 0.698957\n",
      "Epoch 6, loss: 0.697736\n",
      "Epoch 7, loss: 0.696811\n",
      "Epoch 8, loss: 0.696191\n",
      "Epoch 9, loss: 0.695979\n",
      "Epoch 10, loss: 0.695117\n",
      "Epoch 11, loss: 0.694506\n",
      "Epoch 12, loss: 0.693600\n",
      "Epoch 13, loss: 0.692790\n",
      "Epoch 14, loss: 0.692525\n",
      "Epoch 15, loss: 0.691961\n",
      "Epoch 16, loss: 0.691098\n",
      "Epoch 17, loss: 0.690686\n",
      "Epoch 18, loss: 0.689984\n",
      "Epoch 19, loss: 0.689007\n",
      "Epoch 20, loss: 0.688674\n",
      "Epoch 21, loss: 0.687470\n",
      "Epoch 22, loss: 0.687307\n",
      "Epoch 23, loss: 0.686478\n",
      "Epoch 24, loss: 0.685853\n",
      "Epoch 25, loss: 0.685262\n",
      "Epoch 26, loss: 0.684612\n",
      "Epoch 27, loss: 0.683868\n",
      "Epoch 28, loss: 0.683428\n",
      "Epoch 29, loss: 0.682532\n",
      "Epoch 30, loss: 0.682226\n",
      "Epoch 31, loss: 0.681134\n",
      "Epoch 32, loss: 0.680790\n",
      "Epoch 33, loss: 0.679914\n",
      "Epoch 34, loss: 0.679417\n",
      "Epoch 35, loss: 0.678724\n",
      "Epoch 36, loss: 0.677982\n",
      "Epoch 37, loss: 0.677140\n",
      "Epoch 38, loss: 0.676510\n",
      "Epoch 39, loss: 0.675884\n",
      "Epoch 40, loss: 0.675352\n",
      "Epoch 41, loss: 0.674433\n",
      "Epoch 42, loss: 0.674123\n",
      "Epoch 43, loss: 0.673392\n",
      "Epoch 44, loss: 0.672684\n",
      "Epoch 45, loss: 0.672034\n",
      "Epoch 46, loss: 0.671502\n",
      "Epoch 47, loss: 0.670846\n",
      "Epoch 48, loss: 0.670357\n",
      "Epoch 49, loss: 0.669464\n",
      "Epoch 50, loss: 0.668926\n",
      "Epoch 51, loss: 0.668079\n",
      "Epoch 52, loss: 0.667916\n",
      "Epoch 53, loss: 0.666814\n",
      "Epoch 54, loss: 0.666623\n",
      "Epoch 55, loss: 0.665630\n",
      "Epoch 56, loss: 0.665242\n",
      "Epoch 57, loss: 0.664321\n",
      "Epoch 58, loss: 0.663681\n",
      "Epoch 59, loss: 0.663351\n",
      "Epoch 60, loss: 0.662981\n",
      "Epoch 61, loss: 0.662144\n",
      "Epoch 62, loss: 0.661254\n",
      "Epoch 63, loss: 0.660735\n",
      "Epoch 64, loss: 0.660080\n",
      "Epoch 65, loss: 0.659150\n",
      "Epoch 66, loss: 0.658637\n",
      "Epoch 67, loss: 0.658115\n",
      "Epoch 68, loss: 0.657284\n",
      "Epoch 69, loss: 0.656702\n",
      "Epoch 70, loss: 0.656217\n",
      "Epoch 71, loss: 0.655468\n",
      "Epoch 72, loss: 0.655014\n",
      "Epoch 73, loss: 0.654541\n",
      "Epoch 74, loss: 0.653703\n",
      "Epoch 75, loss: 0.653175\n",
      "Epoch 76, loss: 0.652526\n",
      "Epoch 77, loss: 0.651853\n",
      "Epoch 78, loss: 0.651025\n",
      "Epoch 79, loss: 0.650575\n",
      "Epoch 80, loss: 0.649894\n",
      "Epoch 81, loss: 0.649177\n",
      "Epoch 82, loss: 0.648428\n",
      "Epoch 83, loss: 0.647977\n",
      "Epoch 84, loss: 0.647448\n",
      "Epoch 85, loss: 0.646820\n",
      "Epoch 86, loss: 0.645882\n",
      "Epoch 87, loss: 0.645362\n",
      "Epoch 88, loss: 0.645079\n",
      "Epoch 89, loss: 0.644342\n",
      "Epoch 90, loss: 0.643730\n",
      "Epoch 91, loss: 0.643420\n",
      "Epoch 92, loss: 0.642399\n",
      "Epoch 93, loss: 0.641766\n",
      "Epoch 94, loss: 0.641072\n",
      "Epoch 95, loss: 0.640737\n",
      "Epoch 96, loss: 0.640016\n",
      "Epoch 97, loss: 0.639388\n",
      "Epoch 98, loss: 0.638514\n",
      "Epoch 99, loss: 0.638323\n",
      "accuracy = 0.07944444444444444\n",
      "learning_rate = 1e-06, reg_strength = 1\n",
      "Epoch 0, loss: 0.071156\n",
      "Epoch 1, loss: 0.071126\n",
      "Epoch 2, loss: 0.071134\n",
      "Epoch 3, loss: 0.071122\n",
      "Epoch 4, loss: 0.071082\n",
      "Epoch 5, loss: 0.071097\n",
      "Epoch 6, loss: 0.071106\n",
      "Epoch 7, loss: 0.071074\n",
      "Epoch 8, loss: 0.071058\n",
      "Epoch 9, loss: 0.071036\n",
      "Epoch 10, loss: 0.071050\n",
      "Epoch 11, loss: 0.071077\n",
      "Epoch 12, loss: 0.071047\n",
      "Epoch 13, loss: 0.071026\n",
      "Epoch 14, loss: 0.071023\n",
      "Epoch 15, loss: 0.071094\n",
      "Epoch 16, loss: 0.071019\n",
      "Epoch 17, loss: 0.070988\n",
      "Epoch 18, loss: 0.071040\n",
      "Epoch 19, loss: 0.070986\n",
      "Epoch 20, loss: 0.071005\n",
      "Epoch 21, loss: 0.070958\n",
      "Epoch 22, loss: 0.070979\n",
      "Epoch 23, loss: 0.070967\n",
      "Epoch 24, loss: 0.070970\n",
      "Epoch 25, loss: 0.070978\n",
      "Epoch 26, loss: 0.070918\n",
      "Epoch 27, loss: 0.070916\n",
      "Epoch 28, loss: 0.070956\n",
      "Epoch 29, loss: 0.070932\n",
      "Epoch 30, loss: 0.070924\n",
      "Epoch 31, loss: 0.070930\n",
      "Epoch 32, loss: 0.070960\n",
      "Epoch 33, loss: 0.070889\n",
      "Epoch 34, loss: 0.070940\n",
      "Epoch 35, loss: 0.070931\n",
      "Epoch 36, loss: 0.070874\n",
      "Epoch 37, loss: 0.070860\n",
      "Epoch 38, loss: 0.070883\n",
      "Epoch 39, loss: 0.070878\n",
      "Epoch 40, loss: 0.070858\n",
      "Epoch 41, loss: 0.070865\n",
      "Epoch 42, loss: 0.070883\n",
      "Epoch 43, loss: 0.070849\n",
      "Epoch 44, loss: 0.070810\n",
      "Epoch 45, loss: 0.070810\n",
      "Epoch 46, loss: 0.070831\n",
      "Epoch 47, loss: 0.070799\n",
      "Epoch 48, loss: 0.070781\n",
      "Epoch 49, loss: 0.070793\n",
      "Epoch 50, loss: 0.070780\n",
      "Epoch 51, loss: 0.070783\n",
      "Epoch 52, loss: 0.070790\n",
      "Epoch 53, loss: 0.070782\n",
      "Epoch 54, loss: 0.070731\n",
      "Epoch 55, loss: 0.070755\n",
      "Epoch 56, loss: 0.070764\n",
      "Epoch 57, loss: 0.070770\n",
      "Epoch 58, loss: 0.070716\n",
      "Epoch 59, loss: 0.070755\n",
      "Epoch 60, loss: 0.070735\n",
      "Epoch 61, loss: 0.070692\n",
      "Epoch 62, loss: 0.070691\n",
      "Epoch 63, loss: 0.070707\n",
      "Epoch 64, loss: 0.070679\n",
      "Epoch 65, loss: 0.070673\n",
      "Epoch 66, loss: 0.070646\n",
      "Epoch 67, loss: 0.070704\n",
      "Epoch 68, loss: 0.070703\n",
      "Epoch 69, loss: 0.070675\n",
      "Epoch 70, loss: 0.070684\n",
      "Epoch 71, loss: 0.070630\n",
      "Epoch 72, loss: 0.070663\n",
      "Epoch 73, loss: 0.070634\n",
      "Epoch 74, loss: 0.070641\n",
      "Epoch 75, loss: 0.070632\n",
      "Epoch 76, loss: 0.070635\n",
      "Epoch 77, loss: 0.070645\n",
      "Epoch 78, loss: 0.070580\n",
      "Epoch 79, loss: 0.070581\n",
      "Epoch 80, loss: 0.070581\n",
      "Epoch 81, loss: 0.070580\n",
      "Epoch 82, loss: 0.070582\n",
      "Epoch 83, loss: 0.070563\n",
      "Epoch 84, loss: 0.070556\n",
      "Epoch 85, loss: 0.070542\n",
      "Epoch 86, loss: 0.070548\n",
      "Epoch 87, loss: 0.070513\n",
      "Epoch 88, loss: 0.070515\n",
      "Epoch 89, loss: 0.070549\n",
      "Epoch 90, loss: 0.070498\n",
      "Epoch 91, loss: 0.070516\n",
      "Epoch 92, loss: 0.070502\n",
      "Epoch 93, loss: 0.070506\n",
      "Epoch 94, loss: 0.070510\n",
      "Epoch 95, loss: 0.070496\n",
      "Epoch 96, loss: 0.070491\n",
      "Epoch 97, loss: 0.070479\n",
      "Epoch 98, loss: 0.070424\n",
      "Epoch 99, loss: 0.070439\n",
      "accuracy = 0.09722222222222222\n",
      "learning_rate = 1e-06, reg_strength = 0.1\n",
      "Epoch 0, loss: 0.007171\n",
      "Epoch 1, loss: 0.007170\n",
      "Epoch 2, loss: 0.007166\n",
      "Epoch 3, loss: 0.007170\n",
      "Epoch 4, loss: 0.007172\n",
      "Epoch 5, loss: 0.007170\n",
      "Epoch 6, loss: 0.007171\n",
      "Epoch 7, loss: 0.007172\n",
      "Epoch 8, loss: 0.007173\n",
      "Epoch 9, loss: 0.007170\n",
      "Epoch 10, loss: 0.007167\n",
      "Epoch 11, loss: 0.007171\n",
      "Epoch 12, loss: 0.007170\n",
      "Epoch 13, loss: 0.007170\n",
      "Epoch 14, loss: 0.007168\n",
      "Epoch 15, loss: 0.007172\n",
      "Epoch 16, loss: 0.007170\n",
      "Epoch 17, loss: 0.007168\n",
      "Epoch 18, loss: 0.007167\n",
      "Epoch 19, loss: 0.007172\n",
      "Epoch 20, loss: 0.007172\n",
      "Epoch 21, loss: 0.007171\n",
      "Epoch 22, loss: 0.007171\n",
      "Epoch 23, loss: 0.007172\n",
      "Epoch 24, loss: 0.007168\n",
      "Epoch 25, loss: 0.007166\n",
      "Epoch 26, loss: 0.007171\n",
      "Epoch 27, loss: 0.007169\n",
      "Epoch 28, loss: 0.007169\n",
      "Epoch 29, loss: 0.007172\n",
      "Epoch 30, loss: 0.007169\n",
      "Epoch 31, loss: 0.007170\n",
      "Epoch 32, loss: 0.007169\n",
      "Epoch 33, loss: 0.007167\n",
      "Epoch 34, loss: 0.007169\n",
      "Epoch 35, loss: 0.007169\n",
      "Epoch 36, loss: 0.007170\n",
      "Epoch 37, loss: 0.007172\n",
      "Epoch 38, loss: 0.007166\n",
      "Epoch 39, loss: 0.007170\n",
      "Epoch 40, loss: 0.007168\n",
      "Epoch 41, loss: 0.007172\n",
      "Epoch 42, loss: 0.007169\n",
      "Epoch 43, loss: 0.007169\n",
      "Epoch 44, loss: 0.007170\n",
      "Epoch 45, loss: 0.007167\n",
      "Epoch 46, loss: 0.007166\n",
      "Epoch 47, loss: 0.007166\n",
      "Epoch 48, loss: 0.007167\n",
      "Epoch 49, loss: 0.007168\n",
      "Epoch 50, loss: 0.007164\n",
      "Epoch 51, loss: 0.007168\n",
      "Epoch 52, loss: 0.007171\n",
      "Epoch 53, loss: 0.007166\n",
      "Epoch 54, loss: 0.007169\n",
      "Epoch 55, loss: 0.007168\n",
      "Epoch 56, loss: 0.007168\n",
      "Epoch 57, loss: 0.007168\n",
      "Epoch 58, loss: 0.007167\n",
      "Epoch 59, loss: 0.007171\n",
      "Epoch 60, loss: 0.007168\n",
      "Epoch 61, loss: 0.007165\n",
      "Epoch 62, loss: 0.007163\n",
      "Epoch 63, loss: 0.007167\n",
      "Epoch 64, loss: 0.007169\n",
      "Epoch 65, loss: 0.007164\n",
      "Epoch 66, loss: 0.007168\n",
      "Epoch 67, loss: 0.007167\n",
      "Epoch 68, loss: 0.007169\n",
      "Epoch 69, loss: 0.007168\n",
      "Epoch 70, loss: 0.007163\n",
      "Epoch 71, loss: 0.007169\n",
      "Epoch 72, loss: 0.007165\n",
      "Epoch 73, loss: 0.007166\n",
      "Epoch 74, loss: 0.007167\n",
      "Epoch 75, loss: 0.007163\n",
      "Epoch 76, loss: 0.007167\n",
      "Epoch 77, loss: 0.007165\n",
      "Epoch 78, loss: 0.007167\n",
      "Epoch 79, loss: 0.007165\n",
      "Epoch 80, loss: 0.007168\n",
      "Epoch 81, loss: 0.007167\n",
      "Epoch 82, loss: 0.007166\n",
      "Epoch 83, loss: 0.007167\n",
      "Epoch 84, loss: 0.007165\n",
      "Epoch 85, loss: 0.007165\n",
      "Epoch 86, loss: 0.007167\n",
      "Epoch 87, loss: 0.007165\n",
      "Epoch 88, loss: 0.007162\n",
      "Epoch 89, loss: 0.007166\n",
      "Epoch 90, loss: 0.007163\n",
      "Epoch 91, loss: 0.007164\n",
      "Epoch 92, loss: 0.007164\n",
      "Epoch 93, loss: 0.007163\n",
      "Epoch 94, loss: 0.007167\n",
      "Epoch 95, loss: 0.007164\n",
      "Epoch 96, loss: 0.007169\n",
      "Epoch 97, loss: 0.007162\n",
      "Epoch 98, loss: 0.007164\n",
      "Epoch 99, loss: 0.007163\n",
      "accuracy = 0.11333333333333333\n",
      "learning_rate = 1e-06, reg_strength = 0.01\n",
      "Epoch 0, loss: 0.000705\n",
      "Epoch 1, loss: 0.000705\n",
      "Epoch 2, loss: 0.000705\n",
      "Epoch 3, loss: 0.000705\n",
      "Epoch 4, loss: 0.000705\n",
      "Epoch 5, loss: 0.000706\n",
      "Epoch 6, loss: 0.000705\n",
      "Epoch 7, loss: 0.000705\n",
      "Epoch 8, loss: 0.000705\n",
      "Epoch 9, loss: 0.000705\n",
      "Epoch 10, loss: 0.000705\n",
      "Epoch 11, loss: 0.000705\n",
      "Epoch 12, loss: 0.000705\n",
      "Epoch 13, loss: 0.000705\n",
      "Epoch 14, loss: 0.000706\n",
      "Epoch 15, loss: 0.000705\n",
      "Epoch 16, loss: 0.000705\n",
      "Epoch 17, loss: 0.000705\n",
      "Epoch 18, loss: 0.000705\n",
      "Epoch 19, loss: 0.000705\n",
      "Epoch 20, loss: 0.000705\n",
      "Epoch 21, loss: 0.000705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, loss: 0.000705\n",
      "Epoch 23, loss: 0.000705\n",
      "Epoch 24, loss: 0.000706\n",
      "Epoch 25, loss: 0.000705\n",
      "Epoch 26, loss: 0.000705\n",
      "Epoch 27, loss: 0.000705\n",
      "Epoch 28, loss: 0.000705\n",
      "Epoch 29, loss: 0.000705\n",
      "Epoch 30, loss: 0.000705\n",
      "Epoch 31, loss: 0.000705\n",
      "Epoch 32, loss: 0.000705\n",
      "Epoch 33, loss: 0.000705\n",
      "Epoch 34, loss: 0.000705\n",
      "Epoch 35, loss: 0.000705\n",
      "Epoch 36, loss: 0.000705\n",
      "Epoch 37, loss: 0.000705\n",
      "Epoch 38, loss: 0.000705\n",
      "Epoch 39, loss: 0.000705\n",
      "Epoch 40, loss: 0.000705\n",
      "Epoch 41, loss: 0.000705\n",
      "Epoch 42, loss: 0.000705\n",
      "Epoch 43, loss: 0.000705\n",
      "Epoch 44, loss: 0.000705\n",
      "Epoch 45, loss: 0.000705\n",
      "Epoch 46, loss: 0.000705\n",
      "Epoch 47, loss: 0.000705\n",
      "Epoch 48, loss: 0.000705\n",
      "Epoch 49, loss: 0.000705\n",
      "Epoch 50, loss: 0.000705\n",
      "Epoch 51, loss: 0.000705\n",
      "Epoch 52, loss: 0.000705\n",
      "Epoch 53, loss: 0.000705\n",
      "Epoch 54, loss: 0.000705\n",
      "Epoch 55, loss: 0.000705\n",
      "Epoch 56, loss: 0.000706\n",
      "Epoch 57, loss: 0.000705\n",
      "Epoch 58, loss: 0.000705\n",
      "Epoch 59, loss: 0.000705\n",
      "Epoch 60, loss: 0.000705\n",
      "Epoch 61, loss: 0.000705\n",
      "Epoch 62, loss: 0.000705\n",
      "Epoch 63, loss: 0.000705\n",
      "Epoch 64, loss: 0.000705\n",
      "Epoch 65, loss: 0.000705\n",
      "Epoch 66, loss: 0.000705\n",
      "Epoch 67, loss: 0.000705\n",
      "Epoch 68, loss: 0.000705\n",
      "Epoch 69, loss: 0.000705\n",
      "Epoch 70, loss: 0.000705\n",
      "Epoch 71, loss: 0.000705\n",
      "Epoch 72, loss: 0.000706\n",
      "Epoch 73, loss: 0.000705\n",
      "Epoch 74, loss: 0.000705\n",
      "Epoch 75, loss: 0.000705\n",
      "Epoch 76, loss: 0.000705\n",
      "Epoch 77, loss: 0.000705\n",
      "Epoch 78, loss: 0.000705\n",
      "Epoch 79, loss: 0.000705\n",
      "Epoch 80, loss: 0.000705\n",
      "Epoch 81, loss: 0.000705\n",
      "Epoch 82, loss: 0.000705\n",
      "Epoch 83, loss: 0.000705\n",
      "Epoch 84, loss: 0.000705\n",
      "Epoch 85, loss: 0.000705\n",
      "Epoch 86, loss: 0.000705\n",
      "Epoch 87, loss: 0.000705\n",
      "Epoch 88, loss: 0.000705\n",
      "Epoch 89, loss: 0.000705\n",
      "Epoch 90, loss: 0.000705\n",
      "Epoch 91, loss: 0.000705\n",
      "Epoch 92, loss: 0.000705\n",
      "Epoch 93, loss: 0.000705\n",
      "Epoch 94, loss: 0.000705\n",
      "Epoch 95, loss: 0.000705\n",
      "Epoch 96, loss: 0.000705\n",
      "Epoch 97, loss: 0.000705\n",
      "Epoch 98, loss: 0.000705\n",
      "Epoch 99, loss: 0.000705\n",
      "accuracy = 0.10444444444444445\n",
      "learning_rate = 1e-06, reg_strength = 0.001\n",
      "Epoch 0, loss: 0.000070\n",
      "Epoch 1, loss: 0.000070\n",
      "Epoch 2, loss: 0.000070\n",
      "Epoch 3, loss: 0.000070\n",
      "Epoch 4, loss: 0.000070\n",
      "Epoch 5, loss: 0.000070\n",
      "Epoch 6, loss: 0.000070\n",
      "Epoch 7, loss: 0.000070\n",
      "Epoch 8, loss: 0.000070\n",
      "Epoch 9, loss: 0.000070\n",
      "Epoch 10, loss: 0.000070\n",
      "Epoch 11, loss: 0.000070\n",
      "Epoch 12, loss: 0.000070\n",
      "Epoch 13, loss: 0.000070\n",
      "Epoch 14, loss: 0.000070\n",
      "Epoch 15, loss: 0.000070\n",
      "Epoch 16, loss: 0.000070\n",
      "Epoch 17, loss: 0.000070\n",
      "Epoch 18, loss: 0.000070\n",
      "Epoch 19, loss: 0.000070\n",
      "Epoch 20, loss: 0.000070\n",
      "Epoch 21, loss: 0.000070\n",
      "Epoch 22, loss: 0.000070\n",
      "Epoch 23, loss: 0.000070\n",
      "Epoch 24, loss: 0.000070\n",
      "Epoch 25, loss: 0.000070\n",
      "Epoch 26, loss: 0.000070\n",
      "Epoch 27, loss: 0.000070\n",
      "Epoch 28, loss: 0.000070\n",
      "Epoch 29, loss: 0.000070\n",
      "Epoch 30, loss: 0.000070\n",
      "Epoch 31, loss: 0.000070\n",
      "Epoch 32, loss: 0.000070\n",
      "Epoch 33, loss: 0.000070\n",
      "Epoch 34, loss: 0.000070\n",
      "Epoch 35, loss: 0.000070\n",
      "Epoch 36, loss: 0.000070\n",
      "Epoch 37, loss: 0.000070\n",
      "Epoch 38, loss: 0.000070\n",
      "Epoch 39, loss: 0.000070\n",
      "Epoch 40, loss: 0.000070\n",
      "Epoch 41, loss: 0.000070\n",
      "Epoch 42, loss: 0.000070\n",
      "Epoch 43, loss: 0.000070\n",
      "Epoch 44, loss: 0.000070\n",
      "Epoch 45, loss: 0.000070\n",
      "Epoch 46, loss: 0.000070\n",
      "Epoch 47, loss: 0.000070\n",
      "Epoch 48, loss: 0.000070\n",
      "Epoch 49, loss: 0.000070\n",
      "Epoch 50, loss: 0.000070\n",
      "Epoch 51, loss: 0.000070\n",
      "Epoch 52, loss: 0.000070\n",
      "Epoch 53, loss: 0.000070\n",
      "Epoch 54, loss: 0.000070\n",
      "Epoch 55, loss: 0.000070\n",
      "Epoch 56, loss: 0.000070\n",
      "Epoch 57, loss: 0.000070\n",
      "Epoch 58, loss: 0.000070\n",
      "Epoch 59, loss: 0.000070\n",
      "Epoch 60, loss: 0.000070\n",
      "Epoch 61, loss: 0.000070\n",
      "Epoch 62, loss: 0.000070\n",
      "Epoch 63, loss: 0.000070\n",
      "Epoch 64, loss: 0.000070\n",
      "Epoch 65, loss: 0.000070\n",
      "Epoch 66, loss: 0.000070\n",
      "Epoch 67, loss: 0.000070\n",
      "Epoch 68, loss: 0.000070\n",
      "Epoch 69, loss: 0.000070\n",
      "Epoch 70, loss: 0.000070\n",
      "Epoch 71, loss: 0.000070\n",
      "Epoch 72, loss: 0.000070\n",
      "Epoch 73, loss: 0.000070\n",
      "Epoch 74, loss: 0.000070\n",
      "Epoch 75, loss: 0.000070\n",
      "Epoch 76, loss: 0.000070\n",
      "Epoch 77, loss: 0.000070\n",
      "Epoch 78, loss: 0.000070\n",
      "Epoch 79, loss: 0.000070\n",
      "Epoch 80, loss: 0.000070\n",
      "Epoch 81, loss: 0.000070\n",
      "Epoch 82, loss: 0.000070\n",
      "Epoch 83, loss: 0.000070\n",
      "Epoch 84, loss: 0.000070\n",
      "Epoch 85, loss: 0.000070\n",
      "Epoch 86, loss: 0.000070\n",
      "Epoch 87, loss: 0.000070\n",
      "Epoch 88, loss: 0.000070\n",
      "Epoch 89, loss: 0.000070\n",
      "Epoch 90, loss: 0.000070\n",
      "Epoch 91, loss: 0.000070\n",
      "Epoch 92, loss: 0.000070\n",
      "Epoch 93, loss: 0.000070\n",
      "Epoch 94, loss: 0.000070\n",
      "Epoch 95, loss: 0.000070\n",
      "Epoch 96, loss: 0.000070\n",
      "Epoch 97, loss: 0.000070\n",
      "Epoch 98, loss: 0.000070\n",
      "Epoch 99, loss: 0.000070\n",
      "accuracy = 0.09\n",
      "learning_rate = 1e-06, reg_strength = 0.0001\n",
      "Epoch 0, loss: 0.000007\n",
      "Epoch 1, loss: 0.000007\n",
      "Epoch 2, loss: 0.000007\n",
      "Epoch 3, loss: 0.000007\n",
      "Epoch 4, loss: 0.000007\n",
      "Epoch 5, loss: 0.000007\n",
      "Epoch 6, loss: 0.000007\n",
      "Epoch 7, loss: 0.000007\n",
      "Epoch 8, loss: 0.000007\n",
      "Epoch 9, loss: 0.000007\n",
      "Epoch 10, loss: 0.000007\n",
      "Epoch 11, loss: 0.000007\n",
      "Epoch 12, loss: 0.000007\n",
      "Epoch 13, loss: 0.000007\n",
      "Epoch 14, loss: 0.000007\n",
      "Epoch 15, loss: 0.000007\n",
      "Epoch 16, loss: 0.000007\n",
      "Epoch 17, loss: 0.000007\n",
      "Epoch 18, loss: 0.000007\n",
      "Epoch 19, loss: 0.000007\n",
      "Epoch 20, loss: 0.000007\n",
      "Epoch 21, loss: 0.000007\n",
      "Epoch 22, loss: 0.000007\n",
      "Epoch 23, loss: 0.000007\n",
      "Epoch 24, loss: 0.000007\n",
      "Epoch 25, loss: 0.000007\n",
      "Epoch 26, loss: 0.000007\n",
      "Epoch 27, loss: 0.000007\n",
      "Epoch 28, loss: 0.000007\n",
      "Epoch 29, loss: 0.000007\n",
      "Epoch 30, loss: 0.000007\n",
      "Epoch 31, loss: 0.000007\n",
      "Epoch 32, loss: 0.000007\n",
      "Epoch 33, loss: 0.000007\n",
      "Epoch 34, loss: 0.000007\n",
      "Epoch 35, loss: 0.000007\n",
      "Epoch 36, loss: 0.000007\n",
      "Epoch 37, loss: 0.000007\n",
      "Epoch 38, loss: 0.000007\n",
      "Epoch 39, loss: 0.000007\n",
      "Epoch 40, loss: 0.000007\n",
      "Epoch 41, loss: 0.000007\n",
      "Epoch 42, loss: 0.000007\n",
      "Epoch 43, loss: 0.000007\n",
      "Epoch 44, loss: 0.000007\n",
      "Epoch 45, loss: 0.000007\n",
      "Epoch 46, loss: 0.000007\n",
      "Epoch 47, loss: 0.000007\n",
      "Epoch 48, loss: 0.000007\n",
      "Epoch 49, loss: 0.000007\n",
      "Epoch 50, loss: 0.000007\n",
      "Epoch 51, loss: 0.000007\n",
      "Epoch 52, loss: 0.000007\n",
      "Epoch 53, loss: 0.000007\n",
      "Epoch 54, loss: 0.000007\n",
      "Epoch 55, loss: 0.000007\n",
      "Epoch 56, loss: 0.000007\n",
      "Epoch 57, loss: 0.000007\n",
      "Epoch 58, loss: 0.000007\n",
      "Epoch 59, loss: 0.000007\n",
      "Epoch 60, loss: 0.000007\n",
      "Epoch 61, loss: 0.000007\n",
      "Epoch 62, loss: 0.000007\n",
      "Epoch 63, loss: 0.000007\n",
      "Epoch 64, loss: 0.000007\n",
      "Epoch 65, loss: 0.000007\n",
      "Epoch 66, loss: 0.000007\n",
      "Epoch 67, loss: 0.000007\n",
      "Epoch 68, loss: 0.000007\n",
      "Epoch 69, loss: 0.000007\n",
      "Epoch 70, loss: 0.000007\n",
      "Epoch 71, loss: 0.000007\n",
      "Epoch 72, loss: 0.000007\n",
      "Epoch 73, loss: 0.000007\n",
      "Epoch 74, loss: 0.000007\n",
      "Epoch 75, loss: 0.000007\n",
      "Epoch 76, loss: 0.000007\n",
      "Epoch 77, loss: 0.000007\n",
      "Epoch 78, loss: 0.000007\n",
      "Epoch 79, loss: 0.000007\n",
      "Epoch 80, loss: 0.000007\n",
      "Epoch 81, loss: 0.000007\n",
      "Epoch 82, loss: 0.000007\n",
      "Epoch 83, loss: 0.000007\n",
      "Epoch 84, loss: 0.000007\n",
      "Epoch 85, loss: 0.000007\n",
      "Epoch 86, loss: 0.000007\n",
      "Epoch 87, loss: 0.000007\n",
      "Epoch 88, loss: 0.000007\n",
      "Epoch 89, loss: 0.000007\n",
      "Epoch 90, loss: 0.000007\n",
      "Epoch 91, loss: 0.000007\n",
      "Epoch 92, loss: 0.000007\n",
      "Epoch 93, loss: 0.000007\n",
      "Epoch 94, loss: 0.000007\n",
      "Epoch 95, loss: 0.000007\n",
      "Epoch 96, loss: 0.000007\n",
      "Epoch 97, loss: 0.000007\n",
      "Epoch 98, loss: 0.000007\n",
      "Epoch 99, loss: 0.000007\n",
      "accuracy = 0.09111111111111111\n",
      "learning_rate = 1e-06, reg_strength = 1e-05\n",
      "Epoch 0, loss: 0.000001\n",
      "Epoch 1, loss: 0.000001\n",
      "Epoch 2, loss: 0.000001\n",
      "Epoch 3, loss: 0.000001\n",
      "Epoch 4, loss: 0.000001\n",
      "Epoch 5, loss: 0.000001\n",
      "Epoch 6, loss: 0.000001\n",
      "Epoch 7, loss: 0.000001\n",
      "Epoch 8, loss: 0.000001\n",
      "Epoch 9, loss: 0.000001\n",
      "Epoch 10, loss: 0.000001\n",
      "Epoch 11, loss: 0.000001\n",
      "Epoch 12, loss: 0.000001\n",
      "Epoch 13, loss: 0.000001\n",
      "Epoch 14, loss: 0.000001\n",
      "Epoch 15, loss: 0.000001\n",
      "Epoch 16, loss: 0.000001\n",
      "Epoch 17, loss: 0.000001\n",
      "Epoch 18, loss: 0.000001\n",
      "Epoch 19, loss: 0.000001\n",
      "Epoch 20, loss: 0.000001\n",
      "Epoch 21, loss: 0.000001\n",
      "Epoch 22, loss: 0.000001\n",
      "Epoch 23, loss: 0.000001\n",
      "Epoch 24, loss: 0.000001\n",
      "Epoch 25, loss: 0.000001\n",
      "Epoch 26, loss: 0.000001\n",
      "Epoch 27, loss: 0.000001\n",
      "Epoch 28, loss: 0.000001\n",
      "Epoch 29, loss: 0.000001\n",
      "Epoch 30, loss: 0.000001\n",
      "Epoch 31, loss: 0.000001\n",
      "Epoch 32, loss: 0.000001\n",
      "Epoch 33, loss: 0.000001\n",
      "Epoch 34, loss: 0.000001\n",
      "Epoch 35, loss: 0.000001\n",
      "Epoch 36, loss: 0.000001\n",
      "Epoch 37, loss: 0.000001\n",
      "Epoch 38, loss: 0.000001\n",
      "Epoch 39, loss: 0.000001\n",
      "Epoch 40, loss: 0.000001\n",
      "Epoch 41, loss: 0.000001\n",
      "Epoch 42, loss: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, loss: 0.000001\n",
      "Epoch 44, loss: 0.000001\n",
      "Epoch 45, loss: 0.000001\n",
      "Epoch 46, loss: 0.000001\n",
      "Epoch 47, loss: 0.000001\n",
      "Epoch 48, loss: 0.000001\n",
      "Epoch 49, loss: 0.000001\n",
      "Epoch 50, loss: 0.000001\n",
      "Epoch 51, loss: 0.000001\n",
      "Epoch 52, loss: 0.000001\n",
      "Epoch 53, loss: 0.000001\n",
      "Epoch 54, loss: 0.000001\n",
      "Epoch 55, loss: 0.000001\n",
      "Epoch 56, loss: 0.000001\n",
      "Epoch 57, loss: 0.000001\n",
      "Epoch 58, loss: 0.000001\n",
      "Epoch 59, loss: 0.000001\n",
      "Epoch 60, loss: 0.000001\n",
      "Epoch 61, loss: 0.000001\n",
      "Epoch 62, loss: 0.000001\n",
      "Epoch 63, loss: 0.000001\n",
      "Epoch 64, loss: 0.000001\n",
      "Epoch 65, loss: 0.000001\n",
      "Epoch 66, loss: 0.000001\n",
      "Epoch 67, loss: 0.000001\n",
      "Epoch 68, loss: 0.000001\n",
      "Epoch 69, loss: 0.000001\n",
      "Epoch 70, loss: 0.000001\n",
      "Epoch 71, loss: 0.000001\n",
      "Epoch 72, loss: 0.000001\n",
      "Epoch 73, loss: 0.000001\n",
      "Epoch 74, loss: 0.000001\n",
      "Epoch 75, loss: 0.000001\n",
      "Epoch 76, loss: 0.000001\n",
      "Epoch 77, loss: 0.000001\n",
      "Epoch 78, loss: 0.000001\n",
      "Epoch 79, loss: 0.000001\n",
      "Epoch 80, loss: 0.000001\n",
      "Epoch 81, loss: 0.000001\n",
      "Epoch 82, loss: 0.000001\n",
      "Epoch 83, loss: 0.000001\n",
      "Epoch 84, loss: 0.000001\n",
      "Epoch 85, loss: 0.000001\n",
      "Epoch 86, loss: 0.000001\n",
      "Epoch 87, loss: 0.000001\n",
      "Epoch 88, loss: 0.000001\n",
      "Epoch 89, loss: 0.000001\n",
      "Epoch 90, loss: 0.000001\n",
      "Epoch 91, loss: 0.000001\n",
      "Epoch 92, loss: 0.000001\n",
      "Epoch 93, loss: 0.000001\n",
      "Epoch 94, loss: 0.000001\n",
      "Epoch 95, loss: 0.000001\n",
      "Epoch 96, loss: 0.000001\n",
      "Epoch 97, loss: 0.000001\n",
      "Epoch 98, loss: 0.000001\n",
      "Epoch 99, loss: 0.000001\n",
      "accuracy = 0.08333333333333333\n",
      "learning_rate = 1e-06, reg_strength = 1e-06\n",
      "Epoch 0, loss: 0.000000\n",
      "Epoch 1, loss: 0.000000\n",
      "Epoch 2, loss: 0.000000\n",
      "Epoch 3, loss: 0.000000\n",
      "Epoch 4, loss: 0.000000\n",
      "Epoch 5, loss: 0.000000\n",
      "Epoch 6, loss: 0.000000\n",
      "Epoch 7, loss: 0.000000\n",
      "Epoch 8, loss: 0.000000\n",
      "Epoch 9, loss: 0.000000\n",
      "Epoch 10, loss: 0.000000\n",
      "Epoch 11, loss: 0.000000\n",
      "Epoch 12, loss: 0.000000\n",
      "Epoch 13, loss: 0.000000\n",
      "Epoch 14, loss: 0.000000\n",
      "Epoch 15, loss: 0.000000\n",
      "Epoch 16, loss: 0.000000\n",
      "Epoch 17, loss: 0.000000\n",
      "Epoch 18, loss: 0.000000\n",
      "Epoch 19, loss: 0.000000\n",
      "Epoch 20, loss: 0.000000\n",
      "Epoch 21, loss: 0.000000\n",
      "Epoch 22, loss: 0.000000\n",
      "Epoch 23, loss: 0.000000\n",
      "Epoch 24, loss: 0.000000\n",
      "Epoch 25, loss: 0.000000\n",
      "Epoch 26, loss: 0.000000\n",
      "Epoch 27, loss: 0.000000\n",
      "Epoch 28, loss: 0.000000\n",
      "Epoch 29, loss: 0.000000\n",
      "Epoch 30, loss: 0.000000\n",
      "Epoch 31, loss: 0.000000\n",
      "Epoch 32, loss: 0.000000\n",
      "Epoch 33, loss: 0.000000\n",
      "Epoch 34, loss: 0.000000\n",
      "Epoch 35, loss: 0.000000\n",
      "Epoch 36, loss: 0.000000\n",
      "Epoch 37, loss: 0.000000\n",
      "Epoch 38, loss: 0.000000\n",
      "Epoch 39, loss: 0.000000\n",
      "Epoch 40, loss: 0.000000\n",
      "Epoch 41, loss: 0.000000\n",
      "Epoch 42, loss: 0.000000\n",
      "Epoch 43, loss: 0.000000\n",
      "Epoch 44, loss: 0.000000\n",
      "Epoch 45, loss: 0.000000\n",
      "Epoch 46, loss: 0.000000\n",
      "Epoch 47, loss: 0.000000\n",
      "Epoch 48, loss: 0.000000\n",
      "Epoch 49, loss: 0.000000\n",
      "Epoch 50, loss: 0.000000\n",
      "Epoch 51, loss: 0.000000\n",
      "Epoch 52, loss: 0.000000\n",
      "Epoch 53, loss: 0.000000\n",
      "Epoch 54, loss: 0.000000\n",
      "Epoch 55, loss: 0.000000\n",
      "Epoch 56, loss: 0.000000\n",
      "Epoch 57, loss: 0.000000\n",
      "Epoch 58, loss: 0.000000\n",
      "Epoch 59, loss: 0.000000\n",
      "Epoch 60, loss: 0.000000\n",
      "Epoch 61, loss: 0.000000\n",
      "Epoch 62, loss: 0.000000\n",
      "Epoch 63, loss: 0.000000\n",
      "Epoch 64, loss: 0.000000\n",
      "Epoch 65, loss: 0.000000\n",
      "Epoch 66, loss: 0.000000\n",
      "Epoch 67, loss: 0.000000\n",
      "Epoch 68, loss: 0.000000\n",
      "Epoch 69, loss: 0.000000\n",
      "Epoch 70, loss: 0.000000\n",
      "Epoch 71, loss: 0.000000\n",
      "Epoch 72, loss: 0.000000\n",
      "Epoch 73, loss: 0.000000\n",
      "Epoch 74, loss: 0.000000\n",
      "Epoch 75, loss: 0.000000\n",
      "Epoch 76, loss: 0.000000\n",
      "Epoch 77, loss: 0.000000\n",
      "Epoch 78, loss: 0.000000\n",
      "Epoch 79, loss: 0.000000\n",
      "Epoch 80, loss: 0.000000\n",
      "Epoch 81, loss: 0.000000\n",
      "Epoch 82, loss: 0.000000\n",
      "Epoch 83, loss: 0.000000\n",
      "Epoch 84, loss: 0.000000\n",
      "Epoch 85, loss: 0.000000\n",
      "Epoch 86, loss: 0.000000\n",
      "Epoch 87, loss: 0.000000\n",
      "Epoch 88, loss: 0.000000\n",
      "Epoch 89, loss: 0.000000\n",
      "Epoch 90, loss: 0.000000\n",
      "Epoch 91, loss: 0.000000\n",
      "Epoch 92, loss: 0.000000\n",
      "Epoch 93, loss: 0.000000\n",
      "Epoch 94, loss: 0.000000\n",
      "Epoch 95, loss: 0.000000\n",
      "Epoch 96, loss: 0.000000\n",
      "Epoch 97, loss: 0.000000\n",
      "Epoch 98, loss: 0.000000\n",
      "Epoch 99, loss: 0.000000\n",
      "accuracy = 0.10111111111111111\n",
      "best validation accuracy achieved: 0.207222\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "reg_strengths = [1e1, 1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = -1.0\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "num_samples = train_y.shape[0]\n",
    "indices = np.arange(num_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split_index = num_samples * 2 // 10\n",
    "train_indices = indices[split_index:]\n",
    "val_indices = indices[:split_index]\n",
    "\n",
    "cv_train_X = train_X[train_indices]\n",
    "cv_train_y = train_y[train_indices]\n",
    "\n",
    "val_X = train_X[val_indices]\n",
    "val_y = train_y[val_indices]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for reg_strength in reg_strengths:\n",
    "        print(f'learning_rate = {learning_rate}, reg_strength = {reg_strength}')\n",
    "        clsr = linear_classifer.LinearSoftmaxClassifier()\n",
    "        clsr.fit(cv_train_X, cv_train_y, epochs=100, learning_rate=learning_rate, batch_size=300, reg=reg_strength)\n",
    "        clsr_pred = clsr.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(clsr_pred, val_y)\n",
    "        print(f'accuracy = {accuracy}')\n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = clsr\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)\n",
    "assert best_val_accuracy > 0.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.209000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
